{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ü§ñ Desarrollo de Modelos A/B Testing - Predicci√≥n Precio de Cierre t+1\n",
        "## DeAcero Steel Price Predictor - Implementaci√≥n Robusta de 15 Modelos\n",
        "\n",
        "---\n",
        "\n",
        "### üìã **Objetivo Principal**\n",
        "Predecir el **precio de cierre del d√≠a siguiente (t+1)** de la varilla corrugada LME mediante A/B testing robusto de 15 modelos diferentes.\n",
        "\n",
        "### üéØ **Estructura del Experimento**\n",
        "\n",
        "**5 Arquitecturas de Modelo:**\n",
        "1. **ARIMAX-GARCH**: Modelo econom√©trico con volatilidad condicional\n",
        "2. **XGBoost**: Gradient Boosting para capturar no-linealidades\n",
        "3. **LightGBM**: Gradient Boosting optimizado para velocidad\n",
        "4. **Markov Regime-Switching VAR**: Modelo adaptativo de reg√≠menes\n",
        "5. **MIDAS**: Mixed Data Sampling para frecuencias mixtas\n",
        "\n",
        "**3 Combinaciones de Variables:**\n",
        "1. **Fundamental Pura** (5 variables): iron, coking, gas_natural, aluminio_lme, commodities\n",
        "2. **H√≠brida Balanceada** (6 variables): precio_varilla_lme_lag_1, volatility_20, iron, coking, commodities, VIX\n",
        "3. **R√©gimen-Adaptativa** (6 variables): iron, coking, steel, VIX, sp500, tasa_interes_banxico\n",
        "\n",
        "**Total: 5 √ó 3 = 15 modelos**\n",
        "\n",
        "### üìä **Metodolog√≠a de Validaci√≥n**\n",
        "- **Time Series Cross-Validation**: 3 folds con walk-forward analysis\n",
        "- **B√∫squeda de Hiperpar√°metros**: GridSearchCV / Optuna\n",
        "- **M√©tricas**: RMSE, MAE, MAPE, Directional Accuracy, Hit Rate (¬±2%)\n",
        "- **Test Estad√≠stico**: Diebold-Mariano para comparaci√≥n significativa\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Librer√≠as cargadas exitosamente\n",
            "üìÖ Fecha de ejecuci√≥n: 2025-09-29 23:37:12\n",
            "üêç Python version: 3.13.7\n"
          ]
        }
      ],
      "source": [
        "# Configuraci√≥n inicial y librer√≠as\n",
        "import sys\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Importar pandas y numpy primero\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "import json\n",
        "import pickle\n",
        "import joblib\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Visualizaci√≥n\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# Machine Learning\n",
        "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "# Gradient Boosting\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Time Series\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from statsmodels.tsa.stattools import adfuller, acf, pacf\n",
        "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
        "from arch import arch_model\n",
        "\n",
        "# Optimizaci√≥n de hiperpar√°metros\n",
        "import optuna\n",
        "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "\n",
        "# Configuraci√≥n de estilo\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette('husl')\n",
        "\n",
        "print(\"üöÄ Librer√≠as cargadas exitosamente\")\n",
        "print(f\"üìÖ Fecha de ejecuci√≥n: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(f\"üêç Python version: {sys.version.split()[0]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Funciones de evaluaci√≥n definidas\n"
          ]
        }
      ],
      "source": [
        "# Funciones auxiliares para m√©tricas y evaluaci√≥n\n",
        "\n",
        "def calculate_directional_accuracy(y_true, y_pred):\n",
        "    \"\"\"Calcula la precisi√≥n direccional (si predice correctamente subida/bajada)\"\"\"\n",
        "    y_true_diff = np.diff(y_true)\n",
        "    y_pred_diff = np.diff(y_pred)\n",
        "    return np.mean(np.sign(y_true_diff) == np.sign(y_pred_diff)) * 100\n",
        "\n",
        "def calculate_hit_rate(y_true, y_pred, threshold=0.02):\n",
        "    \"\"\"Calcula el porcentaje de predicciones dentro del umbral especificado\"\"\"\n",
        "    relative_error = np.abs((y_true - y_pred) / y_true)\n",
        "    return np.mean(relative_error <= threshold) * 100\n",
        "\n",
        "def calculate_sharpe_ratio(returns, risk_free_rate=0.02):\n",
        "    \"\"\"Calcula el Sharpe Ratio de una estrategia de trading\"\"\"\n",
        "    excess_returns = returns - risk_free_rate/252  # Ajuste diario\n",
        "    if np.std(excess_returns) == 0:\n",
        "        return 0\n",
        "    return np.sqrt(252) * np.mean(excess_returns) / np.std(excess_returns)\n",
        "\n",
        "def diebold_mariano_test(e1, e2, h=1):\n",
        "    \"\"\"\n",
        "    Diebold-Mariano test para comparar precisi√≥n de pron√≥sticos\n",
        "    H0: Los dos modelos tienen igual precisi√≥n\n",
        "    \"\"\"\n",
        "    d = e1**2 - e2**2\n",
        "    mean_d = np.mean(d)\n",
        "    \n",
        "    # Autocovarianza\n",
        "    def autocovariance(xi, k):\n",
        "        return np.mean((xi[:-k] - mean_d) * (xi[k:] - mean_d))\n",
        "    \n",
        "    # Varianza con correcci√≥n Newey-West\n",
        "    gamma = [autocovariance(d, k) for k in range(h)]\n",
        "    v_d = gamma[0] + 2 * sum(gamma[1:])\n",
        "    \n",
        "    # Estad√≠stico DM\n",
        "    dm_stat = mean_d / np.sqrt(v_d / len(d))\n",
        "    \n",
        "    # P-value (two-tailed)\n",
        "    from scipy import stats\n",
        "    p_value = 2 * (1 - stats.norm.cdf(abs(dm_stat)))\n",
        "    \n",
        "    return dm_stat, p_value\n",
        "\n",
        "def evaluate_model(y_true, y_pred, model_name=\"Model\"):\n",
        "    \"\"\"Evaluaci√≥n completa del modelo con todas las m√©tricas\"\"\"\n",
        "    metrics = {\n",
        "        'model': model_name,\n",
        "        'rmse': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
        "        'mae': mean_absolute_error(y_true, y_pred),\n",
        "        'mape': mean_absolute_percentage_error(y_true, y_pred) * 100,\n",
        "        'r2': r2_score(y_true, y_pred),\n",
        "        'directional_accuracy': calculate_directional_accuracy(y_true, y_pred),\n",
        "        'hit_rate_2pct': calculate_hit_rate(y_true, y_pred, 0.02),\n",
        "        'hit_rate_5pct': calculate_hit_rate(y_true, y_pred, 0.05)\n",
        "    }\n",
        "    return metrics\n",
        "\n",
        "print(\"‚úÖ Funciones de evaluaci√≥n definidas\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìÇ Cargando datos diarios...\n",
            "‚úÖ Datos diarios cargados: 1498 observaciones, 24 variables\n",
            "   Per√≠odo: 2020-01-02 a 2025-09-29\n",
            "\n",
            "üìÇ Cargando datos mensuales...\n",
            "‚úÖ Datos mensuales cargados: 68 observaciones, 9 variables\n",
            "   Per√≠odo: 2020-01-01 a 2025-08-01\n",
            "\n",
            "üéØ Variable objetivo: precio_varilla_lme\n",
            "   Rango de precios: $408.50 - $590.70\n",
            "   Precio promedio: $495.13\n",
            "   Volatilidad (std): $30.57\n"
          ]
        }
      ],
      "source": [
        "# Carga de datos\n",
        "\n",
        "# Rutas de datos\n",
        "DATA_PATH = '../data/processed'\n",
        "DAILY_DATA_PATH = f'{DATA_PATH}/daily_time_series/daily_series_consolidated_latest.csv'\n",
        "MONTHLY_DATA_PATH = f'{DATA_PATH}/monthly_time_series/monthly_series_consolidated_latest.csv'\n",
        "\n",
        "# Cargar datos diarios\n",
        "print(\"üìÇ Cargando datos diarios...\")\n",
        "daily_data = pd.read_csv(DAILY_DATA_PATH)\n",
        "for col in daily_data.columns:\n",
        "    daily_data[col] = daily_data[col].interpolate(method='linear', limit_direction='both')\n",
        "daily_data['fecha'] = pd.to_datetime(daily_data['fecha'])\n",
        "daily_data.set_index('fecha', inplace=True)\n",
        "daily_data = daily_data.sort_index()\n",
        "\n",
        "print(f\"‚úÖ Datos diarios cargados: {daily_data.shape[0]} observaciones, {daily_data.shape[1]} variables\")\n",
        "print(f\"   Per√≠odo: {daily_data.index.min().date()} a {daily_data.index.max().date()}\")\n",
        "\n",
        "# Cargar datos mensuales\n",
        "print(\"\\nüìÇ Cargando datos mensuales...\")\n",
        "monthly_data = pd.read_csv(MONTHLY_DATA_PATH)\n",
        "\n",
        "for col in monthly_data.columns:\n",
        "    monthly_data[col] = monthly_data[col].interpolate(method='linear', limit_direction='both')\n",
        "monthly_data['fecha'] = pd.to_datetime(monthly_data['fecha'])\n",
        "monthly_data.set_index('fecha', inplace=True)\n",
        "monthly_data = monthly_data.sort_index()\n",
        "\n",
        "print(f\"‚úÖ Datos mensuales cargados: {monthly_data.shape[0]} observaciones, {monthly_data.shape[1]} variables\")\n",
        "print(f\"   Per√≠odo: {monthly_data.index.min().date()} a {monthly_data.index.max().date()}\")\n",
        "\n",
        "# Variable objetivo\n",
        "TARGET_VAR = 'precio_varilla_lme'\n",
        "print(f\"\\nüéØ Variable objetivo: {TARGET_VAR}\")\n",
        "print(f\"   Rango de precios: ${daily_data[TARGET_VAR].min():.2f} - ${daily_data[TARGET_VAR].max():.2f}\")\n",
        "print(f\"   Precio promedio: ${daily_data[TARGET_VAR].mean():.2f}\")\n",
        "print(f\"   Volatilidad (std): ${daily_data[TARGET_VAR].std():.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîß Creando features t√©cnicas...\n",
            "‚úÖ Features creadas: 49 variables totales\n",
            "   Observaciones despu√©s de limpiar NaN: 1449\n"
          ]
        }
      ],
      "source": [
        "# Feature Engineering y preparaci√≥n de datos\n",
        "\n",
        "def create_features(df, target_var=TARGET_VAR):\n",
        "    \"\"\"Crea features t√©cnicas y de mercado\"\"\"\n",
        "    features_df = df.copy()\n",
        "    \n",
        "    # 1. Lags de la variable objetivo\n",
        "    for lag in [1, 2, 3, 5, 10, 20]:\n",
        "        features_df[f'{target_var}_lag_{lag}'] = df[target_var].shift(lag)\n",
        "    \n",
        "    # 2. Medias m√≥viles\n",
        "    for window in [5, 10, 20, 50]:\n",
        "        features_df[f'{target_var}_ma_{window}'] = df[target_var].rolling(window=window).mean()\n",
        "    \n",
        "    # 3. Volatilidad rolling\n",
        "    for window in [5, 10, 20]:\n",
        "        features_df[f'{target_var}_volatility_{window}'] = df[target_var].pct_change().rolling(window=window).std()\n",
        "    \n",
        "    # 4. Retornos\n",
        "    features_df[f'{target_var}_return_1d'] = df[target_var].pct_change()\n",
        "    features_df[f'{target_var}_return_5d'] = df[target_var].pct_change(5)\n",
        "    features_df[f'{target_var}_return_20d'] = df[target_var].pct_change(20)\n",
        "    \n",
        "    # 5. RSI\n",
        "    def calculate_rsi(prices, period=14):\n",
        "        delta = prices.diff()\n",
        "        gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()\n",
        "        loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()\n",
        "        rs = gain / loss\n",
        "        rsi = 100 - (100 / (1 + rs))\n",
        "        return rsi\n",
        "    \n",
        "    features_df[f'{target_var}_rsi_14'] = calculate_rsi(df[target_var])\n",
        "    \n",
        "    # 6. Bollinger Bands\n",
        "    bb_window = 20\n",
        "    bb_std = 2\n",
        "    bb_ma = df[target_var].rolling(window=bb_window).mean()\n",
        "    bb_std_val = df[target_var].rolling(window=bb_window).std()\n",
        "    features_df[f'{target_var}_bb_upper'] = bb_ma + (bb_std_val * bb_std)\n",
        "    features_df[f'{target_var}_bb_lower'] = bb_ma - (bb_std_val * bb_std)\n",
        "    features_df[f'{target_var}_bb_width'] = features_df[f'{target_var}_bb_upper'] - features_df[f'{target_var}_bb_lower']\n",
        "    features_df[f'{target_var}_bb_position'] = (df[target_var] - features_df[f'{target_var}_bb_lower']) / features_df[f'{target_var}_bb_width']\n",
        "    \n",
        "    # 7. Features de calendario\n",
        "    features_df['day_of_week'] = df.index.dayofweek\n",
        "    features_df['day_of_month'] = df.index.day\n",
        "    features_df['month'] = df.index.month\n",
        "    features_df['quarter'] = df.index.quarter\n",
        "    \n",
        "    return features_df\n",
        "\n",
        "# Crear features\n",
        "print(\"üîß Creando features t√©cnicas...\")\n",
        "daily_features = create_features(daily_data)\n",
        "print(f\"‚úÖ Features creadas: {daily_features.shape[1]} variables totales\")\n",
        "\n",
        "# Eliminar filas con NaN (debido a lags y rolling windows)\n",
        "daily_features = daily_features.dropna()\n",
        "print(f\"   Observaciones despu√©s de limpiar NaN: {daily_features.shape[0]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Combinaciones de variables definidas:\n",
            "\n",
            "   FUNDAMENTAL:\n",
            "      ‚úÖ iron\n",
            "      ‚úÖ coking\n",
            "      ‚úÖ gas_natural\n",
            "      ‚úÖ aluminio_lme\n",
            "      ‚úÖ commodities\n",
            "\n",
            "   HIBRIDA:\n",
            "      ‚úÖ precio_varilla_lme_lag_1\n",
            "      ‚úÖ precio_varilla_lme_volatility_20\n",
            "      ‚úÖ iron\n",
            "      ‚úÖ coking\n",
            "      ‚úÖ commodities\n",
            "      ‚úÖ VIX\n",
            "\n",
            "   REGIME:\n",
            "      ‚úÖ iron\n",
            "      ‚úÖ coking\n",
            "      ‚úÖ steel\n",
            "      ‚úÖ VIX\n",
            "      ‚úÖ sp500\n",
            "      ‚úÖ tasa_interes_banxico\n"
          ]
        }
      ],
      "source": [
        "# Definici√≥n de las 3 combinaciones de variables\n",
        "\n",
        "# Combinaci√≥n 1: FUNDAMENTAL PURA (5 variables)\n",
        "fundamental_vars = [\n",
        "    'iron',\n",
        "    'coking', \n",
        "    'gas_natural',\n",
        "    'aluminio_lme',\n",
        "    'commodities'\n",
        "]\n",
        "\n",
        "# Combinaci√≥n 2: H√çBRIDA BALANCEADA (6 variables + features t√©cnicas)\n",
        "hibrida_vars = [\n",
        "    f'{TARGET_VAR}_lag_1',\n",
        "    f'{TARGET_VAR}_volatility_20',\n",
        "    'iron',\n",
        "    'coking',\n",
        "    'commodities',\n",
        "    'VIX'\n",
        "]\n",
        "\n",
        "# Combinaci√≥n 3: R√âGIMEN-ADAPTATIVA (6 variables)\n",
        "regime_vars = [\n",
        "    'iron',\n",
        "    'coking',\n",
        "    'steel',\n",
        "    'VIX',\n",
        "    'sp500',\n",
        "    'tasa_interes_banxico'\n",
        "]\n",
        "\n",
        "# Diccionario de combinaciones\n",
        "variable_combinations = {\n",
        "    'fundamental': fundamental_vars,\n",
        "    'hibrida': hibrida_vars,\n",
        "    'regime': regime_vars\n",
        "}\n",
        "\n",
        "print(\"üìä Combinaciones de variables definidas:\")\n",
        "for name, vars in variable_combinations.items():\n",
        "    print(f\"\\n   {name.upper()}:\")\n",
        "    for var in vars:\n",
        "        if var in daily_features.columns:\n",
        "            print(f\"      ‚úÖ {var}\")\n",
        "        else:\n",
        "            print(f\"      ‚ùå {var} (no encontrada)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì¶ Preparando datos para cada combinaci√≥n de variables...\n",
            "\n",
            "Procesando fundamental...\n",
            "   ‚úÖ Shape: X=(1447, 5), y=(1447,)\n",
            "   Per√≠odo: 2020-03-12 a 2025-09-26\n",
            "Procesando hibrida...\n",
            "   ‚úÖ Shape: X=(1447, 6), y=(1447,)\n",
            "   Per√≠odo: 2020-03-12 a 2025-09-26\n",
            "Procesando regime...\n",
            "   ‚úÖ Shape: X=(1447, 6), y=(1447,)\n",
            "   Per√≠odo: 2020-03-12 a 2025-09-26\n",
            "\n",
            "‚úÖ Datos preparados para modelado\n"
          ]
        }
      ],
      "source": [
        "# Preparaci√≥n de datos para modelado\n",
        "\n",
        "def prepare_data_for_modeling(df, target_var, feature_vars, forecast_horizon=1):\n",
        "    \"\"\"\n",
        "    Prepara los datos para modelado con predicci√≥n t+forecast_horizon\n",
        "    \"\"\"\n",
        "    # Seleccionar features\n",
        "    X = df[feature_vars].copy()\n",
        "    \n",
        "    # Variable objetivo: precio en t+forecast_horizon\n",
        "    y = df[target_var].shift(-forecast_horizon)\n",
        "    \n",
        "    # Eliminar NaN\n",
        "    valid_idx = ~(X.isna().any(axis=1) | y.isna())\n",
        "    X = X[valid_idx]\n",
        "    y = y[valid_idx]\n",
        "    \n",
        "    # Convertir a retornos logar√≠tmicos para estacionariedad\n",
        "    X_returns = pd.DataFrame(index=X.index)\n",
        "    for col in X.columns:\n",
        "        if col.endswith('_lag_1') or 'volatility' in col or 'VIX' in col or 'tasa_interes' in col:\n",
        "            # Mantener en niveles\n",
        "            X_returns[col] = X[col]\n",
        "        else:\n",
        "            # Convertir a retornos logar√≠tmicos\n",
        "            X_returns[col] = np.log(X[col] / X[col].shift(1))\n",
        "    \n",
        "    # Target: retorno logar√≠tmico\n",
        "    y_returns = np.log(y / df[target_var][valid_idx])\n",
        "    \n",
        "    # Eliminar infinitos y NaN resultantes\n",
        "    valid_returns = ~(X_returns.isna().any(axis=1) | y_returns.isna() | \n",
        "                     np.isinf(X_returns).any(axis=1) | np.isinf(y_returns))\n",
        "    \n",
        "    X_clean = X_returns[valid_returns]\n",
        "    y_clean = y_returns[valid_returns]\n",
        "    \n",
        "    return X_clean, y_clean\n",
        "\n",
        "# Preparar datos para cada combinaci√≥n\n",
        "print(\"üì¶ Preparando datos para cada combinaci√≥n de variables...\\n\")\n",
        "\n",
        "prepared_data = {}\n",
        "for combo_name, vars_list in variable_combinations.items():\n",
        "    print(f\"Procesando {combo_name}...\")\n",
        "    try:\n",
        "        X, y = prepare_data_for_modeling(daily_features, TARGET_VAR, vars_list)\n",
        "        prepared_data[combo_name] = {'X': X, 'y': y}\n",
        "        print(f\"   ‚úÖ Shape: X={X.shape}, y={y.shape}\")\n",
        "        print(f\"   Per√≠odo: {X.index.min().date()} a {X.index.max().date()}\")\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Error: {e}\")\n",
        "        \n",
        "print(\"\\n‚úÖ Datos preparados para modelado\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Configuraci√≥n de Time Series Cross-Validation:\n",
            "   - N√∫mero de splits: 3\n",
            "   - Tama√±o de entrenamiento: 500 d√≠as\n",
            "   - Tama√±o de prueba: 60 d√≠as\n",
            "   - Gap entre train y test: 0 d√≠as\n",
            "\n",
            "Ejemplo de splits para combinaci√≥n 'fundamental':\n",
            "\n",
            "   Split 1:\n",
            "      Train: 2020-03-12 a 2022-02-09 (500 d√≠as)\n",
            "      Test:  2022-02-10 a 2022-05-04 (60 d√≠as)\n",
            "\n",
            "   Split 2:\n",
            "      Train: 2021-11-23 a 2023-10-23 (500 d√≠as)\n",
            "      Test:  2023-10-24 a 2024-01-15 (60 d√≠as)\n",
            "\n",
            "   Split 3:\n",
            "      Train: 2023-08-04 a 2025-07-03 (500 d√≠as)\n",
            "      Test:  2025-07-04 a 2025-09-25 (60 d√≠as)\n"
          ]
        }
      ],
      "source": [
        "# Configuraci√≥n de Time Series Cross-Validation\n",
        "\n",
        "class TimeSeriesCV:\n",
        "    \"\"\"Validaci√≥n cruzada espec√≠fica para series temporales con walk-forward analysis\"\"\"\n",
        "    \n",
        "    def __init__(self, n_splits=3, train_size=500, test_size=60, gap=0):\n",
        "        self.n_splits = n_splits\n",
        "        self.train_size = train_size\n",
        "        self.test_size = test_size\n",
        "        self.gap = gap\n",
        "    \n",
        "    def split(self, X, y=None):\n",
        "        n_samples = len(X)\n",
        "        indices = np.arange(n_samples)\n",
        "        \n",
        "        # Calcular el paso entre splits\n",
        "        step = (n_samples - self.train_size - self.test_size - self.gap) // (self.n_splits - 1)\n",
        "        \n",
        "        for i in range(self.n_splits):\n",
        "            train_start = i * step\n",
        "            train_end = train_start + self.train_size\n",
        "            test_start = train_end + self.gap\n",
        "            test_end = min(test_start + self.test_size, n_samples)\n",
        "            \n",
        "            if test_end > n_samples:\n",
        "                break\n",
        "                \n",
        "            train_idx = indices[train_start:train_end]\n",
        "            test_idx = indices[test_start:test_end]\n",
        "            \n",
        "            yield train_idx, test_idx\n",
        "\n",
        "# Crear objeto de validaci√≥n cruzada\n",
        "tscv = TimeSeriesCV(n_splits=3, train_size=500, test_size=60, gap=0)\n",
        "\n",
        "# Visualizar los splits\n",
        "print(\"üìä Configuraci√≥n de Time Series Cross-Validation:\")\n",
        "print(f\"   - N√∫mero de splits: 3\")\n",
        "print(f\"   - Tama√±o de entrenamiento: 500 d√≠as\")\n",
        "print(f\"   - Tama√±o de prueba: 60 d√≠as\")\n",
        "print(f\"   - Gap entre train y test: 0 d√≠as\\n\")\n",
        "\n",
        "# Mostrar ejemplo de splits para la primera combinaci√≥n\n",
        "if prepared_data:\n",
        "    first_combo = list(prepared_data.keys())[0]\n",
        "    X_example = prepared_data[first_combo]['X']\n",
        "    y_example = prepared_data[first_combo]['y']\n",
        "    \n",
        "    print(f\"Ejemplo de splits para combinaci√≥n '{first_combo}':\")\n",
        "    for i, (train_idx, test_idx) in enumerate(tscv.split(X_example)):\n",
        "        train_dates = X_example.iloc[train_idx].index\n",
        "        test_dates = X_example.iloc[test_idx].index\n",
        "        print(f\"\\n   Split {i+1}:\")\n",
        "        print(f\"      Train: {train_dates.min().date()} a {train_dates.max().date()} ({len(train_idx)} d√≠as)\")\n",
        "        print(f\"      Test:  {test_dates.min().date()} a {test_dates.max().date()} ({len(test_idx)} d√≠as)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ü§ñ Implementaci√≥n de los 5 Modelos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Modelo 1: ARIMAX-GARCH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Funci√≥n ARIMAX-GARCH definida\n"
          ]
        }
      ],
      "source": [
        "# Modelo 1: ARIMAX-GARCH\n",
        "\n",
        "def train_arimax_garch_model(X_train, y_train, X_test, y_test, combo_name):\n",
        "    \"\"\"\n",
        "    Entrena modelo ARIMAX con componente GARCH para volatilidad\n",
        "    \"\"\"\n",
        "    from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "    \n",
        "    # Preparar datos para ARIMAX\n",
        "    # Combinar X e y para mantener alineaci√≥n temporal\n",
        "    train_data = pd.concat([y_train, X_train], axis=1)\n",
        "    test_data = pd.concat([y_test, X_test], axis=1)\n",
        "    \n",
        "    # Buscar mejores √≥rdenes ARIMA con AIC\n",
        "    best_aic = np.inf\n",
        "    best_order = None\n",
        "    best_model = None\n",
        "    \n",
        "    # Grid search simplificado para √≥rdenes ARIMA\n",
        "    p_values = [0, 1, 2]\n",
        "    d_values = [0, 1]\n",
        "    q_values = [0, 1, 2]\n",
        "    \n",
        "    for p in p_values:\n",
        "        for d in d_values:\n",
        "            for q in q_values:\n",
        "                try:\n",
        "                    model = SARIMAX(y_train, \n",
        "                                   exog=X_train,\n",
        "                                   order=(p, d, q),\n",
        "                                   enforce_stationarity=False,\n",
        "                                   enforce_invertibility=False)\n",
        "                    \n",
        "                    fitted = model.fit(disp=False)\n",
        "                    \n",
        "                    if fitted.aic < best_aic:\n",
        "                        best_aic = fitted.aic\n",
        "                        best_order = (p, d, q)\n",
        "                        best_model = fitted\n",
        "                        \n",
        "                except Exception:\n",
        "                    continue\n",
        "    \n",
        "    # Si encontramos un modelo v√°lido\n",
        "    if best_model is not None:\n",
        "        # Predicciones ARIMAX\n",
        "        y_pred_train = best_model.fittedvalues\n",
        "        y_pred_test = best_model.forecast(steps=len(X_test), exog=X_test)\n",
        "        \n",
        "        # Residuos para GARCH\n",
        "        residuals = y_train - y_pred_train\n",
        "        \n",
        "        # Ajustar modelo GARCH(1,1) a los residuos\n",
        "        try:\n",
        "            garch_model = arch_model(residuals, vol='Garch', p=1, q=1, dist='t')\n",
        "            garch_fitted = garch_model.fit(disp='off')\n",
        "            \n",
        "            # Pron√≥stico de volatilidad\n",
        "            volatility_forecast = garch_fitted.forecast(horizon=len(X_test))\n",
        "            \n",
        "            # Intervalos de confianza usando volatilidad GARCH\n",
        "            vol_test = np.sqrt(volatility_forecast.variance.values[-1, :])\n",
        "            \n",
        "        except:\n",
        "            # Si GARCH falla, usar volatilidad constante\n",
        "            vol_test = np.std(residuals) * np.ones(len(X_test))\n",
        "    else:\n",
        "        # Modelo de respaldo si ARIMAX falla\n",
        "        y_pred_train = np.mean(y_train) * np.ones(len(y_train))\n",
        "        y_pred_test = np.mean(y_train) * np.ones(len(y_test))\n",
        "        best_order = (0, 0, 0)\n",
        "        vol_test = np.std(y_train) * np.ones(len(y_test))\n",
        "    \n",
        "    # M√©tricas\n",
        "    train_metrics = evaluate_model(y_train.values, y_pred_train, f\"ARIMAX-GARCH_{combo_name}_train\")\n",
        "    test_metrics = evaluate_model(y_test.values, y_pred_test, f\"ARIMAX-GARCH_{combo_name}_test\")\n",
        "    \n",
        "    return {\n",
        "        'model': best_model,\n",
        "        'arima_order': best_order,\n",
        "        'train_metrics': train_metrics,\n",
        "        'test_metrics': test_metrics,\n",
        "        'predictions': {'train': y_pred_train, 'test': y_pred_test},\n",
        "        'volatility_forecast': vol_test if 'vol_test' in locals() else None\n",
        "    }\n",
        "\n",
        "print(\"‚úÖ Funci√≥n ARIMAX-GARCH definida\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Modelo 2: XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Funci√≥n XGBoost definida\n"
          ]
        }
      ],
      "source": [
        "# Modelo 2: XGBoost con Optuna para b√∫squeda de hiperpar√°metros\n",
        "\n",
        "def train_xgboost_model(X_train, y_train, X_test, y_test, combo_name, n_trials=50):\n",
        "    \"\"\"\n",
        "    Entrena modelo XGBoost con b√∫squeda de hiperpar√°metros usando Optuna\n",
        "    \"\"\"\n",
        "    \n",
        "    def objective(trial):\n",
        "        # Hiperpar√°metros a optimizar\n",
        "        params = {\n",
        "            'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
        "            'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
        "            'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.3, log=True),\n",
        "            'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
        "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
        "            'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
        "            'gamma': trial.suggest_float('gamma', 0, 0.5),\n",
        "            'reg_alpha': trial.suggest_float('reg_alpha', 0, 1.0),\n",
        "            'reg_lambda': trial.suggest_float('reg_lambda', 0, 1.0),\n",
        "            'random_state': 42,\n",
        "            'n_jobs': -1\n",
        "        }\n",
        "        \n",
        "        # Crear modelo sin early_stopping_rounds para la optimizaci√≥n\n",
        "        model = xgb.XGBRegressor(**params)\n",
        "        \n",
        "        # Validaci√≥n cruzada con TimeSeriesCV\n",
        "        cv_scores = []\n",
        "        for train_idx, val_idx in tscv.split(X_train):\n",
        "            X_cv_train = X_train.iloc[train_idx]\n",
        "            y_cv_train = y_train.iloc[train_idx]\n",
        "            X_cv_val = X_train.iloc[val_idx]\n",
        "            y_cv_val = y_train.iloc[val_idx]\n",
        "            \n",
        "            # Entrenar sin early stopping en la validaci√≥n cruzada\n",
        "            model.fit(X_cv_train, y_cv_train, verbose=False)\n",
        "            \n",
        "            y_pred = model.predict(X_cv_val)\n",
        "            rmse = np.sqrt(mean_squared_error(y_cv_val, y_pred))\n",
        "            cv_scores.append(rmse)\n",
        "        \n",
        "        return np.mean(cv_scores)\n",
        "    \n",
        "    # Crear estudio de Optuna\n",
        "    study = optuna.create_study(direction='minimize', study_name=f'xgboost_{combo_name}')\n",
        "    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
        "    \n",
        "    # Entrenar modelo final con mejores hiperpar√°metros\n",
        "    best_params = study.best_params\n",
        "    best_params['random_state'] = 42\n",
        "    best_params['n_jobs'] = -1\n",
        "    \n",
        "    # Para el modelo final, usar early stopping con un conjunto de validaci√≥n\n",
        "    X_train_split = X_train.iloc[:-int(len(X_train)*0.2)]\n",
        "    y_train_split = y_train.iloc[:-int(len(y_train)*0.2)]\n",
        "    X_val_split = X_train.iloc[-int(len(X_train)*0.2):]\n",
        "    y_val_split = y_train.iloc[-int(len(y_train)*0.2):]\n",
        "    \n",
        "    # Agregar early stopping solo para el modelo final\n",
        "    best_params['early_stopping_rounds'] = 50\n",
        "    \n",
        "    final_model = xgb.XGBRegressor(**best_params)\n",
        "    final_model.fit(X_train_split, y_train_split, \n",
        "                   eval_set=[(X_val_split, y_val_split)],\n",
        "                   verbose=False)\n",
        "    \n",
        "    # Predicciones\n",
        "    y_pred_train = final_model.predict(X_train)\n",
        "    y_pred_test = final_model.predict(X_test)\n",
        "    \n",
        "    # M√©tricas\n",
        "    train_metrics = evaluate_model(y_train, y_pred_train, f\"XGBoost_{combo_name}_train\")\n",
        "    test_metrics = evaluate_model(y_test, y_pred_test, f\"XGBoost_{combo_name}_test\")\n",
        "    \n",
        "    return {\n",
        "        'model': final_model,\n",
        "        'best_params': best_params,\n",
        "        'train_metrics': train_metrics,\n",
        "        'test_metrics': test_metrics,\n",
        "        'predictions': {'train': y_pred_train, 'test': y_pred_test}\n",
        "    }\n",
        "\n",
        "print(\"‚úÖ Funci√≥n XGBoost definida\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Modelo 3: LightGBM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Funci√≥n LightGBM definida\n"
          ]
        }
      ],
      "source": [
        "# Modelo 3: LightGBM con Optuna\n",
        "\n",
        "def train_lightgbm_model(X_train, y_train, X_test, y_test, combo_name, n_trials=50):\n",
        "    \"\"\"\n",
        "    Entrena modelo LightGBM con b√∫squeda de hiperpar√°metros usando Optuna\n",
        "    \"\"\"\n",
        "    \n",
        "    def objective(trial):\n",
        "        # Hiperpar√°metros a optimizar\n",
        "        params = {\n",
        "            'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
        "            'num_leaves': trial.suggest_int('num_leaves', 10, 100),\n",
        "            'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
        "            'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.3, log=True),\n",
        "            'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),\n",
        "            'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),\n",
        "            'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n",
        "            'min_child_samples': trial.suggest_int('min_child_samples', 5, 50),\n",
        "            'reg_alpha': trial.suggest_float('reg_alpha', 0, 1.0),\n",
        "            'reg_lambda': trial.suggest_float('reg_lambda', 0, 1.0),\n",
        "            'random_state': 42,\n",
        "            'n_jobs': -1,\n",
        "            'verbosity': -1\n",
        "        }\n",
        "        \n",
        "        # Crear y entrenar modelo\n",
        "        model = lgb.LGBMRegressor(**params)\n",
        "        \n",
        "        # Validaci√≥n cruzada con TimeSeriesCV\n",
        "        cv_scores = []\n",
        "        for train_idx, val_idx in tscv.split(X_train):\n",
        "            X_cv_train = X_train.iloc[train_idx]\n",
        "            y_cv_train = y_train.iloc[train_idx]\n",
        "            X_cv_val = X_train.iloc[val_idx]\n",
        "            y_cv_val = y_train.iloc[val_idx]\n",
        "            \n",
        "            model.fit(X_cv_train, y_cv_train,\n",
        "                     eval_set=[(X_cv_val, y_cv_val)],\n",
        "                     callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)])\n",
        "            \n",
        "            y_pred = model.predict(X_cv_val)\n",
        "            rmse = np.sqrt(mean_squared_error(y_cv_val, y_pred))\n",
        "            cv_scores.append(rmse)\n",
        "        \n",
        "        return np.mean(cv_scores)\n",
        "    \n",
        "    # Crear estudio de Optuna\n",
        "    study = optuna.create_study(direction='minimize', study_name=f'lightgbm_{combo_name}')\n",
        "    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
        "    \n",
        "    # Entrenar modelo final con mejores hiperpar√°metros\n",
        "    best_params = study.best_params\n",
        "    best_params['random_state'] = 42\n",
        "    best_params['n_jobs'] = -1\n",
        "    best_params['verbosity'] = -1\n",
        "    \n",
        "    final_model = lgb.LGBMRegressor(**best_params)\n",
        "    final_model.fit(X_train, y_train)\n",
        "    \n",
        "    # Predicciones\n",
        "    y_pred_train = final_model.predict(X_train)\n",
        "    y_pred_test = final_model.predict(X_test)\n",
        "    \n",
        "    # M√©tricas\n",
        "    train_metrics = evaluate_model(y_train, y_pred_train, f\"LightGBM_{combo_name}_train\")\n",
        "    test_metrics = evaluate_model(y_test, y_pred_test, f\"LightGBM_{combo_name}_test\")\n",
        "    \n",
        "    # Feature importance\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'feature': X_train.columns,\n",
        "        'importance': final_model.feature_importances_\n",
        "    }).sort_values('importance', ascending=False)\n",
        "    \n",
        "    return {\n",
        "        'model': final_model,\n",
        "        'best_params': best_params,\n",
        "        'train_metrics': train_metrics,\n",
        "        'test_metrics': test_metrics,\n",
        "        'predictions': {'train': y_pred_train, 'test': y_pred_test},\n",
        "        'feature_importance': feature_importance\n",
        "    }\n",
        "\n",
        "print(\"‚úÖ Funci√≥n LightGBM definida\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Modelo 4: Markov Regime-Switching VAR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Funci√≥n Markov Regime-Switching definida\n"
          ]
        }
      ],
      "source": [
        "# Modelo 4: Markov Regime-Switching VAR (simplificado)\n",
        "\n",
        "def train_regime_switching_model(X_train, y_train, X_test, y_test, combo_name):\n",
        "    \"\"\"\n",
        "    Entrena modelo de cambio de r√©gimen de Markov\n",
        "    Versi√≥n simplificada usando clustering para detectar reg√≠menes\n",
        "    \"\"\"\n",
        "    from sklearn.cluster import KMeans\n",
        "    from sklearn.mixture import GaussianMixture\n",
        "    \n",
        "    # Detectar reg√≠menes usando GMM en los retornos y volatilidad\n",
        "    returns = y_train.pct_change().dropna()\n",
        "    volatility = returns.rolling(window=20).std().dropna()\n",
        "    \n",
        "    # Crear features para detecci√≥n de r√©gimen\n",
        "    regime_features = pd.DataFrame({\n",
        "        'returns': returns[volatility.index],\n",
        "        'volatility': volatility\n",
        "    }).dropna()\n",
        "    \n",
        "    # Ajustar GMM con 3 reg√≠menes\n",
        "    n_regimes = 3\n",
        "    gmm = GaussianMixture(n_components=n_regimes, random_state=42)\n",
        "    \n",
        "    if len(regime_features) > 0:\n",
        "        regimes_train = gmm.fit_predict(regime_features)\n",
        "        \n",
        "        # Expandir reg√≠menes a todo el conjunto de entrenamiento\n",
        "        regime_series = pd.Series(index=y_train.index)\n",
        "        regime_series.iloc[len(y_train) - len(regimes_train):] = regimes_train\n",
        "        regime_series = regime_series.fillna(method='bfill').fillna(0)\n",
        "        \n",
        "        # Entrenar un modelo separado para cada r√©gimen\n",
        "        models_by_regime = {}\n",
        "        \n",
        "        for regime in range(n_regimes):\n",
        "            mask = regime_series == regime\n",
        "            if mask.sum() > 10:  # M√≠nimo de observaciones\n",
        "                X_regime = X_train[mask]\n",
        "                y_regime = y_train[mask]\n",
        "                \n",
        "                # Usar XGBoost para cada r√©gimen\n",
        "                model = xgb.XGBRegressor(\n",
        "                    n_estimators=100,\n",
        "                    max_depth=5,\n",
        "                    learning_rate=0.1,\n",
        "                    random_state=42\n",
        "                )\n",
        "                model.fit(X_regime, y_regime)\n",
        "                models_by_regime[regime] = model\n",
        "        \n",
        "        # Para predicci√≥n, detectar r√©gimen actual y usar modelo correspondiente\n",
        "        if len(models_by_regime) > 0:\n",
        "            # Detectar r√©gimen para test\n",
        "            test_returns = y_test.pct_change().fillna(0)\n",
        "            test_volatility = test_returns.rolling(window=min(20, len(test_returns))).std().fillna(0)\n",
        "            \n",
        "            test_regime_features = pd.DataFrame({\n",
        "                'returns': test_returns,\n",
        "                'volatility': test_volatility\n",
        "            })\n",
        "            \n",
        "            regimes_test = gmm.predict(test_regime_features)\n",
        "            \n",
        "            # Predicciones por r√©gimen\n",
        "            y_pred_train = np.zeros(len(y_train))\n",
        "            y_pred_test = np.zeros(len(y_test))\n",
        "            \n",
        "            # Train predictions\n",
        "            for regime in models_by_regime:\n",
        "                mask = regime_series == regime\n",
        "                if mask.sum() > 0:\n",
        "                    y_pred_train[mask] = models_by_regime[regime].predict(X_train[mask])\n",
        "            \n",
        "            # Test predictions\n",
        "            for i, regime in enumerate(regimes_test):\n",
        "                if regime in models_by_regime:\n",
        "                    y_pred_test[i] = models_by_regime[regime].predict(X_test.iloc[[i]])\n",
        "                else:\n",
        "                    # Usar modelo del r√©gimen m√°s com√∫n si no existe\n",
        "                    default_regime = regime_series.mode()[0]\n",
        "                    if default_regime in models_by_regime:\n",
        "                        y_pred_test[i] = models_by_regime[default_regime].predict(X_test.iloc[[i]])\n",
        "        else:\n",
        "            # Fallback a predicci√≥n simple\n",
        "            y_pred_train = np.mean(y_train) * np.ones(len(y_train))\n",
        "            y_pred_test = np.mean(y_train) * np.ones(len(y_test))\n",
        "    else:\n",
        "        # Fallback si no hay suficientes datos\n",
        "        y_pred_train = np.mean(y_train) * np.ones(len(y_train))\n",
        "        y_pred_test = np.mean(y_train) * np.ones(len(y_test))\n",
        "        regimes_train = np.zeros(len(y_train))\n",
        "        regimes_test = np.zeros(len(y_test))\n",
        "    \n",
        "    # M√©tricas\n",
        "    train_metrics = evaluate_model(y_train.values, y_pred_train, f\"RegimeSwitching_{combo_name}_train\")\n",
        "    test_metrics = evaluate_model(y_test.values, y_pred_test, f\"RegimeSwitching_{combo_name}_test\")\n",
        "    \n",
        "    return {\n",
        "        'model': {'gmm': gmm, 'models_by_regime': models_by_regime if 'models_by_regime' in locals() else {}},\n",
        "        'train_metrics': train_metrics,\n",
        "        'test_metrics': test_metrics,\n",
        "        'predictions': {'train': y_pred_train, 'test': y_pred_test},\n",
        "        'regimes': {'train': regimes_train if 'regimes_train' in locals() else None, \n",
        "                   'test': regimes_test if 'regimes_test' in locals() else None}\n",
        "    }\n",
        "\n",
        "print(\"‚úÖ Funci√≥n Markov Regime-Switching definida\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Modelo 5: MIDAS (Mixed Data Sampling)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Funci√≥n MIDAS definida\n"
          ]
        }
      ],
      "source": [
        "# Modelo 5: MIDAS (Mixed Data Sampling) - Implementaci√≥n simplificada\n",
        "\n",
        "def train_midas_model(X_train, y_train, X_test, y_test, combo_name, monthly_data):\n",
        "    \"\"\"\n",
        "    Entrena modelo MIDAS para combinar frecuencias diarias y mensuales\n",
        "    Implementaci√≥n simplificada usando weighted regression\n",
        "    \"\"\"\n",
        "    \n",
        "    # Alinear datos mensuales con datos diarios\n",
        "    # Resamplear datos mensuales a diarios con forward fill\n",
        "    monthly_resampled = monthly_data.resample('D').ffill()\n",
        "    \n",
        "    # Alinear con √≠ndice de X_train\n",
        "    monthly_aligned_train = monthly_resampled.reindex(X_train.index, method='ffill')\n",
        "    monthly_aligned_test = monthly_resampled.reindex(X_test.index, method='ffill')\n",
        "    \n",
        "    # Seleccionar columnas mensuales relevantes\n",
        "    monthly_cols = ['inflacion_mensual_mexico', 'produccion_industrial_usa', \n",
        "                   'produccion_metalurgica_mexico']\n",
        "    \n",
        "    # Filtrar columnas que existen\n",
        "    monthly_cols = [col for col in monthly_cols if col in monthly_aligned_train.columns]\n",
        "    \n",
        "    if len(monthly_cols) > 0:\n",
        "        # Crear ponderaciones exponenciales para datos mensuales (Almon polynomial)\n",
        "        def almon_weights(n_lags, theta1=0.1, theta2=-0.01):\n",
        "            \"\"\"Genera ponderaciones Almon para MIDAS\"\"\"\n",
        "            lags = np.arange(n_lags)\n",
        "            weights = np.exp(theta1 * lags + theta2 * lags**2)\n",
        "            return weights / weights.sum()\n",
        "        \n",
        "        # Crear features ponderadas de datos mensuales\n",
        "        n_lags = 30  # 30 d√≠as de historia\n",
        "        weights = almon_weights(n_lags)\n",
        "        \n",
        "        # Aplicar ponderaciones a variables mensuales\n",
        "        midas_features_train = pd.DataFrame(index=X_train.index)\n",
        "        midas_features_test = pd.DataFrame(index=X_test.index)\n",
        "        \n",
        "        for col in monthly_cols:\n",
        "            if col in monthly_aligned_train.columns:\n",
        "                # Crear lags ponderados\n",
        "                weighted_col = np.zeros(len(X_train))\n",
        "                for i in range(len(X_train)):\n",
        "                    if i >= n_lags:\n",
        "                        values = monthly_aligned_train[col].iloc[i-n_lags:i].values\n",
        "                        if len(values) == n_lags and not np.isnan(values).any():\n",
        "                            weighted_col[i] = np.sum(values * weights)\n",
        "                        else:\n",
        "                            weighted_col[i] = monthly_aligned_train[col].iloc[i] if i < len(monthly_aligned_train) else 0\n",
        "                    else:\n",
        "                        weighted_col[i] = monthly_aligned_train[col].iloc[i] if i < len(monthly_aligned_train) else 0\n",
        "                \n",
        "                midas_features_train[f'midas_{col}'] = weighted_col\n",
        "                \n",
        "                # Mismo proceso para test\n",
        "                weighted_col_test = np.zeros(len(X_test))\n",
        "                for i in range(len(X_test)):\n",
        "                    values = monthly_aligned_test[col].iloc[max(0, i-n_lags):i].values\n",
        "                    if len(values) > 0 and not np.isnan(values).all():\n",
        "                        if len(values) < n_lags:\n",
        "                            # Padding con el √∫ltimo valor si no hay suficientes lags\n",
        "                            padded_values = np.pad(values, (n_lags - len(values), 0), 'edge')\n",
        "                            weighted_col_test[i] = np.sum(padded_values * weights)\n",
        "                        else:\n",
        "                            weighted_col_test[i] = np.sum(values[-n_lags:] * weights)\n",
        "                    else:\n",
        "                        weighted_col_test[i] = monthly_aligned_test[col].iloc[i] if i < len(monthly_aligned_test) else 0\n",
        "                \n",
        "                midas_features_test[f'midas_{col}'] = weighted_col_test\n",
        "        \n",
        "        # Combinar features diarias con MIDAS features\n",
        "        X_train_midas = pd.concat([X_train, midas_features_train], axis=1)\n",
        "        X_test_midas = pd.concat([X_test, midas_features_test], axis=1)\n",
        "        \n",
        "        # Limpiar NaN e infinitos\n",
        "        X_train_midas = X_train_midas.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
        "        X_test_midas = X_test_midas.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
        "        \n",
        "    else:\n",
        "        # Si no hay datos mensuales, usar solo datos diarios\n",
        "        X_train_midas = X_train\n",
        "        X_test_midas = X_test\n",
        "    \n",
        "    # Entrenar modelo XGBoost con features MIDAS\n",
        "    model = xgb.XGBRegressor(\n",
        "        n_estimators=300,\n",
        "        max_depth=6,\n",
        "        learning_rate=0.05,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        random_state=42\n",
        "    )\n",
        "    \n",
        "    model.fit(X_train_midas, y_train)\n",
        "    \n",
        "    # Predicciones\n",
        "    y_pred_train = model.predict(X_train_midas)\n",
        "    y_pred_test = model.predict(X_test_midas)\n",
        "    \n",
        "    # M√©tricas\n",
        "    train_metrics = evaluate_model(y_train.values, y_pred_train, f\"MIDAS_{combo_name}_train\")\n",
        "    test_metrics = evaluate_model(y_test.values, y_pred_test, f\"MIDAS_{combo_name}_test\")\n",
        "    \n",
        "    # Feature importance\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'feature': X_train_midas.columns,\n",
        "        'importance': model.feature_importances_\n",
        "    }).sort_values('importance', ascending=False)\n",
        "    \n",
        "    return {\n",
        "        'model': model,\n",
        "        'train_metrics': train_metrics,\n",
        "        'test_metrics': test_metrics,\n",
        "        'predictions': {'train': y_pred_train, 'test': y_pred_test},\n",
        "        'feature_importance': feature_importance,\n",
        "        'midas_features': list(midas_features_train.columns) if 'midas_features_train' in locals() else []\n",
        "    }\n",
        "\n",
        "print(\"‚úÖ Funci√≥n MIDAS definida\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Ejecuci√≥n del A/B Testing - 15 Modelos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ INICIANDO A/B TESTING DE 15 MODELOS\n",
            "\n",
            "============================================================\n",
            "\n",
            "üìä COMBINACI√ìN: FUNDAMENTAL\n",
            "----------------------------------------\n",
            "   üßπ Datos limpiados: 1447 observaciones v√°lidas de 1447 originales\n",
            "   Train: 1374 observaciones\n",
            "   Test:  73 observaciones\n",
            "\n",
            "   ü§ñ Entrenando ARIMAX-GARCH...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      ‚úÖ RMSE Test: 0.0373\n",
            "      ‚úÖ MAPE Test: 480525111470.10%\n",
            "      ‚úÖ Dir. Acc: 40.3%\n",
            "      ‚úÖ Hit Rate ¬±2%: 0.0%\n",
            "\n",
            "   ü§ñ Entrenando XGBoost...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d682681e01c341a0818b22ccf3cd5702",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      ‚úÖ RMSE Test: 0.0373\n",
            "      ‚úÖ MAPE Test: 2252845426420.60%\n",
            "      ‚úÖ Dir. Acc: 1.4%\n",
            "      ‚úÖ Hit Rate ¬±2%: 0.0%\n",
            "\n",
            "   ü§ñ Entrenando LightGBM...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e944a7cbee4c46dc9884683dcef0b525",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[200]\tvalid_0's l2: 0.00116561\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[20]\tvalid_0's l2: 0.00161506\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[34]\tvalid_0's l2: 0.00121036\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's l2: 0.00125521\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[5]\tvalid_0's l2: 0.00162076\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[68]\tvalid_0's l2: 0.00121723\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's l2: 0.00125507\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[316]\tvalid_0's l2: 0.00159125\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[8]\tvalid_0's l2: 0.00121824\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[37]\tvalid_0's l2: 0.00122909\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's l2: 0.00162176\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[39]\tvalid_0's l2: 0.00121759\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[261]\tvalid_0's l2: 0.00124404\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[2]\tvalid_0's l2: 0.00162108\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[56]\tvalid_0's l2: 0.00121742\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[10]\tvalid_0's l2: 0.00125381\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[153]\tvalid_0's l2: 0.00160125\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[45]\tvalid_0's l2: 0.00120793\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[10]\tvalid_0's l2: 0.00123574\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[34]\tvalid_0's l2: 0.00161251\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[3]\tvalid_0's l2: 0.00121942\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[88]\tvalid_0's l2: 0.00123502\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[7]\tvalid_0's l2: 0.0016159\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[79]\tvalid_0's l2: 0.00121558\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[51]\tvalid_0's l2: 0.00124413\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's l2: 0.00162179\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[95]\tvalid_0's l2: 0.0012149\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[150]\tvalid_0's l2: 0.00120702\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[2]\tvalid_0's l2: 0.00161912\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[13]\tvalid_0's l2: 0.00121267\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[7]\tvalid_0's l2: 0.00121138\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[8]\tvalid_0's l2: 0.0015816\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[10]\tvalid_0's l2: 0.0011911\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[5]\tvalid_0's l2: 0.00120735\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[5]\tvalid_0's l2: 0.00157811\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[35]\tvalid_0's l2: 0.00116809\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[5]\tvalid_0's l2: 0.00121972\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[7]\tvalid_0's l2: 0.00156617\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[7]\tvalid_0's l2: 0.00117474\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[81]\tvalid_0's l2: 0.00118475\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[4]\tvalid_0's l2: 0.00158728\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[7]\tvalid_0's l2: 0.00118901\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[25]\tvalid_0's l2: 0.00123465\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's l2: 0.00162371\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's l2: 0.0012202\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[86]\tvalid_0's l2: 0.00117782\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[3]\tvalid_0's l2: 0.00163093\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[2]\tvalid_0's l2: 0.00123101\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[57]\tvalid_0's l2: 0.00118957\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[4]\tvalid_0's l2: 0.00160679\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[15]\tvalid_0's l2: 0.00120132\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[16]\tvalid_0's l2: 0.00120912\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[2]\tvalid_0's l2: 0.00163003\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[7]\tvalid_0's l2: 0.00118301\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[132]\tvalid_0's l2: 0.00119961\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[8]\tvalid_0's l2: 0.00161178\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[45]\tvalid_0's l2: 0.00119462\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[165]\tvalid_0's l2: 0.0012493\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[4]\tvalid_0's l2: 0.00162073\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[160]\tvalid_0's l2: 0.00121526\n",
            "      ‚úÖ RMSE Test: 0.0434\n",
            "      ‚úÖ MAPE Test: 222174878974797.56%\n",
            "      ‚úÖ Dir. Acc: 41.7%\n",
            "      ‚úÖ Hit Rate ¬±2%: 0.0%\n",
            "\n",
            "   ü§ñ Entrenando RegimeSwitching...\n",
            "      ‚ùå Error: Input X contains infinity or a value too large for dtype('float64')....\n",
            "\n",
            "   ü§ñ Entrenando MIDAS...\n",
            "      ‚úÖ RMSE Test: 0.0414\n",
            "      ‚úÖ MAPE Test: 102040105016278.64%\n",
            "      ‚úÖ Dir. Acc: 48.6%\n",
            "      ‚úÖ Hit Rate ¬±2%: 0.0%\n",
            "\n",
            "\n",
            "üìä COMBINACI√ìN: HIBRIDA\n",
            "----------------------------------------\n",
            "   üßπ Datos limpiados: 1447 observaciones v√°lidas de 1447 originales\n",
            "   Train: 1374 observaciones\n",
            "   Test:  73 observaciones\n",
            "\n",
            "   ü§ñ Entrenando ARIMAX-GARCH...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      ‚úÖ RMSE Test: 0.0377\n",
            "      ‚úÖ MAPE Test: 43985185838539.72%\n",
            "      ‚úÖ Dir. Acc: 43.1%\n",
            "      ‚úÖ Hit Rate ¬±2%: 0.0%\n",
            "\n",
            "   ü§ñ Entrenando XGBoost...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "de4d54f601594f04aeb1e15d4179ae2f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      ‚úÖ RMSE Test: 0.0373\n",
            "      ‚úÖ MAPE Test: 2305231672908.27%\n",
            "      ‚úÖ Dir. Acc: 1.4%\n",
            "      ‚úÖ Hit Rate ¬±2%: 0.0%\n",
            "\n",
            "   ü§ñ Entrenando LightGBM...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4d016eb33ff94d2db53d67b869d85623",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[8]\tvalid_0's l2: 0.00124917\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[111]\tvalid_0's l2: 0.00160611\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[182]\tvalid_0's l2: 0.00118609\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[85]\tvalid_0's l2: 0.00125446\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[195]\tvalid_0's l2: 0.00161931\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[192]\tvalid_0's l2: 0.00121781\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[30]\tvalid_0's l2: 0.00124254\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[39]\tvalid_0's l2: 0.00157912\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[19]\tvalid_0's l2: 0.00118182\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[51]\tvalid_0's l2: 0.00123925\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[28]\tvalid_0's l2: 0.001607\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[56]\tvalid_0's l2: 0.00120015\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[28]\tvalid_0's l2: 0.00121704\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[38]\tvalid_0's l2: 0.00158776\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[21]\tvalid_0's l2: 0.00120303\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[180]\tvalid_0's l2: 0.00123586\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[230]\tvalid_0's l2: 0.00159124\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[330]\tvalid_0's l2: 0.00114298\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[4]\tvalid_0's l2: 0.0012215\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[21]\tvalid_0's l2: 0.00148534\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[29]\tvalid_0's l2: 0.00108238\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[90]\tvalid_0's l2: 0.00124809\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[148]\tvalid_0's l2: 0.00161278\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[175]\tvalid_0's l2: 0.00120245\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[10]\tvalid_0's l2: 0.00123013\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[13]\tvalid_0's l2: 0.00160061\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[20]\tvalid_0's l2: 0.0011749\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[120]\tvalid_0's l2: 0.00124568\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[424]\tvalid_0's l2: 0.0015934\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[504]\tvalid_0's l2: 0.00116073\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[148]\tvalid_0's l2: 0.00123605\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[112]\tvalid_0's l2: 0.00161303\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[98]\tvalid_0's l2: 0.00121535\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[100]\tvalid_0's l2: 0.00119642\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[56]\tvalid_0's l2: 0.00158094\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[116]\tvalid_0's l2: 0.00117979\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[109]\tvalid_0's l2: 0.00124196\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[375]\tvalid_0's l2: 0.00156004\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[356]\tvalid_0's l2: 0.00116629\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[8]\tvalid_0's l2: 0.00121439\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[25]\tvalid_0's l2: 0.0015804\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[8]\tvalid_0's l2: 0.0011696\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[276]\tvalid_0's l2: 0.0012364\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[392]\tvalid_0's l2: 0.00159629\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[657]\tvalid_0's l2: 0.00114615\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[30]\tvalid_0's l2: 0.00122779\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[11]\tvalid_0's l2: 0.0016021\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[130]\tvalid_0's l2: 0.00114583\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[111]\tvalid_0's l2: 0.00123794\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[128]\tvalid_0's l2: 0.00159065\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[252]\tvalid_0's l2: 0.00115055\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[30]\tvalid_0's l2: 0.00121181\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[90]\tvalid_0's l2: 0.00157099\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[144]\tvalid_0's l2: 0.00114289\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[26]\tvalid_0's l2: 0.00123785\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[7]\tvalid_0's l2: 0.00160265\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[28]\tvalid_0's l2: 0.00118775\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[9]\tvalid_0's l2: 0.00120056\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[54]\tvalid_0's l2: 0.00156567\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[79]\tvalid_0's l2: 0.00112327\n",
            "      ‚úÖ RMSE Test: 0.0412\n",
            "      ‚úÖ MAPE Test: 124296372791881.55%\n",
            "      ‚úÖ Dir. Acc: 48.6%\n",
            "      ‚úÖ Hit Rate ¬±2%: 0.0%\n",
            "\n",
            "   ü§ñ Entrenando RegimeSwitching...\n",
            "      ‚ùå Error: Input X contains infinity or a value too large for dtype('float64')....\n",
            "\n",
            "   ü§ñ Entrenando MIDAS...\n",
            "      ‚úÖ RMSE Test: 0.0414\n",
            "      ‚úÖ MAPE Test: 159211976238478.06%\n",
            "      ‚úÖ Dir. Acc: 38.9%\n",
            "      ‚úÖ Hit Rate ¬±2%: 1.4%\n",
            "\n",
            "\n",
            "üìä COMBINACI√ìN: REGIME\n",
            "----------------------------------------\n",
            "   üßπ Datos limpiados: 1447 observaciones v√°lidas de 1447 originales\n",
            "   Train: 1374 observaciones\n",
            "   Test:  73 observaciones\n",
            "\n",
            "   ü§ñ Entrenando ARIMAX-GARCH...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
            "  self._init_dates(dates, freq)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      ‚úÖ RMSE Test: 0.0375\n",
            "      ‚úÖ MAPE Test: 32403009800584.91%\n",
            "      ‚úÖ Dir. Acc: 44.4%\n",
            "      ‚úÖ Hit Rate ¬±2%: 0.0%\n",
            "\n",
            "   ü§ñ Entrenando XGBoost...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/franciscojavierriverapaleo/test_gerencia/deacero_env/lib/python3.13/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "282704a0ab094656a06d3deef9763e39",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      ‚úÖ RMSE Test: 0.0373\n",
            "      ‚úÖ MAPE Test: 2319075030716.49%\n",
            "      ‚úÖ Dir. Acc: 1.4%\n",
            "      ‚úÖ Hit Rate ¬±2%: 0.0%\n",
            "\n",
            "   ü§ñ Entrenando LightGBM...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f7f806d5f1fb41aca66f0dbd53141697",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[175]\tvalid_0's l2: 0.001245\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[5]\tvalid_0's l2: 0.00162065\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[2]\tvalid_0's l2: 0.00121912\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[36]\tvalid_0's l2: 0.00120772\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[29]\tvalid_0's l2: 0.00160738\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[2]\tvalid_0's l2: 0.00121921\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[27]\tvalid_0's l2: 0.0012366\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's l2: 0.00162178\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[9]\tvalid_0's l2: 0.00121358\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[26]\tvalid_0's l2: 0.00124983\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[9]\tvalid_0's l2: 0.00161059\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[3]\tvalid_0's l2: 0.0012176\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[5]\tvalid_0's l2: 0.00123429\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's l2: 0.00163228\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's l2: 0.00122297\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[49]\tvalid_0's l2: 0.00122075\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[72]\tvalid_0's l2: 0.00161506\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[6]\tvalid_0's l2: 0.00121015\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[5]\tvalid_0's l2: 0.00125116\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[53]\tvalid_0's l2: 0.00156332\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[7]\tvalid_0's l2: 0.00120772\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[11]\tvalid_0's l2: 0.00125372\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's l2: 0.00162112\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's l2: 0.00121912\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[133]\tvalid_0's l2: 0.00120944\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[8]\tvalid_0's l2: 0.00161246\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[10]\tvalid_0's l2: 0.00121704\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[36]\tvalid_0's l2: 0.0012431\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[23]\tvalid_0's l2: 0.00161089\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[34]\tvalid_0's l2: 0.00121417\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[395]\tvalid_0's l2: 0.00124039\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[13]\tvalid_0's l2: 0.00162106\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[393]\tvalid_0's l2: 0.00121516\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[16]\tvalid_0's l2: 0.00118571\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[7]\tvalid_0's l2: 0.00161562\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's l2: 0.00122102\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[16]\tvalid_0's l2: 0.00118234\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[7]\tvalid_0's l2: 0.00160668\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's l2: 0.00122076\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[24]\tvalid_0's l2: 0.00115593\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[6]\tvalid_0's l2: 0.00160758\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's l2: 0.00121912\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[16]\tvalid_0's l2: 0.00118225\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[4]\tvalid_0's l2: 0.00161777\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[16]\tvalid_0's l2: 0.00121289\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[35]\tvalid_0's l2: 0.0011975\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's l2: 0.00162112\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's l2: 0.00121912\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[269]\tvalid_0's l2: 0.00125166\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[15]\tvalid_0's l2: 0.00162101\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[4]\tvalid_0's l2: 0.00121911\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[16]\tvalid_0's l2: 0.00121621\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[4]\tvalid_0's l2: 0.00161648\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's l2: 0.00121912\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[265]\tvalid_0's l2: 0.00123661\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's l2: 0.00162112\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's l2: 0.00121912\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[31]\tvalid_0's l2: 0.00119938\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[10]\tvalid_0's l2: 0.00160858\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[23]\tvalid_0's l2: 0.00121614\n",
            "      ‚úÖ RMSE Test: 0.0381\n",
            "      ‚úÖ MAPE Test: 13237267069178.63%\n",
            "      ‚úÖ Dir. Acc: 52.8%\n",
            "      ‚úÖ Hit Rate ¬±2%: 0.0%\n",
            "\n",
            "   ü§ñ Entrenando RegimeSwitching...\n",
            "      ‚ùå Error: Input X contains infinity or a value too large for dtype('float64')....\n",
            "\n",
            "   ü§ñ Entrenando MIDAS...\n",
            "      ‚úÖ RMSE Test: 0.0406\n",
            "      ‚úÖ MAPE Test: 45601642322211.01%\n",
            "      ‚úÖ Dir. Acc: 48.6%\n",
            "      ‚úÖ Hit Rate ¬±2%: 0.0%\n",
            "\n",
            "\n",
            "============================================================\n",
            "‚úÖ A/B TESTING COMPLETADO\n",
            "   Total de modelos entrenados: 15\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Ejecutar todos los modelos con todas las combinaciones de variables\n",
        "\n",
        "# Dividir datos en train/test (80/20 del per√≠odo total)\n",
        "def split_time_series_data(X, y, test_size=0.05):\n",
        "    \"\"\"Divide datos de series temporales manteniendo orden temporal\"\"\"\n",
        "    n_samples = len(X)\n",
        "    split_idx = int(n_samples * (1 - test_size))\n",
        "    \n",
        "    X_train = X.iloc[:split_idx]\n",
        "    X_test = X.iloc[split_idx:]\n",
        "    y_train = y.iloc[:split_idx]\n",
        "    y_test = y.iloc[split_idx:]\n",
        "    \n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "# Funci√≥n para limpiar datos de valores infinitos y NaN\n",
        "def clean_data_for_training(X, y):\n",
        "    \"\"\"Limpia datos de valores infinitos, NaN y valores extremos\"\"\"\n",
        "    # Crear copias para no modificar originales\n",
        "    X_clean = X.copy()\n",
        "    y_clean = y.copy()\n",
        "    \n",
        "    # Reemplazar infinitos con NaN\n",
        "    X_clean = X_clean.replace([np.inf, -np.inf], np.nan)\n",
        "    \n",
        "    # Identificar filas con NaN en X o y\n",
        "    mask_x = ~X_clean.isnull().any(axis=1)\n",
        "    mask_y = ~y_clean.isnull()\n",
        "    mask_combined = mask_x & mask_y\n",
        "    \n",
        "    # Filtrar datos\n",
        "    X_clean = X_clean[mask_combined]\n",
        "    y_clean = y_clean[mask_combined]\n",
        "    \n",
        "    # Detectar y manejar valores extremos (outliers)\n",
        "    for col in X_clean.select_dtypes(include=[np.number]).columns:\n",
        "        Q1 = X_clean[col].quantile(0.01)\n",
        "        Q3 = X_clean[col].quantile(0.99)\n",
        "        IQR = Q3 - Q1\n",
        "        lower_bound = Q1 - 3 * IQR\n",
        "        upper_bound = Q3 + 3 * IQR\n",
        "        \n",
        "        # Clip valores extremos\n",
        "        X_clean[col] = X_clean[col].clip(lower_bound, upper_bound)\n",
        "    \n",
        "    # Verificar que no hay valores infinitos o NaN restantes\n",
        "    assert not X_clean.isnull().any().any(), \"A√∫n hay valores NaN en X\"\n",
        "    assert not y_clean.isnull().any(), \"A√∫n hay valores NaN en y\"\n",
        "    assert not np.isinf(X_clean.values).any(), \"A√∫n hay valores infinitos en X\"\n",
        "    assert not np.isinf(y_clean.values).any(), \"A√∫n hay valores infinitos en y\"\n",
        "    \n",
        "    print(f\"   üßπ Datos limpiados: {len(X_clean)} observaciones v√°lidas de {len(X)} originales\")\n",
        "    \n",
        "    return X_clean, y_clean\n",
        "\n",
        "# Almacenar todos los resultados\n",
        "all_results = []\n",
        "\n",
        "# Lista de modelos a entrenar\n",
        "models_to_train = [\n",
        "    ('ARIMAX-GARCH', train_arimax_garch_model),\n",
        "    ('XGBoost', train_xgboost_model),\n",
        "    ('LightGBM', train_lightgbm_model),\n",
        "    ('RegimeSwitching', train_regime_switching_model),\n",
        "    ('MIDAS', train_midas_model)\n",
        "]\n",
        "\n",
        "print(\"üöÄ INICIANDO A/B TESTING DE 15 MODELOS\\n\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Iterar sobre cada combinaci√≥n de variables\n",
        "for combo_name, combo_data in prepared_data.items():\n",
        "    print(f\"\\nüìä COMBINACI√ìN: {combo_name.upper()}\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    X = combo_data['X']\n",
        "    y = combo_data['y']\n",
        "    \n",
        "    # Limpiar datos antes del split\n",
        "    try:\n",
        "        X_clean, y_clean = clean_data_for_training(X, y)\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Error limpiando datos: {str(e)}\")\n",
        "        continue\n",
        "    \n",
        "    # Split train/test con datos limpios\n",
        "    X_train, X_test, y_train, y_test = split_time_series_data(X_clean, y_clean)\n",
        "    \n",
        "    print(f\"   Train: {X_train.shape[0]} observaciones\")\n",
        "    print(f\"   Test:  {X_test.shape[0]} observaciones\\n\")\n",
        "    \n",
        "    # Entrenar cada modelo\n",
        "    for model_name, train_function in models_to_train:\n",
        "        print(f\"   ü§ñ Entrenando {model_name}...\")\n",
        "        \n",
        "        try:\n",
        "            # Verificaci√≥n adicional antes de entrenar\n",
        "            if np.isinf(X_train.values).any() or np.isinf(X_test.values).any():\n",
        "                raise ValueError(f\"Valores infinitos detectados en datos de entrada para {model_name}\")\n",
        "            \n",
        "            if np.isinf(y_train.values).any() or np.isinf(y_test.values).any():\n",
        "                raise ValueError(f\"Valores infinitos detectados en variable objetivo para {model_name}\")\n",
        "            \n",
        "            # Ajustar par√°metros seg√∫n el modelo\n",
        "            if model_name == 'XGBoost':\n",
        "                result = train_function(X_train, y_train, X_test, y_test, combo_name, n_trials=20)\n",
        "            elif model_name == 'LightGBM':\n",
        "                result = train_function(X_train, y_train, X_test, y_test, combo_name, n_trials=20)\n",
        "            elif model_name == 'MIDAS':\n",
        "                result = train_function(X_train, y_train, X_test, y_test, combo_name, monthly_data)\n",
        "            else:\n",
        "                result = train_function(X_train, y_train, X_test, y_test, combo_name)\n",
        "            \n",
        "            # Guardar resultados\n",
        "            all_results.append({\n",
        "                'model': model_name,\n",
        "                'combination': combo_name,\n",
        "                'train_rmse': result['train_metrics']['rmse'],\n",
        "                'test_rmse': result['test_metrics']['rmse'],\n",
        "                'train_mae': result['train_metrics']['mae'],\n",
        "                'test_mae': result['test_metrics']['mae'],\n",
        "                'train_mape': result['train_metrics']['mape'],\n",
        "                'test_mape': result['test_metrics']['mape'],\n",
        "                'train_r2': result['train_metrics']['r2'],\n",
        "                'test_r2': result['test_metrics']['r2'],\n",
        "                'directional_accuracy': result['test_metrics']['directional_accuracy'],\n",
        "                'hit_rate_2pct': result['test_metrics']['hit_rate_2pct'],\n",
        "                'hit_rate_5pct': result['test_metrics']['hit_rate_5pct'],\n",
        "                'full_result': result\n",
        "            })\n",
        "            \n",
        "            print(f\"      ‚úÖ RMSE Test: {result['test_metrics']['rmse']:.4f}\")\n",
        "            print(f\"      ‚úÖ MAPE Test: {result['test_metrics']['mape']:.2f}%\")\n",
        "            print(f\"      ‚úÖ Dir. Acc: {result['test_metrics']['directional_accuracy']:.1f}%\")\n",
        "            print(f\"      ‚úÖ Hit Rate ¬±2%: {result['test_metrics']['hit_rate_2pct']:.1f}%\\n\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"      ‚ùå Error: {str(e)[:100]}...\\n\")\n",
        "            \n",
        "            # Guardar resultado con error\n",
        "            all_results.append({\n",
        "                'model': model_name,\n",
        "                'combination': combo_name,\n",
        "                'error': str(e),\n",
        "                'train_rmse': np.nan,\n",
        "                'test_rmse': np.nan,\n",
        "                'train_mae': np.nan,\n",
        "                'test_mae': np.nan,\n",
        "                'train_mape': np.nan,\n",
        "                'test_mape': np.nan,\n",
        "                'train_r2': np.nan,\n",
        "                'test_r2': np.nan,\n",
        "                'directional_accuracy': np.nan,\n",
        "                'hit_rate_2pct': np.nan,\n",
        "                'hit_rate_5pct': np.nan\n",
        "            })\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"‚úÖ A/B TESTING COMPLETADO\")\n",
        "print(f\"   Total de modelos entrenados: {len(all_results)}\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç DIAGN√ìSTICO DEL PROBLEMA DE PREDICCI√ìN\n",
            "============================================================\n",
            "\n",
            "üìä Combinaci√≥n: fundamental\n",
            "   Estad√≠sticas de y (retornos logar√≠tmicos):\n",
            "   - Media: 0.000091\n",
            "   - Std: 0.042413\n",
            "   - Min: -0.144173\n",
            "   - Max: 0.140671\n",
            "   - Mediana: 0.000042\n",
            "   - Outliers: 6 (0.4%)\n",
            "   - Ljung-Box p-value (lag 10): 0.0000\n",
            "\n",
            "üìà AN√ÅLISIS DE ESCALA:\n",
            "   Los retornos logar√≠tmicos t√≠picamente son muy peque√±os (~0.001 - 0.01)\n",
            "   Esto puede causar problemas en los modelos si no se escalan apropiadamente\n",
            "\n",
            "üí° COMPARACI√ìN PRECIO ORIGINAL vs RETORNOS:\n",
            "   Precio promedio original: $496.74\n",
            "   Volatilidad precio original: $29.71\n",
            "   Coef. Variaci√≥n precios: 0.060\n",
            "\n",
            "   Retorno simple promedio: 0.000976\n",
            "   Volatilidad retorno simple: 0.042454\n",
            "   Retorno anualizado: 24.61%\n"
          ]
        }
      ],
      "source": [
        "# DIAGN√ìSTICO: Analizar las predicciones\n",
        "\n",
        "print(\"üîç DIAGN√ìSTICO DEL PROBLEMA DE PREDICCI√ìN\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Verificar la distribuci√≥n de la variable objetivo\n",
        "if prepared_data:\n",
        "    for combo_name, combo_data in prepared_data.items():\n",
        "        y = combo_data['y']\n",
        "        print(f\"\\nüìä Combinaci√≥n: {combo_name}\")\n",
        "        print(f\"   Estad√≠sticas de y (retornos logar√≠tmicos):\")\n",
        "        print(f\"   - Media: {y.mean():.6f}\")\n",
        "        print(f\"   - Std: {y.std():.6f}\")\n",
        "        print(f\"   - Min: {y.min():.6f}\")\n",
        "        print(f\"   - Max: {y.max():.6f}\")\n",
        "        print(f\"   - Mediana: {y.median():.6f}\")\n",
        "        \n",
        "        # Verificar si hay valores extremos\n",
        "        q1 = y.quantile(0.25)\n",
        "        q3 = y.quantile(0.75)\n",
        "        iqr = q3 - q1\n",
        "        outliers = ((y < (q1 - 1.5 * iqr)) | (y > (q3 + 1.5 * iqr))).sum()\n",
        "        print(f\"   - Outliers: {outliers} ({outliers/len(y)*100:.1f}%)\")\n",
        "        \n",
        "        # Verificar autocorrelaci√≥n\n",
        "        from statsmodels.stats.diagnostic import acorr_ljungbox\n",
        "        lb_test = acorr_ljungbox(y, lags=10, return_df=True)\n",
        "        print(f\"   - Ljung-Box p-value (lag 10): {lb_test['lb_pvalue'].iloc[-1]:.4f}\")\n",
        "        \n",
        "        break  # Solo analizar la primera combinaci√≥n como ejemplo\n",
        "\n",
        "# Verificar si el problema es la escala de los retornos\n",
        "print(\"\\nüìà AN√ÅLISIS DE ESCALA:\")\n",
        "print(f\"   Los retornos logar√≠tmicos t√≠picamente son muy peque√±os (~0.001 - 0.01)\")\n",
        "print(f\"   Esto puede causar problemas en los modelos si no se escalan apropiadamente\")\n",
        "\n",
        "# Verificar los precios originales vs transformados\n",
        "print(\"\\nüí° COMPARACI√ìN PRECIO ORIGINAL vs RETORNOS:\")\n",
        "original_prices = daily_features[TARGET_VAR].dropna()\n",
        "print(f\"   Precio promedio original: ${original_prices.mean():.2f}\")\n",
        "print(f\"   Volatilidad precio original: ${original_prices.std():.2f}\")\n",
        "print(f\"   Coef. Variaci√≥n precios: {original_prices.std()/original_prices.mean():.3f}\")\n",
        "\n",
        "# Calcular retornos simples para comparar\n",
        "simple_returns = original_prices.pct_change().dropna()\n",
        "print(f\"\\n   Retorno simple promedio: {simple_returns.mean():.6f}\")\n",
        "print(f\"   Volatilidad retorno simple: {simple_returns.std():.6f}\")\n",
        "print(f\"   Retorno anualizado: {simple_returns.mean() * 252:.2%}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä TABLA DE RESULTADOS - TOP 10 MODELOS\n",
            "====================================================================================================\n",
            "           model  combination  test_rmse  test_mae     test_mape   test_r2  directional_accuracy  hit_rate_2pct\n",
            "11       XGBoost       regime   0.037287  0.029024  2.319075e+12 -0.000005              1.388889            0.0\n",
            "6        XGBoost      hibrida   0.037287  0.029024  2.305232e+12 -0.000005              1.388889            0.0\n",
            "1        XGBoost  fundamental   0.037287  0.029024  2.252845e+12 -0.000006              1.388889            0.0\n",
            "0   ARIMAX-GARCH  fundamental   0.037318  0.029067  4.805251e+11 -0.001684             40.277778            0.0\n",
            "10  ARIMAX-GARCH       regime   0.037485  0.029317  3.240301e+13 -0.010670             44.444444            0.0\n",
            "5   ARIMAX-GARCH      hibrida   0.037729  0.029575  4.398519e+13 -0.023842             43.055556            0.0\n",
            "12      LightGBM       regime   0.038103  0.029245  1.323727e+13 -0.044276             52.777778            0.0\n",
            "14         MIDAS       regime   0.040631  0.032050  4.560164e+13 -0.187401             48.611111            0.0\n",
            "7       LightGBM      hibrida   0.041210  0.034572  1.242964e+14 -0.221518             48.611111            0.0\n",
            "4          MIDAS  fundamental   0.041362  0.032333  1.020401e+14 -0.230548             48.611111            0.0\n",
            "\n",
            "\n",
            "üìà ESTAD√çSTICAS POR TIPO DE MODELO\n",
            "============================================================\n",
            "                test_rmse                     test_mape                \\\n",
            "                     mean     std     min          mean           std   \n",
            "model                                                                   \n",
            "ARIMAX-GARCH       0.0375  0.0002  0.0373  2.562291e+13  2.253089e+13   \n",
            "LightGBM           0.0409  0.0027  0.0381  1.199028e+14  1.045381e+14   \n",
            "MIDAS              0.0411  0.0004  0.0406  1.022846e+14  5.680556e+13   \n",
            "RegimeSwitching       NaN     NaN     NaN           NaN           NaN   \n",
            "XGBoost            0.0373  0.0000  0.0373  2.292384e+12  3.493403e+10   \n",
            "\n",
            "                              directional_accuracy          hit_rate_2pct  \\\n",
            "                          min                 mean      max          mean   \n",
            "model                                                                       \n",
            "ARIMAX-GARCH     4.805251e+11              42.5926  44.4444        0.0000   \n",
            "LightGBM         1.323727e+13              47.6852  52.7778        0.0000   \n",
            "MIDAS            4.560164e+13              45.3704  48.6111        0.4566   \n",
            "RegimeSwitching           NaN                  NaN      NaN           NaN   \n",
            "XGBoost          2.252845e+12               1.3889   1.3889        0.0000   \n",
            "\n",
            "                         \n",
            "                    max  \n",
            "model                    \n",
            "ARIMAX-GARCH     0.0000  \n",
            "LightGBM         0.0000  \n",
            "MIDAS            1.3699  \n",
            "RegimeSwitching     NaN  \n",
            "XGBoost          0.0000  \n",
            "\n",
            "\n",
            "üìà ESTAD√çSTICAS POR COMBINACI√ìN DE VARIABLES\n",
            "============================================================\n",
            "            test_rmse                     test_mape                \\\n",
            "                 mean     std     min          mean           std   \n",
            "combination                                                         \n",
            "fundamental    0.0398  0.0031  0.0373  8.173709e+13  1.049688e+14   \n",
            "hibrida        0.0394  0.0022  0.0373  8.244969e+13  7.198668e+13   \n",
            "regime         0.0384  0.0015  0.0373  2.339025e+13  1.933609e+13   \n",
            "\n",
            "                          directional_accuracy          hit_rate_2pct          \n",
            "                      min                 mean      max          mean     max  \n",
            "combination                                                                    \n",
            "fundamental  4.805251e+11              32.9861  48.6111        0.0000  0.0000  \n",
            "hibrida      2.305232e+12              32.9861  48.6111        0.3425  1.3699  \n",
            "regime       2.319075e+12              36.8056  52.7778        0.0000  0.0000  \n"
          ]
        }
      ],
      "source": [
        "# Crear DataFrame con resultados para an√°lisis\n",
        "\n",
        "results_df = pd.DataFrame(all_results)\n",
        "\n",
        "# Eliminar columna 'full_result' para visualizaci√≥n\n",
        "if 'full_result' in results_df.columns:\n",
        "    results_df_display = results_df.drop('full_result', axis=1)\n",
        "else:\n",
        "    results_df_display = results_df\n",
        "\n",
        "# Ordenar por RMSE de test\n",
        "results_df_display = results_df_display.sort_values('test_rmse')\n",
        "\n",
        "print(\"üìä TABLA DE RESULTADOS - TOP 10 MODELOS\")\n",
        "print(\"=\" * 100)\n",
        "\n",
        "# Mostrar top 10 modelos\n",
        "print(results_df_display[['model', 'combination', 'test_rmse', 'test_mae', 'test_mape', 'test_r2', \n",
        "                          'directional_accuracy', 'hit_rate_2pct']].head(10).to_string())\n",
        "\n",
        "# Estad√≠sticas por tipo de modelo\n",
        "print(\"\\n\\nüìà ESTAD√çSTICAS POR TIPO DE MODELO\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "model_stats = results_df_display.groupby('model').agg({\n",
        "    'test_rmse': ['mean', 'std', 'min'],\n",
        "    'test_mape': ['mean', 'std', 'min'],\n",
        "    'directional_accuracy': ['mean', 'max'],\n",
        "    'hit_rate_2pct': ['mean', 'max']\n",
        "}).round(4)\n",
        "\n",
        "print(model_stats)\n",
        "\n",
        "# Estad√≠sticas por combinaci√≥n de variables\n",
        "print(\"\\n\\nüìà ESTAD√çSTICAS POR COMBINACI√ìN DE VARIABLES\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "combo_stats = results_df_display.groupby('combination').agg({\n",
        "    'test_rmse': ['mean', 'std', 'min'],\n",
        "    'test_mape': ['mean', 'std', 'min'],\n",
        "    'directional_accuracy': ['mean', 'max'],\n",
        "    'hit_rate_2pct': ['mean', 'max']\n",
        "}).round(4)\n",
        "\n",
        "print(combo_stats)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## M√©tricas que dan pena!!. Intentemos con otra aproximaci√≥n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß VERSI√ìN 2.0: Modelos Mejorados con Diferentes Enfoques\n",
        "\n",
        "### Cambios principales:\n",
        "1. **Predecir directamente el precio** (no retornos)\n",
        "2. **Usar escalamiento robusto** (RobustScaler)\n",
        "3. **Incluir m√°s features de contexto**\n",
        "4. **Validaci√≥n m√°s estricta**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì¶ Preparando datos con M√âTODO MEJORADO (predecir precio directamente)...\n",
            "\n",
            "Procesando fundamental...\n",
            "   ‚úÖ Shape: X=(1429, 13), y=(1429,)\n",
            "   Rango de y (precios): $408.50 - $590.70\n",
            "   Media de y: $497.30\n",
            "Procesando hibrida...\n",
            "   ‚úÖ Shape: X=(1429, 12), y=(1429,)\n",
            "   Rango de y (precios): $408.50 - $590.70\n",
            "   Media de y: $497.30\n",
            "Procesando regime...\n",
            "   ‚úÖ Shape: X=(1429, 13), y=(1429,)\n",
            "   Rango de y (precios): $408.50 - $590.70\n",
            "   Media de y: $497.30\n",
            "\n",
            "‚úÖ Datos V2 preparados\n"
          ]
        }
      ],
      "source": [
        "# VERSI√ìN MEJORADA: Preparaci√≥n de datos con diferentes enfoques\n",
        "\n",
        "def prepare_data_v2(df, target_var, feature_vars, forecast_horizon=1, method='price'):\n",
        "    \"\"\"\n",
        "    Versi√≥n mejorada de preparaci√≥n de datos con m√∫ltiples m√©todos\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    method : str\n",
        "        'price': Predecir directamente el precio (con escalamiento)\n",
        "        'returns': Predecir retornos simples\n",
        "        'log_returns': Predecir retornos logar√≠tmicos\n",
        "        'diff': Predecir primera diferencia\n",
        "    \"\"\"\n",
        "    \n",
        "    # Seleccionar features\n",
        "    X = df[feature_vars].copy()\n",
        "    \n",
        "    # Variable objetivo: precio en t+forecast_horizon\n",
        "    y_future = df[target_var].shift(-forecast_horizon)\n",
        "    \n",
        "    # Eliminar NaN iniciales\n",
        "    valid_idx = ~(X.isna().any(axis=1) | y_future.isna())\n",
        "    X = X[valid_idx]\n",
        "    y_future = y_future[valid_idx]\n",
        "    y_current = df[target_var][valid_idx]\n",
        "    \n",
        "    # Preparar features (siempre usar cambios/ratios para estacionariedad)\n",
        "    X_processed = pd.DataFrame(index=X.index)\n",
        "    \n",
        "    for col in X.columns:\n",
        "        if col.endswith('_lag_1'):\n",
        "            # Mantener lags como est√°n\n",
        "            X_processed[col] = X[col]\n",
        "        elif 'volatility' in col or 'rsi' in col or 'bb_' in col:\n",
        "            # Indicadores t√©cnicos mantener como est√°n\n",
        "            X_processed[col] = X[col]\n",
        "        elif col in ['VIX', 'tasa_interes_banxico', 'tiie_28_dias']:\n",
        "            # Tasas y volatilidad en niveles\n",
        "            X_processed[col] = X[col]\n",
        "        else:\n",
        "            # Para precios/commodities usar retornos\n",
        "            X_processed[f'{col}_return'] = X[col].pct_change()\n",
        "            X_processed[f'{col}_ma_ratio'] = X[col] / X[col].rolling(20).mean()\n",
        "    \n",
        "    # Preparar variable objetivo seg√∫n el m√©todo\n",
        "    if method == 'price':\n",
        "        # Predecir directamente el precio\n",
        "        y = y_future\n",
        "        # Agregar el precio actual como feature importante\n",
        "        X_processed['current_price'] = y_current\n",
        "        X_processed['price_ma20'] = y_current.rolling(20).mean()\n",
        "        X_processed['price_std20'] = y_current.rolling(20).std()\n",
        "        \n",
        "    elif method == 'returns':\n",
        "        # Predecir retorno simple\n",
        "        y = (y_future - y_current) / y_current\n",
        "        \n",
        "    elif method == 'log_returns':\n",
        "        # Predecir retorno logar√≠tmico\n",
        "        y = np.log(y_future / y_current)\n",
        "        \n",
        "    elif method == 'diff':\n",
        "        # Predecir primera diferencia\n",
        "        y = y_future - y_current\n",
        "        X_processed['current_price'] = y_current\n",
        "        \n",
        "    else:\n",
        "        raise ValueError(f\"M√©todo no reconocido: {method}\")\n",
        "    \n",
        "    # Eliminar filas con NaN o infinitos\n",
        "    valid_final = ~(X_processed.isna().any(axis=1) | y.isna() | \n",
        "                    np.isinf(X_processed).any(axis=1) | np.isinf(y))\n",
        "    \n",
        "    X_clean = X_processed[valid_final]\n",
        "    y_clean = y[valid_final]\n",
        "    \n",
        "    # Guardar informaci√≥n para transformaci√≥n inversa\n",
        "    transform_info = {\n",
        "        'method': method,\n",
        "        'current_prices': y_current[valid_final],\n",
        "        'feature_names': list(X_clean.columns)\n",
        "    }\n",
        "    \n",
        "    return X_clean, y_clean, transform_info\n",
        "\n",
        "# Preparar datos con diferentes m√©todos\n",
        "print(\"üì¶ Preparando datos con M√âTODO MEJORADO (predecir precio directamente)...\\n\")\n",
        "\n",
        "prepared_data_v2 = {}\n",
        "for combo_name, vars_list in variable_combinations.items():\n",
        "    print(f\"Procesando {combo_name}...\")\n",
        "    try:\n",
        "        # Usar m√©todo 'price' para predecir directamente el precio\n",
        "        X, y, info = prepare_data_v2(daily_features, TARGET_VAR, vars_list, method='price')\n",
        "        prepared_data_v2[combo_name] = {'X': X, 'y': y, 'info': info}\n",
        "        print(f\"   ‚úÖ Shape: X={X.shape}, y={y.shape}\")\n",
        "        print(f\"   Rango de y (precios): ${y.min():.2f} - ${y.max():.2f}\")\n",
        "        print(f\"   Media de y: ${y.mean():.2f}\")\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Error: {e}\")\n",
        "\n",
        "print(\"\\n‚úÖ Datos V2 preparados\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Modelos V2 definidos con escalamiento robusto\n"
          ]
        }
      ],
      "source": [
        "# MODELOS MEJORADOS V2: Con escalamiento y predicci√≥n directa de precios\n",
        "\n",
        "def train_xgboost_v2(X_train, y_train, X_test, y_test, combo_name, n_trials=30):\n",
        "    \"\"\"\n",
        "    XGBoost mejorado con escalamiento y predicci√≥n directa de precios\n",
        "    \"\"\"\n",
        "    from sklearn.preprocessing import RobustScaler\n",
        "    \n",
        "    # Escalar features y target\n",
        "    scaler_X = RobustScaler()\n",
        "    scaler_y = RobustScaler()\n",
        "    \n",
        "    X_train_scaled = scaler_X.fit_transform(X_train)\n",
        "    X_test_scaled = scaler_X.transform(X_test)\n",
        "    \n",
        "    y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1)).ravel()\n",
        "    y_test_original = y_test.values  # Guardar valores originales para m√©tricas\n",
        "    \n",
        "    def objective(trial):\n",
        "        params = {\n",
        "            'n_estimators': trial.suggest_int('n_estimators', 200, 1500),\n",
        "            'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
        "            'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.3, log=True),\n",
        "            'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
        "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
        "            'min_child_weight': trial.suggest_int('min_child_weight', 1, 15),\n",
        "            'gamma': trial.suggest_float('gamma', 0, 0.5),\n",
        "            'reg_alpha': trial.suggest_float('reg_alpha', 0, 2.0),\n",
        "            'reg_lambda': trial.suggest_float('reg_lambda', 0, 2.0),\n",
        "            'random_state': 42,\n",
        "            'objective': 'reg:squarederror',\n",
        "            'n_jobs': -1\n",
        "        }\n",
        "        \n",
        "        # Validaci√≥n cruzada temporal\n",
        "        tscv_inner = TimeSeriesCV(n_splits=3, train_size=400, test_size=50)\n",
        "        cv_scores = []\n",
        "        \n",
        "        for train_idx, val_idx in tscv_inner.split(X_train_scaled):\n",
        "            X_cv_train = X_train_scaled[train_idx]\n",
        "            y_cv_train = y_train_scaled[train_idx]\n",
        "            X_cv_val = X_train_scaled[val_idx]\n",
        "            y_cv_val = y_train_scaled[val_idx]\n",
        "            \n",
        "            model = xgb.XGBRegressor(**params)\n",
        "            model.fit(X_cv_train, y_cv_train)\n",
        "            \n",
        "            # Predecir y desescalar\n",
        "            y_pred_scaled = model.predict(X_cv_val)\n",
        "            y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).ravel()\n",
        "            y_true = scaler_y.inverse_transform(y_cv_val.reshape(-1, 1)).ravel()\n",
        "            \n",
        "            # RMSE en escala original\n",
        "            rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "            cv_scores.append(rmse)\n",
        "        \n",
        "        return np.mean(cv_scores)\n",
        "    \n",
        "    # Optimizaci√≥n con Optuna\n",
        "    study = optuna.create_study(direction='minimize', study_name=f'xgboost_v2_{combo_name}')\n",
        "    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
        "    \n",
        "    # Entrenar modelo final\n",
        "    best_params = study.best_params\n",
        "    best_params['random_state'] = 42\n",
        "    best_params['objective'] = 'reg:squarederror'\n",
        "    best_params['n_jobs'] = -1\n",
        "    \n",
        "    final_model = xgb.XGBRegressor(**best_params)\n",
        "    final_model.fit(X_train_scaled, y_train_scaled)\n",
        "    \n",
        "    # Predicciones\n",
        "    y_pred_train_scaled = final_model.predict(X_train_scaled)\n",
        "    y_pred_test_scaled = final_model.predict(X_test_scaled)\n",
        "    \n",
        "    # Desescalar predicciones\n",
        "    y_pred_train = scaler_y.inverse_transform(y_pred_train_scaled.reshape(-1, 1)).ravel()\n",
        "    y_pred_test = scaler_y.inverse_transform(y_pred_test_scaled.reshape(-1, 1)).ravel()\n",
        "    \n",
        "    # M√©tricas en escala original (precios)\n",
        "    train_metrics = evaluate_model(y_train.values, y_pred_train, f\"XGBoost_v2_{combo_name}_train\")\n",
        "    test_metrics = evaluate_model(y_test_original, y_pred_test, f\"XGBoost_v2_{combo_name}_test\")\n",
        "    \n",
        "    # Feature importance\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'feature': X_train.columns,\n",
        "        'importance': final_model.feature_importances_\n",
        "    }).sort_values('importance', ascending=False)\n",
        "    \n",
        "    return {\n",
        "        'model': final_model,\n",
        "        'scalers': {'X': scaler_X, 'y': scaler_y},\n",
        "        'best_params': best_params,\n",
        "        'train_metrics': train_metrics,\n",
        "        'test_metrics': test_metrics,\n",
        "        'predictions': {'train': y_pred_train, 'test': y_pred_test},\n",
        "        'feature_importance': feature_importance\n",
        "    }\n",
        "\n",
        "def train_lightgbm_v2(X_train, y_train, X_test, y_test, combo_name, n_trials=30):\n",
        "    \"\"\"\n",
        "    LightGBM mejorado con escalamiento y predicci√≥n directa de precios\n",
        "    \"\"\"\n",
        "    from sklearn.preprocessing import RobustScaler\n",
        "    \n",
        "    # Escalar features y target\n",
        "    scaler_X = RobustScaler()\n",
        "    scaler_y = RobustScaler()\n",
        "    \n",
        "    X_train_scaled = scaler_X.fit_transform(X_train)\n",
        "    X_test_scaled = scaler_X.transform(X_test)\n",
        "    \n",
        "    y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1)).ravel()\n",
        "    y_test_original = y_test.values\n",
        "    \n",
        "    def objective(trial):\n",
        "        params = {\n",
        "            'n_estimators': trial.suggest_int('n_estimators', 200, 1500),\n",
        "            'num_leaves': trial.suggest_int('num_leaves', 20, 200),\n",
        "            'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
        "            'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.3, log=True),\n",
        "            'feature_fraction': trial.suggest_float('feature_fraction', 0.6, 1.0),\n",
        "            'bagging_fraction': trial.suggest_float('bagging_fraction', 0.6, 1.0),\n",
        "            'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n",
        "            'min_child_samples': trial.suggest_int('min_child_samples', 5, 50),\n",
        "            'reg_alpha': trial.suggest_float('reg_alpha', 0, 2.0),\n",
        "            'reg_lambda': trial.suggest_float('reg_lambda', 0, 2.0),\n",
        "            'random_state': 42,\n",
        "            'objective': 'regression',\n",
        "            'metric': 'rmse',\n",
        "            'n_jobs': -1,\n",
        "            'verbosity': -1\n",
        "        }\n",
        "        \n",
        "        # Validaci√≥n cruzada\n",
        "        model = lgb.LGBMRegressor(**params)\n",
        "        \n",
        "        # Train con early stopping\n",
        "        model.fit(X_train_scaled, y_train_scaled,\n",
        "                 eval_set=[(X_test_scaled, scaler_y.transform(y_test.values.reshape(-1, 1)).ravel())],\n",
        "                 callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)])\n",
        "        \n",
        "        # Evaluar\n",
        "        y_pred_scaled = model.predict(X_test_scaled)\n",
        "        y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).ravel()\n",
        "        rmse = np.sqrt(mean_squared_error(y_test_original, y_pred))\n",
        "        \n",
        "        return rmse\n",
        "    \n",
        "    # Optimizaci√≥n\n",
        "    study = optuna.create_study(direction='minimize', study_name=f'lightgbm_v2_{combo_name}')\n",
        "    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
        "    \n",
        "    # Modelo final\n",
        "    best_params = study.best_params\n",
        "    best_params.update({\n",
        "        'random_state': 42,\n",
        "        'objective': 'regression',\n",
        "        'metric': 'rmse',\n",
        "        'n_jobs': -1,\n",
        "        'verbosity': -1\n",
        "    })\n",
        "    \n",
        "    final_model = lgb.LGBMRegressor(**best_params)\n",
        "    final_model.fit(X_train_scaled, y_train_scaled)\n",
        "    \n",
        "    # Predicciones\n",
        "    y_pred_train_scaled = final_model.predict(X_train_scaled)\n",
        "    y_pred_test_scaled = final_model.predict(X_test_scaled)\n",
        "    \n",
        "    # Desescalar\n",
        "    y_pred_train = scaler_y.inverse_transform(y_pred_train_scaled.reshape(-1, 1)).ravel()\n",
        "    y_pred_test = scaler_y.inverse_transform(y_pred_test_scaled.reshape(-1, 1)).ravel()\n",
        "    \n",
        "    # M√©tricas\n",
        "    train_metrics = evaluate_model(y_train.values, y_pred_train, f\"LightGBM_v2_{combo_name}_train\")\n",
        "    test_metrics = evaluate_model(y_test_original, y_pred_test, f\"LightGBM_v2_{combo_name}_test\")\n",
        "    \n",
        "    return {\n",
        "        'model': final_model,\n",
        "        'scalers': {'X': scaler_X, 'y': scaler_y},\n",
        "        'best_params': best_params,\n",
        "        'train_metrics': train_metrics,\n",
        "        'test_metrics': test_metrics,\n",
        "        'predictions': {'train': y_pred_train, 'test': y_pred_test}\n",
        "    }\n",
        "\n",
        "print(\"‚úÖ Modelos V2 definidos con escalamiento robusto\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Modelos V2 avanzados de series de tiempo definidos\n"
          ]
        }
      ],
      "source": [
        "# MODELOS V2 AVANZADOS - SERIES DE TIEMPO ESPECIALIZADOS\n",
        "\n",
        "def train_arimax_garch_v2(X_train, y_train, X_test, y_test, combo_name, n_trials=15):\n",
        "    \"\"\"\n",
        "    ARIMAX-GARCH V2: Predicci√≥n directa de precios con primera diferencia\n",
        "    - Usa primera diferencia en lugar de retornos log\n",
        "    - GARCH sobre residuos en escala de precios\n",
        "    - Mejor manejo de la volatilidad\n",
        "    \"\"\"\n",
        "    from arch import arch_model\n",
        "    from statsmodels.tsa.arima.model import ARIMA\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    import warnings\n",
        "    warnings.filterwarnings('ignore')\n",
        "    \n",
        "    print(f\"   üîÑ Optimizando ARIMAX-GARCH V2 para {combo_name}...\")\n",
        "    \n",
        "    # Preparar datos - usar primera diferencia\n",
        "    y_train_diff = y_train.diff().dropna()\n",
        "    y_test_diff = y_test.diff().dropna()\n",
        "    \n",
        "    # Ajustar X para coincidir con las diferencias\n",
        "    X_train_adj = X_train.iloc[1:].copy()  # Quitar primera observaci√≥n\n",
        "    X_test_adj = X_test.iloc[1:].copy()\n",
        "    \n",
        "    # Escalar variables ex√≥genas\n",
        "    scaler_X = StandardScaler()\n",
        "    X_train_scaled = scaler_X.fit_transform(X_train_adj)\n",
        "    X_test_scaled = scaler_X.transform(X_test_adj)\n",
        "    \n",
        "    def objective(trial):\n",
        "        try:\n",
        "            # Par√°metros ARIMAX\n",
        "            p = trial.suggest_int('p', 0, 3)\n",
        "            d = 0  # Ya diferenciamos manualmente\n",
        "            q = trial.suggest_int('q', 0, 3)\n",
        "            \n",
        "            # Par√°metros GARCH\n",
        "            garch_p = trial.suggest_int('garch_p', 1, 2)\n",
        "            garch_q = trial.suggest_int('garch_q', 1, 2)\n",
        "            \n",
        "            # Entrenar ARIMAX\n",
        "            arimax_model = ARIMA(\n",
        "                y_train_diff,\n",
        "                exog=X_train_scaled,\n",
        "                order=(p, d, q)\n",
        "            )\n",
        "            arimax_fit = arimax_model.fit(method_kwargs={'warn_convergence': False})\n",
        "            \n",
        "            # Obtener residuos\n",
        "            residuals = arimax_fit.resid\n",
        "            \n",
        "            # Entrenar GARCH sobre residuos\n",
        "            garch_model = arch_model(\n",
        "                residuals,\n",
        "                vol='GARCH',\n",
        "                p=garch_p,\n",
        "                q=garch_q,\n",
        "                rescale=False\n",
        "            )\n",
        "            garch_fit = garch_model.fit(disp='off', show_warning=False)\n",
        "            \n",
        "            # Predicciones ARIMAX\n",
        "            arimax_pred = arimax_fit.forecast(steps=len(y_test_diff), exog=X_test_scaled)\n",
        "            \n",
        "            # Convertir diferencias a niveles\n",
        "            y_pred_levels = np.zeros(len(y_test))\n",
        "            y_pred_levels[0] = y_train.iloc[-1]  # √öltimo valor conocido\n",
        "            \n",
        "            for i in range(1, len(y_pred_levels)):\n",
        "                y_pred_levels[i] = y_pred_levels[i-1] + arimax_pred.iloc[i-1]\n",
        "            \n",
        "            # Calcular RMSE\n",
        "            rmse = np.sqrt(mean_squared_error(y_test.values, y_pred_levels))\n",
        "            return rmse\n",
        "            \n",
        "        except Exception as e:\n",
        "            return float('inf')\n",
        "    \n",
        "    # Optimizaci√≥n\n",
        "    study = optuna.create_study(direction='minimize', study_name=f'arimax_garch_v2_{combo_name}')\n",
        "    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
        "    \n",
        "    # Modelo final con mejores par√°metros\n",
        "    best_params = study.best_params\n",
        "    \n",
        "    try:\n",
        "        # ARIMAX final\n",
        "        final_arimax = ARIMA(\n",
        "            y_train_diff,\n",
        "            exog=X_train_scaled,\n",
        "            order=(best_params['p'], 0, best_params['q'])\n",
        "        )\n",
        "        arimax_fit = final_arimax.fit(method_kwargs={'warn_convergence': False})\n",
        "        \n",
        "        # GARCH final\n",
        "        residuals = arimax_fit.resid\n",
        "        final_garch = arch_model(\n",
        "            residuals,\n",
        "            vol='GARCH',\n",
        "            p=best_params['garch_p'],\n",
        "            q=best_params['garch_q'],\n",
        "            rescale=False\n",
        "        )\n",
        "        garch_fit = final_garch.fit(disp='off', show_warning=False)\n",
        "        \n",
        "        # Predicciones finales\n",
        "        arimax_pred_train = arimax_fit.fittedvalues\n",
        "        arimax_pred_test = arimax_fit.forecast(steps=len(y_test_diff), exog=X_test_scaled)\n",
        "        \n",
        "        # Convertir a niveles de precios\n",
        "        y_pred_train_levels = np.zeros(len(y_train))\n",
        "        y_pred_train_levels[0] = y_train.iloc[0]\n",
        "        for i in range(1, len(y_pred_train_levels)):\n",
        "            y_pred_train_levels[i] = y_pred_train_levels[i-1] + arimax_pred_train.iloc[i-1]\n",
        "        \n",
        "        y_pred_test_levels = np.zeros(len(y_test))\n",
        "        y_pred_test_levels[0] = y_train.iloc[-1]\n",
        "        for i in range(1, len(y_pred_test_levels)):\n",
        "            y_pred_test_levels[i] = y_pred_test_levels[i-1] + arimax_pred_test.iloc[i-1]\n",
        "        \n",
        "        # M√©tricas\n",
        "        train_metrics = evaluate_model(y_train.values, y_pred_train_levels, f\"ARIMAX_GARCH_v2_{combo_name}_train\")\n",
        "        test_metrics = evaluate_model(y_test.values, y_pred_test_levels, f\"ARIMAX_GARCH_v2_{combo_name}_test\")\n",
        "        \n",
        "        return {\n",
        "            'arimax_model': arimax_fit,\n",
        "            'garch_model': garch_fit,\n",
        "            'scaler_X': scaler_X,\n",
        "            'best_params': best_params,\n",
        "            'train_metrics': train_metrics,\n",
        "            'test_metrics': test_metrics,\n",
        "            'predictions': {'train': y_pred_train_levels, 'test': y_pred_test_levels}\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Error en modelo final: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def train_markov_regime_v2(X_train, y_train, X_test, y_test, combo_name, n_trials=15):\n",
        "    \"\"\"\n",
        "    Markov Regime-Switching V2: Detecci√≥n mejorada de reg√≠menes\n",
        "    - Reg√≠menes basados en niveles de precio y volatilidad\n",
        "    - Modelos separados por r√©gimen con escalamiento\n",
        "    - Mejor detecci√≥n de cambios estructurales\n",
        "    \"\"\"\n",
        "    from sklearn.mixture import GaussianMixture\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    import warnings\n",
        "    warnings.filterwarnings('ignore')\n",
        "    \n",
        "    print(f\"   üîÑ Optimizando Markov Regime V2 para {combo_name}...\")\n",
        "    \n",
        "    # Calcular caracter√≠sticas para detecci√≥n de reg√≠menes\n",
        "    price_ma_20 = y_train.rolling(20).mean()\n",
        "    price_volatility = y_train.rolling(20).std()\n",
        "    price_level = (y_train - y_train.rolling(60).mean()) / y_train.rolling(60).std()\n",
        "    \n",
        "    # Crear features para reg√≠menes (sin NaN)\n",
        "    regime_features = pd.DataFrame({\n",
        "        'price_level': price_level,\n",
        "        'volatility': price_volatility,\n",
        "        'ma_ratio': y_train / price_ma_20\n",
        "    }).dropna()\n",
        "    \n",
        "    # Ajustar √≠ndices\n",
        "    start_idx = regime_features.index[0]\n",
        "    y_train_clean = y_train.loc[start_idx:]\n",
        "    X_train_clean = X_train.loc[start_idx:]\n",
        "    \n",
        "    def objective(trial):\n",
        "        try:\n",
        "            # Par√°metros\n",
        "            n_regimes = trial.suggest_int('n_regimes', 2, 4)\n",
        "            \n",
        "            # Detectar reg√≠menes con Gaussian Mixture\n",
        "            gmm = GaussianMixture(\n",
        "                n_components=n_regimes,\n",
        "                covariance_type='full',\n",
        "                random_state=42,\n",
        "                max_iter=100\n",
        "            )\n",
        "            \n",
        "            regime_labels = gmm.fit_predict(regime_features.values)\n",
        "            \n",
        "            # Entrenar modelos por r√©gimen\n",
        "            regime_models = {}\n",
        "            regime_scalers = {}\n",
        "            \n",
        "            for regime in range(n_regimes):\n",
        "                regime_mask = regime_labels == regime\n",
        "                \n",
        "                if np.sum(regime_mask) < 10:  # Muy pocas observaciones\n",
        "                    continue\n",
        "                \n",
        "                # Datos del r√©gimen\n",
        "                X_regime = X_train_clean[regime_mask]\n",
        "                y_regime = y_train_clean[regime_mask]\n",
        "                \n",
        "                if len(X_regime) < 5:\n",
        "                    continue\n",
        "                \n",
        "                # Escalar datos del r√©gimen\n",
        "                scaler_X_regime = StandardScaler()\n",
        "                scaler_y_regime = StandardScaler()\n",
        "                \n",
        "                X_regime_scaled = scaler_X_regime.fit_transform(X_regime)\n",
        "                y_regime_scaled = scaler_y_regime.fit_transform(y_regime.values.reshape(-1, 1)).ravel()\n",
        "                \n",
        "                # Modelo simple para el r√©gimen\n",
        "                from sklearn.ensemble import RandomForestRegressor\n",
        "                model_regime = RandomForestRegressor(\n",
        "                    n_estimators=50,\n",
        "                    max_depth=trial.suggest_int(f'max_depth_{regime}', 3, 10),\n",
        "                    random_state=42,\n",
        "                    n_jobs=-1\n",
        "                )\n",
        "                \n",
        "                model_regime.fit(X_regime_scaled, y_regime_scaled)\n",
        "                \n",
        "                regime_models[regime] = model_regime\n",
        "                regime_scalers[regime] = {'X': scaler_X_regime, 'y': scaler_y_regime}\n",
        "            \n",
        "            if len(regime_models) == 0:\n",
        "                return float('inf')\n",
        "            \n",
        "            # Predicciones en test\n",
        "            # Detectar reg√≠menes en test\n",
        "            price_ma_20_test = pd.concat([y_train.tail(20), y_test]).rolling(20).mean().iloc[20:]\n",
        "            price_volatility_test = pd.concat([y_train.tail(20), y_test]).rolling(20).std().iloc[20:]\n",
        "            price_level_test = (y_test - pd.concat([y_train.tail(60), y_test]).rolling(60).mean().iloc[60:]) / pd.concat([y_train.tail(60), y_test]).rolling(60).std().iloc[60:]\n",
        "            \n",
        "            regime_features_test = pd.DataFrame({\n",
        "                'price_level': price_level_test,\n",
        "                'volatility': price_volatility_test,\n",
        "                'ma_ratio': y_test / price_ma_20_test\n",
        "            }).fillna(0)  # Llenar NaN con 0\n",
        "            \n",
        "            test_regime_labels = gmm.predict(regime_features_test.values)\n",
        "            \n",
        "            # Predicciones por r√©gimen\n",
        "            y_pred_test = np.zeros(len(y_test))\n",
        "            \n",
        "            for i, regime in enumerate(test_regime_labels):\n",
        "                if regime in regime_models:\n",
        "                    X_test_point = X_test.iloc[i:i+1]\n",
        "                    X_test_scaled = regime_scalers[regime]['X'].transform(X_test_point)\n",
        "                    y_pred_scaled = regime_models[regime].predict(X_test_scaled)\n",
        "                    y_pred_test[i] = regime_scalers[regime]['y'].inverse_transform(y_pred_scaled.reshape(-1, 1))[0, 0]\n",
        "                else:\n",
        "                    # Usar r√©gimen m√°s com√∫n si no existe\n",
        "                    common_regime = max(regime_models.keys())\n",
        "                    X_test_point = X_test.iloc[i:i+1]\n",
        "                    X_test_scaled = regime_scalers[common_regime]['X'].transform(X_test_point)\n",
        "                    y_pred_scaled = regime_models[common_regime].predict(X_test_scaled)\n",
        "                    y_pred_test[i] = regime_scalers[common_regime]['y'].inverse_transform(y_pred_scaled.reshape(-1, 1))[0, 0]\n",
        "            \n",
        "            rmse = np.sqrt(mean_squared_error(y_test.values, y_pred_test))\n",
        "            return rmse\n",
        "            \n",
        "        except Exception as e:\n",
        "            return float('inf')\n",
        "    \n",
        "    # Optimizaci√≥n\n",
        "    study = optuna.create_study(direction='minimize', study_name=f'markov_regime_v2_{combo_name}')\n",
        "    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
        "    \n",
        "    # Modelo final\n",
        "    best_params = study.best_params\n",
        "    n_regimes = best_params['n_regimes']\n",
        "    \n",
        "    try:\n",
        "        # GMM final\n",
        "        final_gmm = GaussianMixture(\n",
        "            n_components=n_regimes,\n",
        "            covariance_type='full',\n",
        "            random_state=42,\n",
        "            max_iter=100\n",
        "        )\n",
        "        \n",
        "        regime_labels = final_gmm.fit_predict(regime_features.values)\n",
        "        \n",
        "        # Entrenar modelos finales por r√©gimen\n",
        "        final_regime_models = {}\n",
        "        final_regime_scalers = {}\n",
        "        \n",
        "        for regime in range(n_regimes):\n",
        "            regime_mask = regime_labels == regime\n",
        "            \n",
        "            if np.sum(regime_mask) < 10:\n",
        "                continue\n",
        "            \n",
        "            X_regime = X_train_clean[regime_mask]\n",
        "            y_regime = y_train_clean[regime_mask]\n",
        "            \n",
        "            if len(X_regime) < 5:\n",
        "                continue\n",
        "            \n",
        "            scaler_X_regime = StandardScaler()\n",
        "            scaler_y_regime = StandardScaler()\n",
        "            \n",
        "            X_regime_scaled = scaler_X_regime.fit_transform(X_regime)\n",
        "            y_regime_scaled = scaler_y_regime.fit_transform(y_regime.values.reshape(-1, 1)).ravel()\n",
        "            \n",
        "            from sklearn.ensemble import RandomForestRegressor\n",
        "            model_regime = RandomForestRegressor(\n",
        "                n_estimators=100,\n",
        "                max_depth=best_params.get(f'max_depth_{regime}', 6),\n",
        "                random_state=42,\n",
        "                n_jobs=-1\n",
        "            )\n",
        "            \n",
        "            model_regime.fit(X_regime_scaled, y_regime_scaled)\n",
        "            \n",
        "            final_regime_models[regime] = model_regime\n",
        "            final_regime_scalers[regime] = {'X': scaler_X_regime, 'y': scaler_y_regime}\n",
        "        \n",
        "        # Predicciones finales (train y test)\n",
        "        # Train predictions\n",
        "        y_pred_train = np.zeros(len(y_train_clean))\n",
        "        for i, regime in enumerate(regime_labels):\n",
        "            if regime in final_regime_models:\n",
        "                X_train_point = X_train_clean.iloc[i:i+1]\n",
        "                X_train_scaled = final_regime_scalers[regime]['X'].transform(X_train_point)\n",
        "                y_pred_scaled = final_regime_models[regime].predict(X_train_scaled)\n",
        "                y_pred_train[i] = final_regime_scalers[regime]['y'].inverse_transform(y_pred_scaled.reshape(-1, 1))[0, 0]\n",
        "        \n",
        "        # Test predictions (como en objective)\n",
        "        price_ma_20_test = pd.concat([y_train.tail(20), y_test]).rolling(20).mean().iloc[20:]\n",
        "        price_volatility_test = pd.concat([y_train.tail(20), y_test]).rolling(20).std().iloc[20:]\n",
        "        price_level_test = (y_test - pd.concat([y_train.tail(60), y_test]).rolling(60).mean().iloc[60:]) / pd.concat([y_train.tail(60), y_test]).rolling(60).std().iloc[60:]\n",
        "        \n",
        "        regime_features_test = pd.DataFrame({\n",
        "            'price_level': price_level_test,\n",
        "            'volatility': price_volatility_test,\n",
        "            'ma_ratio': y_test / price_ma_20_test\n",
        "        }).fillna(0)\n",
        "        \n",
        "        test_regime_labels = final_gmm.predict(regime_features_test.values)\n",
        "        \n",
        "        y_pred_test = np.zeros(len(y_test))\n",
        "        for i, regime in enumerate(test_regime_labels):\n",
        "            if regime in final_regime_models:\n",
        "                X_test_point = X_test.iloc[i:i+1]\n",
        "                X_test_scaled = final_regime_scalers[regime]['X'].transform(X_test_point)\n",
        "                y_pred_scaled = final_regime_models[regime].predict(X_test_scaled)\n",
        "                y_pred_test[i] = final_regime_scalers[regime]['y'].inverse_transform(y_pred_scaled.reshape(-1, 1))[0, 0]\n",
        "            else:\n",
        "                common_regime = max(final_regime_models.keys())\n",
        "                X_test_point = X_test.iloc[i:i+1]\n",
        "                X_test_scaled = final_regime_scalers[common_regime]['X'].transform(X_test_point)\n",
        "                y_pred_scaled = final_regime_models[common_regime].predict(X_test_scaled)\n",
        "                y_pred_test[i] = final_regime_scalers[common_regime]['y'].inverse_transform(y_pred_scaled.reshape(-1, 1))[0, 0]\n",
        "        \n",
        "        # M√©tricas\n",
        "        train_metrics = evaluate_model(y_train_clean.values, y_pred_train, f\"Markov_Regime_v2_{combo_name}_train\")\n",
        "        test_metrics = evaluate_model(y_test.values, y_pred_test, f\"Markov_Regime_v2_{combo_name}_test\")\n",
        "        \n",
        "        return {\n",
        "            'gmm_model': final_gmm,\n",
        "            'regime_models': final_regime_models,\n",
        "            'regime_scalers': final_regime_scalers,\n",
        "            'regime_features': regime_features,\n",
        "            'best_params': best_params,\n",
        "            'train_metrics': train_metrics,\n",
        "            'test_metrics': test_metrics,\n",
        "            'predictions': {'train': y_pred_train, 'test': y_pred_test}\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Error en modelo final: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def train_midas_v2(X_train, y_train, X_test, y_test, combo_name, n_trials=15):\n",
        "    \"\"\"\n",
        "    MIDAS V2: Optimizado para predicci√≥n directa de precios\n",
        "    - Variables mensuales usadas directamente\n",
        "    - Mejor manejo de frecuencias mixtas\n",
        "    - Escalamiento robusto con validaci√≥n de datos\n",
        "    \"\"\"\n",
        "    from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "    from sklearn.ensemble import RandomForestRegressor\n",
        "    from sklearn.linear_model import Ridge\n",
        "    import warnings\n",
        "    warnings.filterwarnings('ignore')\n",
        "    \n",
        "    print(f\"   üîÑ Optimizando MIDAS V2 para {combo_name}...\")\n",
        "    \n",
        "    # Identificar variables mensuales vs diarias\n",
        "    monthly_vars = []\n",
        "    daily_vars = []\n",
        "    \n",
        "    for col in X_train.columns:\n",
        "        # Variables que t√≠picamente son mensuales\n",
        "        if any(keyword in col.lower() for keyword in ['pmi', 'cpi', 'gdp', 'employment', 'retail', 'industrial']):\n",
        "            monthly_vars.append(col)\n",
        "        else:\n",
        "            daily_vars.append(col)\n",
        "    \n",
        "    print(f\"   üìä Variables mensuales detectadas: {len(monthly_vars)}\")\n",
        "    print(f\"   üìä Variables diarias detectadas: {len(daily_vars)}\")\n",
        "    \n",
        "    def create_midas_features(X, y, lookback_months=6):\n",
        "        \"\"\"Crear features MIDAS con diferentes horizontes temporales y validaci√≥n robusta\"\"\"\n",
        "        midas_features = []\n",
        "        \n",
        "        # Features diarias (lags cortos)\n",
        "        for col in daily_vars:\n",
        "            if col in X.columns:\n",
        "                for lag in [1, 5, 10, 20]:\n",
        "                    feature = X[col].shift(lag)\n",
        "                    # Validar que no tenga valores infinitos o extremos\n",
        "                    if not feature.isnull().all():\n",
        "                        # Reemplazar infinitos con NaN\n",
        "                        feature = feature.replace([np.inf, -np.inf], np.nan)\n",
        "                        # Winsorizar valores extremos (percentiles 1 y 99)\n",
        "                        if not feature.isnull().all():\n",
        "                            p1, p99 = feature.quantile([0.01, 0.99])\n",
        "                            feature = feature.clip(lower=p1, upper=p99)\n",
        "                        midas_features.append(feature)\n",
        "        \n",
        "        # Features mensuales (lags m√°s largos, agregaciones)\n",
        "        for col in monthly_vars:\n",
        "            if col in X.columns:\n",
        "                # Lags mensuales (aproximadamente 22 d√≠as h√°biles por mes)\n",
        "                for month_lag in range(1, lookback_months + 1):\n",
        "                    lag_days = month_lag * 22\n",
        "                    if lag_days < len(X):\n",
        "                        feature = X[col].shift(lag_days)\n",
        "                        # Validar y limpiar\n",
        "                        if not feature.isnull().all():\n",
        "                            feature = feature.replace([np.inf, -np.inf], np.nan)\n",
        "                            if not feature.isnull().all():\n",
        "                                p1, p99 = feature.quantile([0.01, 0.99])\n",
        "                                feature = feature.clip(lower=p1, upper=p99)\n",
        "                            midas_features.append(feature)\n",
        "                \n",
        "                # Promedios m√≥viles mensuales\n",
        "                for window in [22, 66]:  # 1 mes, 3 meses\n",
        "                    if window < len(X):\n",
        "                        feature = X[col].rolling(window, min_periods=window//2).mean()\n",
        "                        if not feature.isnull().all():\n",
        "                            feature = feature.replace([np.inf, -np.inf], np.nan)\n",
        "                            if not feature.isnull().all():\n",
        "                                p1, p99 = feature.quantile([0.01, 0.99])\n",
        "                                feature = feature.clip(lower=p1, upper=p99)\n",
        "                            midas_features.append(feature)\n",
        "        \n",
        "        # Features de precio (autorregresivos)\n",
        "        for lag in [1, 5, 10, 20, 60]:\n",
        "            if lag < len(y):\n",
        "                feature = y.shift(lag)\n",
        "                if not feature.isnull().all():\n",
        "                    feature = feature.replace([np.inf, -np.inf], np.nan)\n",
        "                    if not feature.isnull().all():\n",
        "                        p1, p99 = feature.quantile([0.01, 0.99])\n",
        "                        feature = feature.clip(lower=p1, upper=p99)\n",
        "                    midas_features.append(feature)\n",
        "        \n",
        "        # Ratios y diferencias (con validaci√≥n)\n",
        "        if len(daily_vars) > 0 and daily_vars[0] in X.columns:\n",
        "            denominator = X[daily_vars[0]]\n",
        "            # Evitar divisi√≥n por cero\n",
        "            denominator_safe = denominator.replace(0, np.nan)\n",
        "            if not denominator_safe.isnull().all():\n",
        "                ratio = y / denominator_safe\n",
        "                ratio = ratio.replace([np.inf, -np.inf], np.nan)\n",
        "                if not ratio.isnull().all():\n",
        "                    p1, p99 = ratio.quantile([0.01, 0.99])\n",
        "                    ratio = ratio.clip(lower=p1, upper=p99)\n",
        "                midas_features.append(ratio)\n",
        "        \n",
        "        # Combinar todas las features\n",
        "        if len(midas_features) == 0:\n",
        "            print(\"   ‚ö†Ô∏è No se pudieron crear features MIDAS v√°lidas\")\n",
        "            return pd.DataFrame()\n",
        "        \n",
        "        midas_df = pd.concat(midas_features, axis=1)\n",
        "        midas_df.columns = [f'midas_feature_{i}' for i in range(len(midas_features))]\n",
        "        \n",
        "        # Eliminar columnas que son completamente NaN\n",
        "        midas_df = midas_df.dropna(axis=1, how='all')\n",
        "        \n",
        "        # Eliminar filas con demasiados NaN (m√°s del 50%)\n",
        "        threshold = len(midas_df.columns) * 0.5\n",
        "        midas_df = midas_df.dropna(thresh=threshold)\n",
        "        \n",
        "        # Llenar NaN restantes con forward fill y backward fill\n",
        "        midas_df = midas_df.fillna(method='ffill').fillna(method='bfill')\n",
        "        \n",
        "        # Verificaci√≥n final de valores infinitos o extremos\n",
        "        for col in midas_df.columns:\n",
        "            if midas_df[col].isnull().all():\n",
        "                continue\n",
        "            # Reemplazar cualquier infinito restante\n",
        "            midas_df[col] = midas_df[col].replace([np.inf, -np.inf], np.nan)\n",
        "            # Llenar NaN con la mediana\n",
        "            if midas_df[col].isnull().any():\n",
        "                midas_df[col] = midas_df[col].fillna(midas_df[col].median())\n",
        "        \n",
        "        return midas_df\n",
        "    \n",
        "    # Crear features MIDAS\n",
        "    midas_train = create_midas_features(X_train, y_train)\n",
        "    midas_test = create_midas_features(X_test, y_test)\n",
        "    \n",
        "    if midas_train.empty or midas_test.empty:\n",
        "        print(f\"   ‚ùå No se pudieron crear features MIDAS v√°lidas\")\n",
        "        return None\n",
        "    \n",
        "    # Asegurar que ambos conjuntos tengan las mismas columnas\n",
        "    common_cols = midas_train.columns.intersection(midas_test.columns)\n",
        "    if len(common_cols) == 0:\n",
        "        print(f\"   ‚ùå No hay columnas comunes entre train y test\")\n",
        "        return None\n",
        "    \n",
        "    midas_train = midas_train[common_cols]\n",
        "    midas_test = midas_test[common_cols]\n",
        "    \n",
        "    # Ajustar y para coincidir con features MIDAS\n",
        "    y_train_adj = y_train.loc[midas_train.index]\n",
        "    y_test_adj = y_test.loc[midas_test.index]\n",
        "    \n",
        "    print(f\"   üìä Features MIDAS creadas: {midas_train.shape[1]}\")\n",
        "    print(f\"   üìä Observaciones train: {len(midas_train)}\")\n",
        "    print(f\"   üìä Observaciones test: {len(midas_test)}\")\n",
        "    \n",
        "    # Verificaci√≥n final de datos\n",
        "    if np.any(np.isinf(midas_train.values)) or np.any(np.isinf(midas_test.values)):\n",
        "        print(f\"   ‚ùå A√∫n hay valores infinitos en los datos\")\n",
        "        return None\n",
        "    \n",
        "    if np.any(np.isnan(midas_train.values)) or np.any(np.isnan(midas_test.values)):\n",
        "        print(f\"   ‚ùå A√∫n hay valores NaN en los datos\")\n",
        "        return None\n",
        "    \n",
        "    def objective(trial):\n",
        "        try:\n",
        "            # Seleccionar tipo de modelo\n",
        "            model_type = trial.suggest_categorical('model_type', ['ridge', 'random_forest'])\n",
        "            \n",
        "            # Usar RobustScaler en lugar de StandardScaler para mejor manejo de outliers\n",
        "            scaler_X = RobustScaler()\n",
        "            scaler_y = RobustScaler()\n",
        "            \n",
        "            X_train_scaled = scaler_X.fit_transform(midas_train)\n",
        "            y_train_scaled = scaler_y.fit_transform(y_train_adj.values.reshape(-1, 1)).ravel()\n",
        "            \n",
        "            X_test_scaled = scaler_X.transform(midas_test)\n",
        "            \n",
        "            # Verificar que no hay infinitos despu√©s del escalamiento\n",
        "            if np.any(np.isinf(X_train_scaled)) or np.any(np.isinf(X_test_scaled)):\n",
        "                return float('inf')\n",
        "            \n",
        "            if model_type == 'ridge':\n",
        "                alpha = trial.suggest_float('alpha', 0.01, 100.0, log=True)\n",
        "                model = Ridge(alpha=alpha, random_state=42)\n",
        "            else:  # random_forest\n",
        "                n_estimators = trial.suggest_int('n_estimators', 50, 200)\n",
        "                max_depth = trial.suggest_int('max_depth', 3, 15)\n",
        "                min_samples_split = trial.suggest_int('min_samples_split', 2, 20)\n",
        "                \n",
        "                model = RandomForestRegressor(\n",
        "                    n_estimators=n_estimators,\n",
        "                    max_depth=max_depth,\n",
        "                    min_samples_split=min_samples_split,\n",
        "                    random_state=42,\n",
        "                    n_jobs=-1\n",
        "                )\n",
        "            \n",
        "            # Entrenar\n",
        "            model.fit(X_train_scaled, y_train_scaled)\n",
        "            \n",
        "            # Predicciones\n",
        "            y_pred_scaled = model.predict(X_test_scaled)\n",
        "            y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).ravel()\n",
        "            \n",
        "            # Verificar que las predicciones son v√°lidas\n",
        "            if np.any(np.isinf(y_pred)) or np.any(np.isnan(y_pred)):\n",
        "                return float('inf')\n",
        "            \n",
        "            rmse = np.sqrt(mean_squared_error(y_test_adj.values, y_pred))\n",
        "            return rmse\n",
        "            \n",
        "        except Exception as e:\n",
        "            return float('inf')\n",
        "    \n",
        "    # Optimizaci√≥n\n",
        "    study = optuna.create_study(direction='minimize', study_name=f'midas_v2_{combo_name}')\n",
        "    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
        "    \n",
        "    # Modelo final\n",
        "    best_params = study.best_params\n",
        "    \n",
        "    try:\n",
        "        # Escaladores finales\n",
        "        final_scaler_X = RobustScaler()\n",
        "        final_scaler_y = RobustScaler()\n",
        "        \n",
        "        X_train_scaled = final_scaler_X.fit_transform(midas_train)\n",
        "        y_train_scaled = final_scaler_y.fit_transform(y_train_adj.values.reshape(-1, 1)).ravel()\n",
        "        \n",
        "        X_test_scaled = final_scaler_X.transform(midas_test)\n",
        "        \n",
        "        # Verificar escalamiento final\n",
        "        if np.any(np.isinf(X_train_scaled)) or np.any(np.isinf(X_test_scaled)):\n",
        "            print(f\"   ‚ùå Valores infinitos despu√©s del escalamiento final\")\n",
        "            return None\n",
        "        \n",
        "        # Modelo final\n",
        "        if best_params['model_type'] == 'ridge':\n",
        "            final_model = Ridge(alpha=best_params['alpha'], random_state=42)\n",
        "        else:\n",
        "            final_model = RandomForestRegressor(\n",
        "                n_estimators=best_params['n_estimators'],\n",
        "                max_depth=best_params['max_depth'],\n",
        "                min_samples_split=best_params['min_samples_split'],\n",
        "                random_state=42,\n",
        "                n_jobs=-1\n",
        "            )\n",
        "        \n",
        "        final_model.fit(X_train_scaled, y_train_scaled)\n",
        "        \n",
        "        # Predicciones finales\n",
        "        y_pred_train_scaled = final_model.predict(X_train_scaled)\n",
        "        y_pred_test_scaled = final_model.predict(X_test_scaled)\n",
        "        \n",
        "        y_pred_train = final_scaler_y.inverse_transform(y_pred_train_scaled.reshape(-1, 1)).ravel()\n",
        "        y_pred_test = final_scaler_y.inverse_transform(y_pred_test_scaled.reshape(-1, 1)).ravel()\n",
        "        \n",
        "        # Verificar predicciones finales\n",
        "        if np.any(np.isinf(y_pred_train)) or np.any(np.isinf(y_pred_test)):\n",
        "            print(f\"   ‚ùå Predicciones finales contienen infinitos\")\n",
        "            return None\n",
        "        \n",
        "        # M√©tricas\n",
        "        train_metrics = evaluate_model(y_train_adj.values, y_pred_train, f\"MIDAS_v2_{combo_name}_train\")\n",
        "        test_metrics = evaluate_model(y_test_adj.values, y_pred_test, f\"MIDAS_v2_{combo_name}_test\")\n",
        "        \n",
        "        return {\n",
        "            'model': final_model,\n",
        "            'scalers': {'X': final_scaler_X, 'y': final_scaler_y},\n",
        "            'midas_features': {'train': midas_train, 'test': midas_test},\n",
        "            'best_params': best_params,\n",
        "            'train_metrics': train_metrics,\n",
        "            'test_metrics': test_metrics,\n",
        "            'predictions': {'train': y_pred_train, 'test': y_pred_test}\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Error en modelo final: {e}\")\n",
        "        return None\n",
        "\n",
        "print(\"‚úÖ Modelos V2 avanzados de series de tiempo definidos\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ EJECUTANDO MODELOS V2 - PREDICCI√ìN DIRECTA DE PRECIOS\n",
            "============================================================\n",
            "\n",
            "==================== COMBINACI√ìN: HIBRIDA ====================\n",
            "\n",
            "üìä Datos para entrenamiento:\n",
            "   X_train: (1143, 12)\n",
            "   X_test: (286, 12)\n",
            "   Rango precios train: $408.50 - $555.99\n",
            "   Rango precios test: $490.94 - $590.70\n",
            "\n",
            "ü§ñ Entrenando XGBoost V2 - hibrida...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f0fba44756f54cf99a56a08a70f56ac5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/15 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   ‚úÖ RMSE Test: $25.06\n",
            "   ‚úÖ MAE Test: $21.06\n",
            "   ‚úÖ MAPE Test: 3.9%\n",
            "   ‚úÖ R¬≤ Test: -1.288\n",
            "   ‚úÖ Dir. Accuracy: 46.0%\n",
            "   ‚úÖ Hit Rate ¬±2%: 24.1%\n",
            "\n",
            "   üìä Top 5 Features m√°s importantes:\n",
            "      price_ma20: 0.709\n",
            "      current_price: 0.038\n",
            "      coking_ma_ratio: 0.031\n",
            "      commodities_ma_ratio: 0.028\n",
            "      VIX: 0.027\n",
            "\n",
            "ü§ñ Entrenando LightGBM V2 - hibrida...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1767a191b4ed4fc982e01287a3dd26b5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/15 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[78]\tvalid_0's rmse: 0.619285\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[976]\tvalid_0's rmse: 0.665067\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[98]\tvalid_0's rmse: 0.622412\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[92]\tvalid_0's rmse: 0.654818\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[83]\tvalid_0's rmse: 0.660847\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[796]\tvalid_0's rmse: 0.949103\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[32]\tvalid_0's rmse: 0.66119\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[49]\tvalid_0's rmse: 0.638023\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[95]\tvalid_0's rmse: 0.656894\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[267]\tvalid_0's rmse: 0.680925\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[663]\tvalid_0's rmse: 0.640821\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[129]\tvalid_0's rmse: 0.630622\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[614]\tvalid_0's rmse: 0.71163\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[113]\tvalid_0's rmse: 0.643992\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[52]\tvalid_0's rmse: 0.645267\n",
            "   ‚úÖ RMSE Test: $25.45\n",
            "   ‚úÖ MAE Test: $21.46\n",
            "   ‚úÖ MAPE Test: 4.0%\n",
            "   ‚úÖ R¬≤ Test: -1.360\n",
            "   ‚úÖ Dir. Accuracy: 56.8%\n",
            "   ‚úÖ Hit Rate ¬±2%: 25.2%\n",
            "\n",
            "ü§ñ Entrenando ARIMAX-GARCH V2 - hibrida...\n",
            "   üîÑ Optimizando ARIMAX-GARCH V2 para hibrida...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "da1780c4244d435dae3bb4b0ee76fc16",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/15 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   ‚úÖ RMSE Test: $34.77\n",
            "   ‚úÖ MAE Test: $28.35\n",
            "   ‚úÖ MAPE Test: 5.2%\n",
            "   ‚úÖ R¬≤ Test: -3.405\n",
            "   ‚úÖ Dir. Accuracy: 71.2%\n",
            "   ‚úÖ Hit Rate ¬±2%: 21.7%\n",
            "\n",
            "ü§ñ Entrenando Markov Regime V2 - hibrida...\n",
            "   üîÑ Optimizando Markov Regime V2 para hibrida...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0931ec864c7d47bd8900ba66b0d92a56",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/15 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   ‚úÖ RMSE Test: $24.40\n",
            "   ‚úÖ MAE Test: $20.28\n",
            "   ‚úÖ MAPE Test: 3.7%\n",
            "   ‚úÖ R¬≤ Test: -1.170\n",
            "   ‚úÖ Dir. Accuracy: 53.0%\n",
            "   ‚úÖ Hit Rate ¬±2%: 26.9%\n",
            "\n",
            "ü§ñ Entrenando MIDAS V2 - hibrida...\n",
            "   üîÑ Optimizando MIDAS V2 para hibrida...\n",
            "   üìä Variables mensuales detectadas: 0\n",
            "   üìä Variables diarias detectadas: 12\n",
            "   üìä Features MIDAS creadas: 54\n",
            "   üìä Observaciones train: 1138\n",
            "   üìä Observaciones test: 281\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5ef4c13b420341969b3df462e8e589bf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/15 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   ‚úÖ RMSE Test: $2.67\n",
            "   ‚úÖ MAE Test: $1.63\n",
            "   ‚úÖ MAPE Test: 0.3%\n",
            "   ‚úÖ R¬≤ Test: 0.974\n",
            "   ‚úÖ Dir. Accuracy: 95.4%\n",
            "   ‚úÖ Hit Rate ¬±2%: 98.9%\n",
            "\n",
            "==================== COMBINACI√ìN: FUNDAMENTAL ====================\n",
            "\n",
            "üìä Datos para entrenamiento:\n",
            "   X_train: (1143, 13)\n",
            "   X_test: (286, 13)\n",
            "   Rango precios train: $408.50 - $555.99\n",
            "   Rango precios test: $490.94 - $590.70\n",
            "\n",
            "ü§ñ Entrenando XGBoost V2 - fundamental...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6e38fc9790dc413e999c5acc4f6c37b9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/15 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   ‚úÖ RMSE Test: $26.19\n",
            "   ‚úÖ MAE Test: $22.11\n",
            "   ‚úÖ MAPE Test: 4.1%\n",
            "   ‚úÖ R¬≤ Test: -1.499\n",
            "   ‚úÖ Dir. Accuracy: 44.6%\n",
            "   ‚úÖ Hit Rate ¬±2%: 25.2%\n",
            "\n",
            "   üìä Top 5 Features m√°s importantes:\n",
            "      price_ma20: 0.417\n",
            "      current_price: 0.212\n",
            "      coking_ma_ratio: 0.067\n",
            "      coking_return: 0.036\n",
            "      commodities_ma_ratio: 0.035\n",
            "\n",
            "ü§ñ Entrenando LightGBM V2 - fundamental...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "542fff94cc414acd8ab4c7ef367e6dae",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/15 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[66]\tvalid_0's rmse: 0.66836\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[24]\tvalid_0's rmse: 0.748904\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[107]\tvalid_0's rmse: 0.698131\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[84]\tvalid_0's rmse: 0.655543\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[491]\tvalid_0's rmse: 0.716396\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[204]\tvalid_0's rmse: 0.633641\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[644]\tvalid_0's rmse: 0.770637\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[603]\tvalid_0's rmse: 0.851035\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[1175]\tvalid_0's rmse: 0.699029\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[327]\tvalid_0's rmse: 0.704183\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[195]\tvalid_0's rmse: 0.636808\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[188]\tvalid_0's rmse: 0.638988\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[432]\tvalid_0's rmse: 0.632206\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1060]\tvalid_0's rmse: 0.636321\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[510]\tvalid_0's rmse: 0.62993\n",
            "   ‚úÖ RMSE Test: $23.40\n",
            "   ‚úÖ MAE Test: $19.53\n",
            "   ‚úÖ MAPE Test: 3.6%\n",
            "   ‚úÖ R¬≤ Test: -0.996\n",
            "   ‚úÖ Dir. Accuracy: 56.1%\n",
            "   ‚úÖ Hit Rate ¬±2%: 27.6%\n",
            "\n",
            "ü§ñ Entrenando ARIMAX-GARCH V2 - fundamental...\n",
            "   üîÑ Optimizando ARIMAX-GARCH V2 para fundamental...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d2ac0d8ae2fe411bbdf82c09ea171980",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/15 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   ‚úÖ RMSE Test: $215.44\n",
            "   ‚úÖ MAE Test: $186.95\n",
            "   ‚úÖ MAPE Test: 34.8%\n",
            "   ‚úÖ R¬≤ Test: -168.129\n",
            "   ‚úÖ Dir. Accuracy: 71.9%\n",
            "   ‚úÖ Hit Rate ¬±2%: 0.0%\n",
            "\n",
            "ü§ñ Entrenando Markov Regime V2 - fundamental...\n",
            "   üîÑ Optimizando Markov Regime V2 para fundamental...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9058768aa5964ba49601b8e37c87493d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/15 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   ‚úÖ RMSE Test: $24.07\n",
            "   ‚úÖ MAE Test: $19.85\n",
            "   ‚úÖ MAPE Test: 3.7%\n",
            "   ‚úÖ R¬≤ Test: -1.110\n",
            "   ‚úÖ Dir. Accuracy: 53.0%\n",
            "   ‚úÖ Hit Rate ¬±2%: 28.0%\n",
            "\n",
            "ü§ñ Entrenando MIDAS V2 - fundamental...\n",
            "   üîÑ Optimizando MIDAS V2 para fundamental...\n",
            "   üìä Variables mensuales detectadas: 0\n",
            "   üìä Variables diarias detectadas: 13\n",
            "   üìä Features MIDAS creadas: 58\n",
            "   üìä Observaciones train: 1138\n",
            "   üìä Observaciones test: 281\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d62b99683177480b843d8677636d10bd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/15 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   ‚úÖ RMSE Test: $15.83\n",
            "   ‚úÖ MAE Test: $12.27\n",
            "   ‚úÖ MAPE Test: 2.3%\n",
            "   ‚úÖ R¬≤ Test: 0.089\n",
            "   ‚úÖ Dir. Accuracy: 55.4%\n",
            "   ‚úÖ Hit Rate ¬±2%: 52.3%\n",
            "\n",
            "==================== COMBINACI√ìN: REGIME ====================\n",
            "\n",
            "üìä Datos para entrenamiento:\n",
            "   X_train: (1143, 13)\n",
            "   X_test: (286, 13)\n",
            "   Rango precios train: $408.50 - $555.99\n",
            "   Rango precios test: $490.94 - $590.70\n",
            "\n",
            "ü§ñ Entrenando XGBoost V2 - regime...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e858ca673909490dacf0c4493c31fc35",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/15 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   ‚úÖ RMSE Test: $29.95\n",
            "   ‚úÖ MAE Test: $25.16\n",
            "   ‚úÖ MAPE Test: 4.6%\n",
            "   ‚úÖ R¬≤ Test: -2.269\n",
            "   ‚úÖ Dir. Accuracy: 47.4%\n",
            "   ‚úÖ Hit Rate ¬±2%: 21.7%\n",
            "\n",
            "   üìä Top 5 Features m√°s importantes:\n",
            "      tasa_interes_banxico: 0.544\n",
            "      price_ma20: 0.154\n",
            "      current_price: 0.060\n",
            "      VIX: 0.031\n",
            "      sp500_ma_ratio: 0.030\n",
            "\n",
            "ü§ñ Entrenando LightGBM V2 - regime...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e6065269ab1b4f00b15cdd2db5c4fdac",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/15 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[358]\tvalid_0's rmse: 0.797341\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[388]\tvalid_0's rmse: 0.788593\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[1320]\tvalid_0's rmse: 0.929908\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[529]\tvalid_0's rmse: 0.931363\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[214]\tvalid_0's rmse: 0.803051\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[410]\tvalid_0's rmse: 0.768423\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[984]\tvalid_0's rmse: 0.795865\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[1100]\tvalid_0's rmse: 0.753922\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[620]\tvalid_0's rmse: 0.752794\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[154]\tvalid_0's rmse: 0.762154\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[73]\tvalid_0's rmse: 0.743133\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[70]\tvalid_0's rmse: 0.754158\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[18]\tvalid_0's rmse: 0.703464\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[9]\tvalid_0's rmse: 0.785908\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[73]\tvalid_0's rmse: 0.719305\n",
            "   ‚úÖ RMSE Test: $27.80\n",
            "   ‚úÖ MAE Test: $23.34\n",
            "   ‚úÖ MAPE Test: 4.3%\n",
            "   ‚úÖ R¬≤ Test: -1.816\n",
            "   ‚úÖ Dir. Accuracy: 53.0%\n",
            "   ‚úÖ Hit Rate ¬±2%: 23.4%\n",
            "\n",
            "ü§ñ Entrenando ARIMAX-GARCH V2 - regime...\n",
            "   üîÑ Optimizando ARIMAX-GARCH V2 para regime...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "63ce5df29cdd4a76bf1400993a6f1049",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/15 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   ‚úÖ RMSE Test: $179.51\n",
            "   ‚úÖ MAE Test: $149.36\n",
            "   ‚úÖ MAPE Test: 27.8%\n",
            "   ‚úÖ R¬≤ Test: -116.428\n",
            "   ‚úÖ Dir. Accuracy: 69.8%\n",
            "   ‚úÖ Hit Rate ¬±2%: 1.7%\n",
            "\n",
            "ü§ñ Entrenando Markov Regime V2 - regime...\n",
            "   üîÑ Optimizando Markov Regime V2 para regime...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1f4c4263d3234f94a8f3164f597d312a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/15 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   ‚úÖ RMSE Test: $31.52\n",
            "   ‚úÖ MAE Test: $26.18\n",
            "   ‚úÖ MAPE Test: 4.8%\n",
            "   ‚úÖ R¬≤ Test: -2.620\n",
            "   ‚úÖ Dir. Accuracy: 54.0%\n",
            "   ‚úÖ Hit Rate ¬±2%: 23.8%\n",
            "\n",
            "ü§ñ Entrenando MIDAS V2 - regime...\n",
            "   üîÑ Optimizando MIDAS V2 para regime...\n",
            "   üìä Variables mensuales detectadas: 0\n",
            "   üìä Variables diarias detectadas: 13\n",
            "   üìä Features MIDAS creadas: 58\n",
            "   üìä Observaciones train: 1138\n",
            "   üìä Observaciones test: 281\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f902e685418e4b5cbbd926c7d9b20b7f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/15 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   ‚úÖ RMSE Test: $17.28\n",
            "   ‚úÖ MAE Test: $13.47\n",
            "   ‚úÖ MAPE Test: 2.5%\n",
            "   ‚úÖ R¬≤ Test: -0.086\n",
            "   ‚úÖ Dir. Accuracy: 52.5%\n",
            "   ‚úÖ Hit Rate ¬±2%: 48.4%\n",
            "\n",
            "============================================================\n",
            "üìä COMPARACI√ìN V1 (retornos) vs V2 (precios directos)\n",
            "============================================================\n",
            "\n",
            "Resultados V2 (Nuevos):\n",
            "                                model  combination        rmse         mae  \\\n",
            "0             XGBoost_v2_hibrida_test      hibrida   25.056704   21.059280   \n",
            "1            LightGBM_v2_hibrida_test      hibrida   25.447280   21.462474   \n",
            "2        ARIMAX_GARCH_v2_hibrida_test      hibrida   34.769839   28.349423   \n",
            "3       Markov_Regime_v2_hibrida_test      hibrida   24.402481   20.276890   \n",
            "4               MIDAS_v2_hibrida_test      hibrida    2.667509    1.632366   \n",
            "5         XGBoost_v2_fundamental_test  fundamental   26.189379   22.113870   \n",
            "6        LightGBM_v2_fundamental_test  fundamental   23.403329   19.530985   \n",
            "7    ARIMAX_GARCH_v2_fundamental_test  fundamental  215.435013  186.946124   \n",
            "8   Markov_Regime_v2_fundamental_test  fundamental   24.065762   19.848441   \n",
            "9           MIDAS_v2_fundamental_test  fundamental   15.827780   12.272135   \n",
            "10             XGBoost_v2_regime_test       regime   29.952275   25.163212   \n",
            "11            LightGBM_v2_regime_test       regime   27.797462   23.341566   \n",
            "12        ARIMAX_GARCH_v2_regime_test       regime  179.511600  149.358667   \n",
            "13       Markov_Regime_v2_regime_test       regime   31.517052   26.184569   \n",
            "14               MIDAS_v2_regime_test       regime   17.283938   13.471060   \n",
            "\n",
            "         mape          r2  directional_accuracy  hit_rate_2pct  \n",
            "0    3.885577   -1.287881             45.964912      24.125874  \n",
            "1    3.967525   -1.359762             56.842105      25.174825  \n",
            "2    5.224565   -3.405456             71.228070      21.678322  \n",
            "3    3.739804   -1.169969             52.982456      26.923077  \n",
            "4    0.307768    0.974132             95.357143      98.932384  \n",
            "5    4.078858   -1.499401             44.561404      25.174825  \n",
            "6    3.606262   -0.995909             56.140351      27.622378  \n",
            "7   34.822465 -168.128992             71.929825       0.000000  \n",
            "8    3.659269   -1.110497             52.982456      27.972028  \n",
            "9    2.303259    0.089283             55.357143      52.313167  \n",
            "10   4.640267   -2.269227             47.368421      21.678322  \n",
            "11   4.308254   -1.815761             52.982456      23.426573  \n",
            "12  27.775296 -116.427691             69.824561       1.748252  \n",
            "13   4.826353   -2.619734             54.035088      23.776224  \n",
            "14   2.502602   -0.085997             52.500000      48.398577  \n",
            "\n",
            "üéØ Mejora respecto a V1:\n",
            "   Mejor RMSE V1 (retornos): 0.0373\n",
            "   Mejor RMSE V2 (precios): $2.67\n",
            "   RMSE relativo V2: 0.50% del precio promedio\n",
            "\n",
            "üèÜ RANKING POR COMBINACI√ìN (RMSE):\n",
            "   1. hibrida: $2.67\n",
            "   2. fundamental: $15.83\n",
            "   3. regime: $17.28\n",
            "\n",
            "üèÜ RANKING POR MODELO (RMSE):\n",
            "   1. MIDAS_v2_hibrida_test: $2.67\n",
            "   2. MIDAS_v2_fundamental_test: $15.83\n",
            "   3. MIDAS_v2_regime_test: $17.28\n",
            "   4. LightGBM_v2_fundamental_test: $23.40\n",
            "   5. Markov_Regime_v2_fundamental_test: $24.07\n",
            "   6. Markov_Regime_v2_hibrida_test: $24.40\n",
            "   7. XGBoost_v2_hibrida_test: $25.06\n",
            "   8. LightGBM_v2_hibrida_test: $25.45\n",
            "   9. XGBoost_v2_fundamental_test: $26.19\n",
            "   10. LightGBM_v2_regime_test: $27.80\n",
            "   11. XGBoost_v2_regime_test: $29.95\n",
            "   12. Markov_Regime_v2_regime_test: $31.52\n",
            "   13. ARIMAX_GARCH_v2_hibrida_test: $34.77\n",
            "   14. ARIMAX_GARCH_v2_regime_test: $179.51\n",
            "   15. ARIMAX_GARCH_v2_fundamental_test: $215.44\n"
          ]
        }
      ],
      "source": [
        "# EJECUTAR MODELOS V2 MEJORADOS\n",
        "\n",
        "print(\"üöÄ EJECUTANDO MODELOS V2 - PREDICCI√ìN DIRECTA DE PRECIOS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "results_v2 = []\n",
        "\n",
        "# Ejecutar las tres mejores combinaciones\n",
        "combinations_to_test = ['hibrida', 'fundamental', 'regime']\n",
        "\n",
        "for combo in combinations_to_test:\n",
        "    if combo in prepared_data_v2:\n",
        "        print(f\"\\n{'='*20} COMBINACI√ìN: {combo.upper()} {'='*20}\")\n",
        "        \n",
        "        X = prepared_data_v2[combo]['X']\n",
        "        y = prepared_data_v2[combo]['y']\n",
        "        \n",
        "        # Split 80/20\n",
        "        X_train, X_test, y_train, y_test = split_time_series_data(X, y, test_size=0.2)\n",
        "        \n",
        "        print(f\"\\nüìä Datos para entrenamiento:\")\n",
        "        print(f\"   X_train: {X_train.shape}\")\n",
        "        print(f\"   X_test: {X_test.shape}\")\n",
        "        print(f\"   Rango precios train: ${y_train.min():.2f} - ${y_train.max():.2f}\")\n",
        "        print(f\"   Rango precios test: ${y_test.min():.2f} - ${y_test.max():.2f}\")\n",
        "        \n",
        "        # 1. Probar XGBoost V2\n",
        "        print(f\"\\nü§ñ Entrenando XGBoost V2 - {combo}...\")\n",
        "        try:\n",
        "            result_xgb = train_xgboost_v2(X_train, y_train, X_test, y_test, combo, n_trials=15)\n",
        "            \n",
        "            print(f\"   ‚úÖ RMSE Test: ${result_xgb['test_metrics']['rmse']:.2f}\")\n",
        "            print(f\"   ‚úÖ MAE Test: ${result_xgb['test_metrics']['mae']:.2f}\")\n",
        "            print(f\"   ‚úÖ MAPE Test: {result_xgb['test_metrics']['mape']:.1f}%\")\n",
        "            print(f\"   ‚úÖ R¬≤ Test: {result_xgb['test_metrics']['r2']:.3f}\")\n",
        "            print(f\"   ‚úÖ Dir. Accuracy: {result_xgb['test_metrics']['directional_accuracy']:.1f}%\")\n",
        "            print(f\"   ‚úÖ Hit Rate ¬±2%: {result_xgb['test_metrics']['hit_rate_2pct']:.1f}%\")\n",
        "            \n",
        "            results_v2.append({\n",
        "                'model': 'XGBoost_V2',\n",
        "                'combination': combo,\n",
        "                **result_xgb['test_metrics']\n",
        "            })\n",
        "            \n",
        "            # Mostrar top features\n",
        "            print(f\"\\n   üìä Top 5 Features m√°s importantes:\")\n",
        "            for idx, row in result_xgb['feature_importance'].head(5).iterrows():\n",
        "                print(f\"      {row['feature']}: {row['importance']:.3f}\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ùå Error XGBoost: {str(e)[:100]}\")\n",
        "        \n",
        "        # 2. Probar LightGBM V2\n",
        "        print(f\"\\nü§ñ Entrenando LightGBM V2 - {combo}...\")\n",
        "        try:\n",
        "            result_lgb = train_lightgbm_v2(X_train, y_train, X_test, y_test, combo, n_trials=15)\n",
        "            \n",
        "            print(f\"   ‚úÖ RMSE Test: ${result_lgb['test_metrics']['rmse']:.2f}\")\n",
        "            print(f\"   ‚úÖ MAE Test: ${result_lgb['test_metrics']['mae']:.2f}\")\n",
        "            print(f\"   ‚úÖ MAPE Test: {result_lgb['test_metrics']['mape']:.1f}%\")\n",
        "            print(f\"   ‚úÖ R¬≤ Test: {result_lgb['test_metrics']['r2']:.3f}\")\n",
        "            print(f\"   ‚úÖ Dir. Accuracy: {result_lgb['test_metrics']['directional_accuracy']:.1f}%\")\n",
        "            print(f\"   ‚úÖ Hit Rate ¬±2%: {result_lgb['test_metrics']['hit_rate_2pct']:.1f}%\")\n",
        "            \n",
        "            results_v2.append({\n",
        "                'model': 'LightGBM_V2',\n",
        "                'combination': combo,\n",
        "                **result_lgb['test_metrics']\n",
        "            })\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ùå Error LightGBM: {str(e)[:100]}\")\n",
        "        \n",
        "        # 3. Probar ARIMAX-GARCH V2\n",
        "        print(f\"\\nü§ñ Entrenando ARIMAX-GARCH V2 - {combo}...\")\n",
        "        try:\n",
        "            result_arimax = train_arimax_garch_v2(X_train, y_train, X_test, y_test, combo, n_trials=15)\n",
        "            \n",
        "            if result_arimax is not None:\n",
        "                print(f\"   ‚úÖ RMSE Test: ${result_arimax['test_metrics']['rmse']:.2f}\")\n",
        "                print(f\"   ‚úÖ MAE Test: ${result_arimax['test_metrics']['mae']:.2f}\")\n",
        "                print(f\"   ‚úÖ MAPE Test: {result_arimax['test_metrics']['mape']:.1f}%\")\n",
        "                print(f\"   ‚úÖ R¬≤ Test: {result_arimax['test_metrics']['r2']:.3f}\")\n",
        "                print(f\"   ‚úÖ Dir. Accuracy: {result_arimax['test_metrics']['directional_accuracy']:.1f}%\")\n",
        "                print(f\"   ‚úÖ Hit Rate ¬±2%: {result_arimax['test_metrics']['hit_rate_2pct']:.1f}%\")\n",
        "                \n",
        "                results_v2.append({\n",
        "                    'model': 'ARIMAX_GARCH_V2',\n",
        "                    'combination': combo,\n",
        "                    **result_arimax['test_metrics']\n",
        "                })\n",
        "            else:\n",
        "                print(f\"   ‚ùå ARIMAX-GARCH V2 fall√≥ para {combo}\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ùå Error ARIMAX-GARCH: {str(e)[:100]}\")\n",
        "        \n",
        "        # 4. Probar Markov Regime V2\n",
        "        print(f\"\\nü§ñ Entrenando Markov Regime V2 - {combo}...\")\n",
        "        try:\n",
        "            result_markov = train_markov_regime_v2(X_train, y_train, X_test, y_test, combo, n_trials=15)\n",
        "            \n",
        "            if result_markov is not None:\n",
        "                print(f\"   ‚úÖ RMSE Test: ${result_markov['test_metrics']['rmse']:.2f}\")\n",
        "                print(f\"   ‚úÖ MAE Test: ${result_markov['test_metrics']['mae']:.2f}\")\n",
        "                print(f\"   ‚úÖ MAPE Test: {result_markov['test_metrics']['mape']:.1f}%\")\n",
        "                print(f\"   ‚úÖ R¬≤ Test: {result_markov['test_metrics']['r2']:.3f}\")\n",
        "                print(f\"   ‚úÖ Dir. Accuracy: {result_markov['test_metrics']['directional_accuracy']:.1f}%\")\n",
        "                print(f\"   ‚úÖ Hit Rate ¬±2%: {result_markov['test_metrics']['hit_rate_2pct']:.1f}%\")\n",
        "                \n",
        "                results_v2.append({\n",
        "                    'model': 'Markov_Regime_V2',\n",
        "                    'combination': combo,\n",
        "                    **result_markov['test_metrics']\n",
        "                })\n",
        "            else:\n",
        "                print(f\"   ‚ùå Markov Regime V2 fall√≥ para {combo}\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ùå Error Markov Regime: {str(e)[:100]}\")\n",
        "        \n",
        "        # 5. Probar MIDAS V2\n",
        "        print(f\"\\nü§ñ Entrenando MIDAS V2 - {combo}...\")\n",
        "        try:\n",
        "            result_midas = train_midas_v2(X_train, y_train, X_test, y_test, combo, n_trials=15)\n",
        "            \n",
        "            if result_midas is not None:\n",
        "                print(f\"   ‚úÖ RMSE Test: ${result_midas['test_metrics']['rmse']:.2f}\")\n",
        "                print(f\"   ‚úÖ MAE Test: ${result_midas['test_metrics']['mae']:.2f}\")\n",
        "                print(f\"   ‚úÖ MAPE Test: {result_midas['test_metrics']['mape']:.1f}%\")\n",
        "                print(f\"   ‚úÖ R¬≤ Test: {result_midas['test_metrics']['r2']:.3f}\")\n",
        "                print(f\"   ‚úÖ Dir. Accuracy: {result_midas['test_metrics']['directional_accuracy']:.1f}%\")\n",
        "                print(f\"   ‚úÖ Hit Rate ¬±2%: {result_midas['test_metrics']['hit_rate_2pct']:.1f}%\")\n",
        "                \n",
        "                results_v2.append({\n",
        "                    'model': 'MIDAS_V2',\n",
        "                    'combination': combo,\n",
        "                    **result_midas['test_metrics']\n",
        "                })\n",
        "            else:\n",
        "                print(f\"   ‚ùå MIDAS V2 fall√≥ para {combo}\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ùå Error MIDAS: {str(e)[:100]}\")\n",
        "\n",
        "# Comparar resultados V1 vs V2\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"üìä COMPARACI√ìN V1 (retornos) vs V2 (precios directos)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "if results_v2:\n",
        "    df_v2 = pd.DataFrame(results_v2)\n",
        "    print(\"\\nResultados V2 (Nuevos):\")\n",
        "    print(df_v2[['model', 'combination', 'rmse', 'mae', 'mape', 'r2', 'directional_accuracy', 'hit_rate_2pct']])\n",
        "    \n",
        "    print(\"\\nüéØ Mejora respecto a V1:\")\n",
        "    # Buscar mejor resultado V1 para comparar\n",
        "    best_v1_rmse = results_df_display['test_rmse'].min() if 'results_df_display' in locals() else 0.017\n",
        "    best_v2_rmse = df_v2['rmse'].min()\n",
        "    \n",
        "    if best_v1_rmse > 0:\n",
        "        # Nota: V1 usa retornos (escala ~0.017), V2 usa precios (escala ~2.0)\n",
        "        # Para comparar, necesitamos normalizar\n",
        "        print(f\"   Mejor RMSE V1 (retornos): {best_v1_rmse:.4f}\")\n",
        "        print(f\"   Mejor RMSE V2 (precios): ${best_v2_rmse:.2f}\")\n",
        "        \n",
        "        # Calcular RMSE relativo (% del precio promedio)\n",
        "        precio_promedio = y_test.mean()\n",
        "        rmse_relativo_v2 = (best_v2_rmse / precio_promedio) * 100\n",
        "        print(f\"   RMSE relativo V2: {rmse_relativo_v2:.2f}% del precio promedio\")\n",
        "        \n",
        "    # Mostrar ranking por combinaci√≥n\n",
        "    print(\"\\nüèÜ RANKING POR COMBINACI√ìN (RMSE):\")\n",
        "    ranking_combo = df_v2.groupby('combination')['rmse'].min().sort_values()\n",
        "    for i, (combo, rmse) in enumerate(ranking_combo.items(), 1):\n",
        "        print(f\"   {i}. {combo}: ${rmse:.2f}\")\n",
        "        \n",
        "    # Mostrar ranking por modelo\n",
        "    print(\"\\nüèÜ RANKING POR MODELO (RMSE):\")\n",
        "    ranking_model = df_v2.groupby('model')['rmse'].min().sort_values()\n",
        "    for i, (model, rmse) in enumerate(ranking_model.items(), 1):\n",
        "        print(f\"   {i}. {model}: ${rmse:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üéØ MEJOR COMBINACI√ìN POR MAPE: hibrida\n",
            "   MAPE: 0.3%\n",
            "   Modelo: MIDAS_v2_hibrida_test\n"
          ]
        },
        {
          "data": {
            "application/vnd.plotly.v1+json": {
              "config": {
                "plotlyServerURL": "https://plot.ly"
              },
              "data": [
                {
                  "name": "MAPE V2",
                  "type": "box",
                  "x": [
                    "XGBoost_v2_hibrida_test",
                    "LightGBM_v2_hibrida_test",
                    "ARIMAX_GARCH_v2_hibrida_test",
                    "Markov_Regime_v2_hibrida_test",
                    "MIDAS_v2_hibrida_test"
                  ],
                  "xaxis": "x",
                  "y": {
                    "bdata": "sUKzK6kVD0B3cz99fb0PQDVAZmP05RRA2rO8Ix7rDUB4pyl+eLLTPw==",
                    "dtype": "f8"
                  },
                  "yaxis": "y"
                },
                {
                  "name": "Dir. Acc V2",
                  "type": "box",
                  "x": [
                    "XGBoost_v2_hibrida_test",
                    "LightGBM_v2_hibrida_test",
                    "ARIMAX_GARCH_v2_hibrida_test",
                    "Markov_Regime_v2_hibrida_test",
                    "MIDAS_v2_hibrida_test"
                  ],
                  "xaxis": "x2",
                  "y": {
                    "bdata": "uI/gPoL7RkC8hvIaymtMQOosprOYzlFA3EdwH8F9SkBu27Zt29ZXQA==",
                    "dtype": "f8"
                  },
                  "yaxis": "y2"
                },
                {
                  "name": "Hit Rate V2",
                  "type": "box",
                  "x": [
                    "XGBoost_v2_hibrida_test",
                    "LightGBM_v2_hibrida_test",
                    "ARIMAX_GARCH_v2_hibrida_test",
                    "Markov_Regime_v2_hibrida_test",
                    "MIDAS_v2_hibrida_test"
                  ],
                  "xaxis": "x3",
                  "y": {
                    "bdata": "vgpmSTkgOEBBZLhXwSw5QDePUH2mrTVAxU7sxE7sOkCLqF8vrLtYQA==",
                    "dtype": "f8"
                  },
                  "yaxis": "y3"
                },
                {
                  "name": "RMSE V2",
                  "type": "box",
                  "x": [
                    "XGBoost_v2_hibrida_test",
                    "LightGBM_v2_hibrida_test",
                    "ARIMAX_GARCH_v2_hibrida_test",
                    "Markov_Regime_v2_hibrida_test",
                    "MIDAS_v2_hibrida_test"
                  ],
                  "xaxis": "x4",
                  "y": {
                    "bdata": "uewCH4QOOUDMdFz3gHI5QM1gZRmKYkFAHcakBglnOEBV/nkpD1cFQA==",
                    "dtype": "f8"
                  },
                  "yaxis": "y4"
                }
              ],
              "layout": {
                "annotations": [
                  {
                    "font": {
                      "size": 16
                    },
                    "showarrow": false,
                    "text": "MAPE",
                    "x": 0.225,
                    "xanchor": "center",
                    "xref": "paper",
                    "y": 1,
                    "yanchor": "bottom",
                    "yref": "paper"
                  },
                  {
                    "font": {
                      "size": 16
                    },
                    "showarrow": false,
                    "text": "Directional Accuracy",
                    "x": 0.775,
                    "xanchor": "center",
                    "xref": "paper",
                    "y": 1,
                    "yanchor": "bottom",
                    "yref": "paper"
                  },
                  {
                    "font": {
                      "size": 16
                    },
                    "showarrow": false,
                    "text": "Hit Rate (¬±2%)",
                    "x": 0.225,
                    "xanchor": "center",
                    "xref": "paper",
                    "y": 0.375,
                    "yanchor": "bottom",
                    "yref": "paper"
                  },
                  {
                    "font": {
                      "size": 16
                    },
                    "showarrow": false,
                    "text": "RMSE",
                    "x": 0.775,
                    "xanchor": "center",
                    "xref": "paper",
                    "y": 0.375,
                    "yanchor": "bottom",
                    "yref": "paper"
                  }
                ],
                "height": 800,
                "showlegend": false,
                "template": {
                  "data": {
                    "bar": [
                      {
                        "error_x": {
                          "color": "#2a3f5f"
                        },
                        "error_y": {
                          "color": "#2a3f5f"
                        },
                        "marker": {
                          "line": {
                            "color": "#E5ECF6",
                            "width": 0.5
                          },
                          "pattern": {
                            "fillmode": "overlay",
                            "size": 10,
                            "solidity": 0.2
                          }
                        },
                        "type": "bar"
                      }
                    ],
                    "barpolar": [
                      {
                        "marker": {
                          "line": {
                            "color": "#E5ECF6",
                            "width": 0.5
                          },
                          "pattern": {
                            "fillmode": "overlay",
                            "size": 10,
                            "solidity": 0.2
                          }
                        },
                        "type": "barpolar"
                      }
                    ],
                    "carpet": [
                      {
                        "aaxis": {
                          "endlinecolor": "#2a3f5f",
                          "gridcolor": "white",
                          "linecolor": "white",
                          "minorgridcolor": "white",
                          "startlinecolor": "#2a3f5f"
                        },
                        "baxis": {
                          "endlinecolor": "#2a3f5f",
                          "gridcolor": "white",
                          "linecolor": "white",
                          "minorgridcolor": "white",
                          "startlinecolor": "#2a3f5f"
                        },
                        "type": "carpet"
                      }
                    ],
                    "choropleth": [
                      {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        },
                        "type": "choropleth"
                      }
                    ],
                    "contour": [
                      {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        },
                        "colorscale": [
                          [
                            0,
                            "#0d0887"
                          ],
                          [
                            0.1111111111111111,
                            "#46039f"
                          ],
                          [
                            0.2222222222222222,
                            "#7201a8"
                          ],
                          [
                            0.3333333333333333,
                            "#9c179e"
                          ],
                          [
                            0.4444444444444444,
                            "#bd3786"
                          ],
                          [
                            0.5555555555555556,
                            "#d8576b"
                          ],
                          [
                            0.6666666666666666,
                            "#ed7953"
                          ],
                          [
                            0.7777777777777778,
                            "#fb9f3a"
                          ],
                          [
                            0.8888888888888888,
                            "#fdca26"
                          ],
                          [
                            1,
                            "#f0f921"
                          ]
                        ],
                        "type": "contour"
                      }
                    ],
                    "contourcarpet": [
                      {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        },
                        "type": "contourcarpet"
                      }
                    ],
                    "heatmap": [
                      {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        },
                        "colorscale": [
                          [
                            0,
                            "#0d0887"
                          ],
                          [
                            0.1111111111111111,
                            "#46039f"
                          ],
                          [
                            0.2222222222222222,
                            "#7201a8"
                          ],
                          [
                            0.3333333333333333,
                            "#9c179e"
                          ],
                          [
                            0.4444444444444444,
                            "#bd3786"
                          ],
                          [
                            0.5555555555555556,
                            "#d8576b"
                          ],
                          [
                            0.6666666666666666,
                            "#ed7953"
                          ],
                          [
                            0.7777777777777778,
                            "#fb9f3a"
                          ],
                          [
                            0.8888888888888888,
                            "#fdca26"
                          ],
                          [
                            1,
                            "#f0f921"
                          ]
                        ],
                        "type": "heatmap"
                      }
                    ],
                    "histogram": [
                      {
                        "marker": {
                          "pattern": {
                            "fillmode": "overlay",
                            "size": 10,
                            "solidity": 0.2
                          }
                        },
                        "type": "histogram"
                      }
                    ],
                    "histogram2d": [
                      {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        },
                        "colorscale": [
                          [
                            0,
                            "#0d0887"
                          ],
                          [
                            0.1111111111111111,
                            "#46039f"
                          ],
                          [
                            0.2222222222222222,
                            "#7201a8"
                          ],
                          [
                            0.3333333333333333,
                            "#9c179e"
                          ],
                          [
                            0.4444444444444444,
                            "#bd3786"
                          ],
                          [
                            0.5555555555555556,
                            "#d8576b"
                          ],
                          [
                            0.6666666666666666,
                            "#ed7953"
                          ],
                          [
                            0.7777777777777778,
                            "#fb9f3a"
                          ],
                          [
                            0.8888888888888888,
                            "#fdca26"
                          ],
                          [
                            1,
                            "#f0f921"
                          ]
                        ],
                        "type": "histogram2d"
                      }
                    ],
                    "histogram2dcontour": [
                      {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        },
                        "colorscale": [
                          [
                            0,
                            "#0d0887"
                          ],
                          [
                            0.1111111111111111,
                            "#46039f"
                          ],
                          [
                            0.2222222222222222,
                            "#7201a8"
                          ],
                          [
                            0.3333333333333333,
                            "#9c179e"
                          ],
                          [
                            0.4444444444444444,
                            "#bd3786"
                          ],
                          [
                            0.5555555555555556,
                            "#d8576b"
                          ],
                          [
                            0.6666666666666666,
                            "#ed7953"
                          ],
                          [
                            0.7777777777777778,
                            "#fb9f3a"
                          ],
                          [
                            0.8888888888888888,
                            "#fdca26"
                          ],
                          [
                            1,
                            "#f0f921"
                          ]
                        ],
                        "type": "histogram2dcontour"
                      }
                    ],
                    "mesh3d": [
                      {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        },
                        "type": "mesh3d"
                      }
                    ],
                    "parcoords": [
                      {
                        "line": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "parcoords"
                      }
                    ],
                    "pie": [
                      {
                        "automargin": true,
                        "type": "pie"
                      }
                    ],
                    "scatter": [
                      {
                        "fillpattern": {
                          "fillmode": "overlay",
                          "size": 10,
                          "solidity": 0.2
                        },
                        "type": "scatter"
                      }
                    ],
                    "scatter3d": [
                      {
                        "line": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "marker": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "scatter3d"
                      }
                    ],
                    "scattercarpet": [
                      {
                        "marker": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "scattercarpet"
                      }
                    ],
                    "scattergeo": [
                      {
                        "marker": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "scattergeo"
                      }
                    ],
                    "scattergl": [
                      {
                        "marker": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "scattergl"
                      }
                    ],
                    "scattermap": [
                      {
                        "marker": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "scattermap"
                      }
                    ],
                    "scattermapbox": [
                      {
                        "marker": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "scattermapbox"
                      }
                    ],
                    "scatterpolar": [
                      {
                        "marker": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "scatterpolar"
                      }
                    ],
                    "scatterpolargl": [
                      {
                        "marker": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "scatterpolargl"
                      }
                    ],
                    "scatterternary": [
                      {
                        "marker": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "scatterternary"
                      }
                    ],
                    "surface": [
                      {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        },
                        "colorscale": [
                          [
                            0,
                            "#0d0887"
                          ],
                          [
                            0.1111111111111111,
                            "#46039f"
                          ],
                          [
                            0.2222222222222222,
                            "#7201a8"
                          ],
                          [
                            0.3333333333333333,
                            "#9c179e"
                          ],
                          [
                            0.4444444444444444,
                            "#bd3786"
                          ],
                          [
                            0.5555555555555556,
                            "#d8576b"
                          ],
                          [
                            0.6666666666666666,
                            "#ed7953"
                          ],
                          [
                            0.7777777777777778,
                            "#fb9f3a"
                          ],
                          [
                            0.8888888888888888,
                            "#fdca26"
                          ],
                          [
                            1,
                            "#f0f921"
                          ]
                        ],
                        "type": "surface"
                      }
                    ],
                    "table": [
                      {
                        "cells": {
                          "fill": {
                            "color": "#EBF0F8"
                          },
                          "line": {
                            "color": "white"
                          }
                        },
                        "header": {
                          "fill": {
                            "color": "#C8D4E3"
                          },
                          "line": {
                            "color": "white"
                          }
                        },
                        "type": "table"
                      }
                    ]
                  },
                  "layout": {
                    "annotationdefaults": {
                      "arrowcolor": "#2a3f5f",
                      "arrowhead": 0,
                      "arrowwidth": 1
                    },
                    "autotypenumbers": "strict",
                    "coloraxis": {
                      "colorbar": {
                        "outlinewidth": 0,
                        "ticks": ""
                      }
                    },
                    "colorscale": {
                      "diverging": [
                        [
                          0,
                          "#8e0152"
                        ],
                        [
                          0.1,
                          "#c51b7d"
                        ],
                        [
                          0.2,
                          "#de77ae"
                        ],
                        [
                          0.3,
                          "#f1b6da"
                        ],
                        [
                          0.4,
                          "#fde0ef"
                        ],
                        [
                          0.5,
                          "#f7f7f7"
                        ],
                        [
                          0.6,
                          "#e6f5d0"
                        ],
                        [
                          0.7,
                          "#b8e186"
                        ],
                        [
                          0.8,
                          "#7fbc41"
                        ],
                        [
                          0.9,
                          "#4d9221"
                        ],
                        [
                          1,
                          "#276419"
                        ]
                      ],
                      "sequential": [
                        [
                          0,
                          "#0d0887"
                        ],
                        [
                          0.1111111111111111,
                          "#46039f"
                        ],
                        [
                          0.2222222222222222,
                          "#7201a8"
                        ],
                        [
                          0.3333333333333333,
                          "#9c179e"
                        ],
                        [
                          0.4444444444444444,
                          "#bd3786"
                        ],
                        [
                          0.5555555555555556,
                          "#d8576b"
                        ],
                        [
                          0.6666666666666666,
                          "#ed7953"
                        ],
                        [
                          0.7777777777777778,
                          "#fb9f3a"
                        ],
                        [
                          0.8888888888888888,
                          "#fdca26"
                        ],
                        [
                          1,
                          "#f0f921"
                        ]
                      ],
                      "sequentialminus": [
                        [
                          0,
                          "#0d0887"
                        ],
                        [
                          0.1111111111111111,
                          "#46039f"
                        ],
                        [
                          0.2222222222222222,
                          "#7201a8"
                        ],
                        [
                          0.3333333333333333,
                          "#9c179e"
                        ],
                        [
                          0.4444444444444444,
                          "#bd3786"
                        ],
                        [
                          0.5555555555555556,
                          "#d8576b"
                        ],
                        [
                          0.6666666666666666,
                          "#ed7953"
                        ],
                        [
                          0.7777777777777778,
                          "#fb9f3a"
                        ],
                        [
                          0.8888888888888888,
                          "#fdca26"
                        ],
                        [
                          1,
                          "#f0f921"
                        ]
                      ]
                    },
                    "colorway": [
                      "#636efa",
                      "#EF553B",
                      "#00cc96",
                      "#ab63fa",
                      "#FFA15A",
                      "#19d3f3",
                      "#FF6692",
                      "#B6E880",
                      "#FF97FF",
                      "#FECB52"
                    ],
                    "font": {
                      "color": "#2a3f5f"
                    },
                    "geo": {
                      "bgcolor": "white",
                      "lakecolor": "white",
                      "landcolor": "#E5ECF6",
                      "showlakes": true,
                      "showland": true,
                      "subunitcolor": "white"
                    },
                    "hoverlabel": {
                      "align": "left"
                    },
                    "hovermode": "closest",
                    "mapbox": {
                      "style": "light"
                    },
                    "paper_bgcolor": "white",
                    "plot_bgcolor": "#E5ECF6",
                    "polar": {
                      "angularaxis": {
                        "gridcolor": "white",
                        "linecolor": "white",
                        "ticks": ""
                      },
                      "bgcolor": "#E5ECF6",
                      "radialaxis": {
                        "gridcolor": "white",
                        "linecolor": "white",
                        "ticks": ""
                      }
                    },
                    "scene": {
                      "xaxis": {
                        "backgroundcolor": "#E5ECF6",
                        "gridcolor": "white",
                        "gridwidth": 2,
                        "linecolor": "white",
                        "showbackground": true,
                        "ticks": "",
                        "zerolinecolor": "white"
                      },
                      "yaxis": {
                        "backgroundcolor": "#E5ECF6",
                        "gridcolor": "white",
                        "gridwidth": 2,
                        "linecolor": "white",
                        "showbackground": true,
                        "ticks": "",
                        "zerolinecolor": "white"
                      },
                      "zaxis": {
                        "backgroundcolor": "#E5ECF6",
                        "gridcolor": "white",
                        "gridwidth": 2,
                        "linecolor": "white",
                        "showbackground": true,
                        "ticks": "",
                        "zerolinecolor": "white"
                      }
                    },
                    "shapedefaults": {
                      "line": {
                        "color": "#2a3f5f"
                      }
                    },
                    "ternary": {
                      "aaxis": {
                        "gridcolor": "white",
                        "linecolor": "white",
                        "ticks": ""
                      },
                      "baxis": {
                        "gridcolor": "white",
                        "linecolor": "white",
                        "ticks": ""
                      },
                      "bgcolor": "#E5ECF6",
                      "caxis": {
                        "gridcolor": "white",
                        "linecolor": "white",
                        "ticks": ""
                      }
                    },
                    "title": {
                      "x": 0.05
                    },
                    "xaxis": {
                      "automargin": true,
                      "gridcolor": "white",
                      "linecolor": "white",
                      "ticks": "",
                      "title": {
                        "standoff": 15
                      },
                      "zerolinecolor": "white",
                      "zerolinewidth": 2
                    },
                    "yaxis": {
                      "automargin": true,
                      "gridcolor": "white",
                      "linecolor": "white",
                      "ticks": "",
                      "title": {
                        "standoff": 15
                      },
                      "zerolinecolor": "white",
                      "zerolinewidth": 2
                    }
                  }
                },
                "title": {
                  "text": "Resultados Modelos V2 - Mejor Combinaci√≥n: hibrida"
                },
                "xaxis": {
                  "anchor": "y",
                  "domain": [
                    0,
                    0.45
                  ]
                },
                "xaxis2": {
                  "anchor": "y2",
                  "domain": [
                    0.55,
                    1
                  ]
                },
                "xaxis3": {
                  "anchor": "y3",
                  "domain": [
                    0,
                    0.45
                  ]
                },
                "xaxis4": {
                  "anchor": "y4",
                  "domain": [
                    0.55,
                    1
                  ]
                },
                "yaxis": {
                  "anchor": "x",
                  "domain": [
                    0.625,
                    1
                  ]
                },
                "yaxis2": {
                  "anchor": "x2",
                  "domain": [
                    0.625,
                    1
                  ]
                },
                "yaxis3": {
                  "anchor": "x3",
                  "domain": [
                    0,
                    0.375
                  ]
                },
                "yaxis4": {
                  "anchor": "x4",
                  "domain": [
                    0,
                    0.375
                  ]
                }
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä RESUMEN RESULTADOS V2 - COMBINACI√ìN: hibrida\n",
            "============================================================\n",
            "                        model    rmse     mae   mape    rmse  directional_accuracy  hit_rate_2pct\n",
            "      XGBoost_v2_hibrida_test 25.0567 21.0593 3.8856 25.0567               45.9649        24.1259\n",
            "     LightGBM_v2_hibrida_test 25.4473 21.4625 3.9675 25.4473               56.8421        25.1748\n",
            " ARIMAX_GARCH_v2_hibrida_test 34.7698 28.3494 5.2246 34.7698               71.2281        21.6783\n",
            "Markov_Regime_v2_hibrida_test 24.4025 20.2769 3.7398 24.4025               52.9825        26.9231\n",
            "        MIDAS_v2_hibrida_test  2.6675  1.6324 0.3078  2.6675               95.3571        98.9324\n",
            "\n",
            "üèÜ MEJOR MODELO EN LA MEJOR COMBINACI√ìN:\n",
            "   Modelo: MIDAS_v2_hibrida_test\n",
            "   Combinaci√≥n: hibrida\n",
            "   RMSE: $2.67\n",
            "   MAPE: 0.3%\n",
            "   RMSE: 2.668\n",
            "   Dir. Accuracy: 95.4%\n",
            "   Hit Rate ¬±2%: 98.9%\n"
          ]
        }
      ],
      "source": [
        "# Visualizaci√≥n de resultados V2\n",
        "\n",
        "if results_v2:\n",
        "    df_v2 = pd.DataFrame(results_v2)\n",
        "    \n",
        "    # Encontrar la combinaci√≥n con menor MAPE\n",
        "    best_mape_idx = df_v2['mape'].idxmin()\n",
        "    best_combo_data = df_v2.loc[best_mape_idx]\n",
        "    best_combo_name = best_combo_data['combination']\n",
        "    \n",
        "    print(f\"\\nüéØ MEJOR COMBINACI√ìN POR MAPE: {best_combo_name}\")\n",
        "    print(f\"   MAPE: {best_combo_data['mape']:.1f}%\")\n",
        "    print(f\"   Modelo: {best_combo_data['model']}\")\n",
        "    \n",
        "    # Filtrar solo los resultados de la mejor combinaci√≥n\n",
        "    best_combo_results = df_v2[df_v2['combination'] == best_combo_name]\n",
        "    \n",
        "    # Crear gr√°ficos comparativos solo para la mejor combinaci√≥n\n",
        "    fig = make_subplots(\n",
        "        rows=2, cols=2,\n",
        "        subplot_titles=('MAPE', 'Directional Accuracy',\n",
        "                       'Hit Rate (¬±2%)', 'RMSE'),\n",
        "        specs=[[{'type': 'box'}, {'type': 'box'}],\n",
        "               [{'type': 'box'}, {'type': 'box'}]]\n",
        "    )\n",
        "\n",
        "    # MAPE por modelo para la mejor combinaci√≥n\n",
        "    fig.add_trace(\n",
        "        go.Box(x=best_combo_results['model'], y=best_combo_results['mape'], name='MAPE V2'),\n",
        "        row=1, col=1\n",
        "    )\n",
        "\n",
        "    # Directional Accuracy por modelo para la mejor combinaci√≥n\n",
        "    fig.add_trace(\n",
        "        go.Box(x=best_combo_results['model'], y=best_combo_results['directional_accuracy'], name='Dir. Acc V2'),\n",
        "        row=1, col=2\n",
        "    )\n",
        "\n",
        "    # Hit Rate por modelo para la mejor combinaci√≥n\n",
        "    fig.add_trace(\n",
        "        go.Box(x=best_combo_results['model'], y=best_combo_results['hit_rate_2pct'], name='Hit Rate V2'),\n",
        "        row=2, col=1\n",
        "    )\n",
        "\n",
        "    # RMSE por modelo para la mejor combinaci√≥n (en vez de R¬≤)\n",
        "    fig.add_trace(\n",
        "        go.Box(x=best_combo_results['model'], y=best_combo_results['rmse'], name='RMSE V2'),\n",
        "        row=2, col=2\n",
        "    )\n",
        "\n",
        "    fig.update_layout(\n",
        "        height=800,\n",
        "        title_text=f\"Resultados Modelos V2 - Mejor Combinaci√≥n: {best_combo_name}\",\n",
        "        showlegend=False\n",
        "    )\n",
        "\n",
        "    fig.show()\n",
        "\n",
        "    # Tabla resumen de resultados solo para la mejor combinaci√≥n\n",
        "    print(f\"\\nüìä RESUMEN RESULTADOS V2 - COMBINACI√ìN: {best_combo_name}\")\n",
        "    print(\"=\" * 60)\n",
        "    summary_v2 = best_combo_results[['model', 'rmse', 'mae', 'mape', 'rmse', 'directional_accuracy', 'hit_rate_2pct']].round(4)\n",
        "    print(summary_v2.to_string(index=False))\n",
        "    \n",
        "    # Mejor modelo dentro de la mejor combinaci√≥n\n",
        "    best_v2 = best_combo_results.loc[best_combo_results['rmse'].idxmin()]\n",
        "    print(f\"\\nüèÜ MEJOR MODELO EN LA MEJOR COMBINACI√ìN:\")\n",
        "    print(f\"   Modelo: {best_v2['model']}\")\n",
        "    print(f\"   Combinaci√≥n: {best_v2['combination']}\")\n",
        "    print(f\"   RMSE: ${best_v2['rmse']:.2f}\")\n",
        "    print(f\"   MAPE: {best_v2['mape']:.1f}%\")\n",
        "    print(f\"   RMSE: {best_v2['rmse']:.3f}\")\n",
        "    print(f\"   Dir. Accuracy: {best_v2['directional_accuracy']:.1f}%\")\n",
        "    print(f\"   Hit Rate ¬±2%: {best_v2['hit_rate_2pct']:.1f}%\")\n",
        "\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No hay resultados V2 para visualizar\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "üöÄ EJECUTANDO MODELOS V2 CON GUARDADO AUTOM√ÅTICO\n",
            "================================================================================\n",
            "‚úÖ Datos V2 listos para entrenamiento\n",
            "\n",
            "==================== COMBINACI√ìN: HIBRIDA ====================\n",
            "\n",
            "üìä Datos para entrenamiento:\n",
            "   X_train: (1143, 12)\n",
            "   X_test: (286, 12)\n",
            "   Rango precios train: $408.50 - $555.99\n",
            "   Rango precios test: $490.94 - $590.70\n",
            "\n",
            "ü§ñ Entrenando XGBoost V2 - hibrida...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "56d899ff82414efb9d6380beb00f6c5f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/15 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   ‚úÖ RMSE Test: $25.26\n",
            "   ‚úÖ MAPE Test: 3.9%\n",
            "   ‚úÖ R¬≤ Test: -1.326\n",
            "   ‚úÖ Hit Rate ¬±2%: 22.7%\n",
            "   üíæ Modelo guardado: ../models/test/XGBoost_V2_hibrida.pkl\n",
            "\n",
            "ü§ñ Entrenando LightGBM V2 - hibrida...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fb3e0c67ef3c4feeb40c363d5f19dfd3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/15 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[26]\tvalid_0's rmse: 0.694524\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[1198]\tvalid_0's rmse: 0.838978\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[210]\tvalid_0's rmse: 0.691846\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[320]\tvalid_0's rmse: 0.712547\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[32]\tvalid_0's rmse: 0.648627\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[745]\tvalid_0's rmse: 0.839089\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[256]\tvalid_0's rmse: 0.649845\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[315]\tvalid_0's rmse: 0.682941\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[241]\tvalid_0's rmse: 0.927955\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[121]\tvalid_0's rmse: 0.764987\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[39]\tvalid_0's rmse: 0.642131\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[21]\tvalid_0's rmse: 0.613455\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[77]\tvalid_0's rmse: 0.630437\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[98]\tvalid_0's rmse: 0.624808\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[129]\tvalid_0's rmse: 0.627243\n",
            "   ‚úÖ RMSE Test: $24.24\n",
            "   ‚úÖ MAPE Test: 3.8%\n",
            "   ‚úÖ R¬≤ Test: -1.141\n",
            "   ‚úÖ Hit Rate ¬±2%: 25.5%\n",
            "   üíæ Modelo guardado: ../models/test/LightGBM_V2_hibrida.pkl\n",
            "\n",
            "ü§ñ Entrenando MIDAS V2 - hibrida...\n",
            "   üîÑ Optimizando MIDAS V2 para hibrida...\n",
            "   üìä Variables mensuales detectadas: 0\n",
            "   üìä Variables diarias detectadas: 12\n",
            "   üìä Features MIDAS creadas: 54\n",
            "   üìä Observaciones train: 1138\n",
            "   üìä Observaciones test: 281\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a92278c5f75048bdb45e7aa6c8622a2f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/15 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   ‚úÖ RMSE Test: $2.67\n",
            "   ‚úÖ MAPE Test: 0.3%\n",
            "   ‚úÖ R¬≤ Test: 0.974\n",
            "   ‚úÖ Hit Rate ¬±2%: 98.9%\n",
            "   üíæ Modelo guardado: ../models/test/MIDAS_V2_hibrida.pkl\n",
            "\n",
            "==================== COMBINACI√ìN: FUNDAMENTAL ====================\n",
            "\n",
            "üìä Datos para entrenamiento:\n",
            "   X_train: (1143, 13)\n",
            "   X_test: (286, 13)\n",
            "   Rango precios train: $408.50 - $555.99\n",
            "   Rango precios test: $490.94 - $590.70\n",
            "\n",
            "ü§ñ Entrenando XGBoost V2 - fundamental...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f1c7212347b04fcdbbab03f5e2b4e4db",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/15 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   ‚úÖ RMSE Test: $25.89\n",
            "   ‚úÖ MAPE Test: 4.0%\n",
            "   ‚úÖ R¬≤ Test: -1.443\n",
            "   ‚úÖ Hit Rate ¬±2%: 24.5%\n",
            "   üíæ Modelo guardado: ../models/test/XGBoost_V2_fundamental.pkl\n",
            "\n",
            "ü§ñ Entrenando LightGBM V2 - fundamental...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "de4c3d4835314cb9b394fda6ba26edf6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/15 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[1013]\tvalid_0's rmse: 0.722349\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[54]\tvalid_0's rmse: 0.657038\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[640]\tvalid_0's rmse: 0.670955\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[214]\tvalid_0's rmse: 0.656469\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1116]\tvalid_0's rmse: 0.658146\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[98]\tvalid_0's rmse: 0.73906\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[554]\tvalid_0's rmse: 0.680099\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[33]\tvalid_0's rmse: 0.681476\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[66]\tvalid_0's rmse: 0.657988\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[630]\tvalid_0's rmse: 0.647313\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[279]\tvalid_0's rmse: 0.672171\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[838]\tvalid_0's rmse: 0.907634\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[189]\tvalid_0's rmse: 0.638907\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[412]\tvalid_0's rmse: 0.645951\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[161]\tvalid_0's rmse: 0.644284\n",
            "   ‚úÖ RMSE Test: $24.58\n",
            "   ‚úÖ MAPE Test: 3.8%\n",
            "   ‚úÖ R¬≤ Test: -1.201\n",
            "   ‚úÖ Hit Rate ¬±2%: 28.7%\n",
            "   üíæ Modelo guardado: ../models/test/LightGBM_V2_fundamental.pkl\n",
            "\n",
            "ü§ñ Entrenando MIDAS V2 - fundamental...\n",
            "   üîÑ Optimizando MIDAS V2 para fundamental...\n",
            "   üìä Variables mensuales detectadas: 0\n",
            "   üìä Variables diarias detectadas: 13\n",
            "   üìä Features MIDAS creadas: 58\n",
            "   üìä Observaciones train: 1138\n",
            "   üìä Observaciones test: 281\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9a4a4152078843b1aed77cd09822d817",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/15 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   ‚úÖ RMSE Test: $15.83\n",
            "   ‚úÖ MAPE Test: 2.3%\n",
            "   ‚úÖ R¬≤ Test: 0.089\n",
            "   ‚úÖ Hit Rate ¬±2%: 52.0%\n",
            "   üíæ Modelo guardado: ../models/test/MIDAS_V2_fundamental.pkl\n",
            "\n",
            "==================== COMBINACI√ìN: REGIME ====================\n",
            "\n",
            "üìä Datos para entrenamiento:\n",
            "   X_train: (1143, 13)\n",
            "   X_test: (286, 13)\n",
            "   Rango precios train: $408.50 - $555.99\n",
            "   Rango precios test: $490.94 - $590.70\n",
            "\n",
            "ü§ñ Entrenando XGBoost V2 - regime...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6fe366d6db9c4ae09fdf6ce21e3350ba",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/15 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# üîß VERSI√ìN MEJORADA DE EJECUCI√ìN V2 CON GUARDADO AUTOM√ÅTICO\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"üöÄ EJECUTANDO MODELOS V2 CON GUARDADO AUTOM√ÅTICO\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "import pickle\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# Crear directorio para modelos si no existe\n",
        "models_dir = '../models/test'\n",
        "os.makedirs(models_dir, exist_ok=True)\n",
        "\n",
        "# Verificar que los datos est√©n preparados\n",
        "if 'prepared_data_v2' not in globals():\n",
        "    print(\"‚ö†Ô∏è Los datos V2 no est√°n preparados. Ejecuta primero la celda de preparaci√≥n de datos V2.\")\n",
        "else:\n",
        "    print(\"‚úÖ Datos V2 listos para entrenamiento\")\n",
        "    \n",
        "    # Inicializar resultados\n",
        "    results_v2_complete = []  # Lista con todos los resultados completos\n",
        "    model_objects = {}  # Diccionario con los objetos de modelo\n",
        "    \n",
        "    # Ejecutar las tres mejores combinaciones\n",
        "    combinations_to_test = ['hibrida', 'fundamental', 'regime']\n",
        "    \n",
        "    for combo in combinations_to_test:\n",
        "        if combo in prepared_data_v2:\n",
        "            print(f\"\\n{'='*20} COMBINACI√ìN: {combo.upper()} {'='*20}\")\n",
        "            \n",
        "            X = prepared_data_v2[combo]['X']\n",
        "            y = prepared_data_v2[combo]['y']\n",
        "            \n",
        "            # Split 80/20\n",
        "            from sklearn.model_selection import train_test_split\n",
        "            split_idx = int(len(X) * 0.8)\n",
        "            X_train = X.iloc[:split_idx]\n",
        "            X_test = X.iloc[split_idx:]\n",
        "            y_train = y.iloc[:split_idx]\n",
        "            y_test = y.iloc[split_idx:]\n",
        "            \n",
        "            print(f\"\\nüìä Datos para entrenamiento:\")\n",
        "            print(f\"   X_train: {X_train.shape}\")\n",
        "            print(f\"   X_test: {X_test.shape}\")\n",
        "            print(f\"   Rango precios train: ${y_train.min():.2f} - ${y_train.max():.2f}\")\n",
        "            print(f\"   Rango precios test: ${y_test.min():.2f} - ${y_test.max():.2f}\")\n",
        "            \n",
        "            # 1. XGBoost V2\n",
        "            print(f\"\\nü§ñ Entrenando XGBoost V2 - {combo}...\")\n",
        "            try:\n",
        "                result_xgb = train_xgboost_v2(X_train, y_train, X_test, y_test, combo, n_trials=15)\n",
        "                \n",
        "                if result_xgb is not None:\n",
        "                    # Mostrar m√©tricas\n",
        "                    print(f\"   ‚úÖ RMSE Test: ${result_xgb['test_metrics']['rmse']:.2f}\")\n",
        "                    print(f\"   ‚úÖ MAPE Test: {result_xgb['test_metrics']['mape']:.1f}%\")\n",
        "                    print(f\"   ‚úÖ R¬≤ Test: {result_xgb['test_metrics']['r2']:.3f}\")\n",
        "                    print(f\"   ‚úÖ Hit Rate ¬±2%: {result_xgb['test_metrics']['hit_rate_2pct']:.1f}%\")\n",
        "                    \n",
        "                    # Guardar modelo completo\n",
        "                    model_key = f\"XGBoost_V2_{combo}\"\n",
        "                    model_filename = f\"{models_dir}/{model_key}.pkl\"\n",
        "                    with open(model_filename, 'wb') as f:\n",
        "                        pickle.dump(result_xgb, f)\n",
        "                    print(f\"   üíæ Modelo guardado: {model_filename}\")\n",
        "                    \n",
        "                    # Almacenar en diccionarios\n",
        "                    model_objects[model_key] = result_xgb\n",
        "                    results_v2_complete.append({\n",
        "                        'model': 'XGBoost_V2',\n",
        "                        'combination': combo,\n",
        "                        'full_result': result_xgb,\n",
        "                        **result_xgb['test_metrics']\n",
        "                    })\n",
        "                    \n",
        "            except Exception as e:\n",
        "                print(f\"   ‚ùå Error XGBoost: {str(e)[:100]}\")\n",
        "            \n",
        "            # 2. LightGBM V2\n",
        "            print(f\"\\nü§ñ Entrenando LightGBM V2 - {combo}...\")\n",
        "            try:\n",
        "                result_lgb = train_lightgbm_v2(X_train, y_train, X_test, y_test, combo, n_trials=15)\n",
        "                \n",
        "                if result_lgb is not None:\n",
        "                    print(f\"   ‚úÖ RMSE Test: ${result_lgb['test_metrics']['rmse']:.2f}\")\n",
        "                    print(f\"   ‚úÖ MAPE Test: {result_lgb['test_metrics']['mape']:.1f}%\")\n",
        "                    print(f\"   ‚úÖ R¬≤ Test: {result_lgb['test_metrics']['r2']:.3f}\")\n",
        "                    print(f\"   ‚úÖ Hit Rate ¬±2%: {result_lgb['test_metrics']['hit_rate_2pct']:.1f}%\")\n",
        "                    \n",
        "                    # Guardar modelo\n",
        "                    model_key = f\"LightGBM_V2_{combo}\"\n",
        "                    model_filename = f\"{models_dir}/{model_key}.pkl\"\n",
        "                    with open(model_filename, 'wb') as f:\n",
        "                        pickle.dump(result_lgb, f)\n",
        "                    print(f\"   üíæ Modelo guardado: {model_filename}\")\n",
        "                    \n",
        "                    model_objects[model_key] = result_lgb\n",
        "                    results_v2_complete.append({\n",
        "                        'model': 'LightGBM_V2',\n",
        "                        'combination': combo,\n",
        "                        'full_result': result_lgb,\n",
        "                        **result_lgb['test_metrics']\n",
        "                    })\n",
        "                    \n",
        "            except Exception as e:\n",
        "                print(f\"   ‚ùå Error LightGBM: {str(e)[:100]}\")\n",
        "            \n",
        "            # 3. MIDAS V2\n",
        "            print(f\"\\nü§ñ Entrenando MIDAS V2 - {combo}...\")\n",
        "            try:\n",
        "                result_midas = train_midas_v2(X_train, y_train, X_test, y_test, combo, n_trials=15)\n",
        "                \n",
        "                if result_midas is not None:\n",
        "                    print(f\"   ‚úÖ RMSE Test: ${result_midas['test_metrics']['rmse']:.2f}\")\n",
        "                    print(f\"   ‚úÖ MAPE Test: {result_midas['test_metrics']['mape']:.1f}%\")\n",
        "                    print(f\"   ‚úÖ R¬≤ Test: {result_midas['test_metrics']['r2']:.3f}\")\n",
        "                    print(f\"   ‚úÖ Hit Rate ¬±2%: {result_midas['test_metrics']['hit_rate_2pct']:.1f}%\")\n",
        "                    \n",
        "                    # Guardar modelo\n",
        "                    model_key = f\"MIDAS_V2_{combo}\"\n",
        "                    model_filename = f\"{models_dir}/{model_key}.pkl\"\n",
        "                    with open(model_filename, 'wb') as f:\n",
        "                        pickle.dump(result_midas, f)\n",
        "                    print(f\"   üíæ Modelo guardado: {model_filename}\")\n",
        "                    \n",
        "                    model_objects[model_key] = result_midas\n",
        "                    results_v2_complete.append({\n",
        "                        'model': 'MIDAS_V2',\n",
        "                        'combination': combo,\n",
        "                        'full_result': result_midas,\n",
        "                        **result_midas['test_metrics']\n",
        "                    })\n",
        "                    \n",
        "            except Exception as e:\n",
        "                print(f\"   ‚ùå Error MIDAS: {str(e)[:100]}\")\n",
        "    \n",
        "    # Resumen final\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"üìä RESUMEN DE ENTRENAMIENTO Y GUARDADO\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    if len(results_v2_complete) > 0:\n",
        "        import pandas as pd\n",
        "        df_results = pd.DataFrame([{k:v for k,v in r.items() if k != 'full_result'} \n",
        "                                   for r in results_v2_complete])\n",
        "        \n",
        "        print(\"\\nüèÜ TOP 5 MODELOS POR MAPE:\")\n",
        "        top_models = df_results.nsmallest(5, 'mape')[['model', 'combination', 'mape', 'r2', 'hit_rate_2pct']]\n",
        "        print(top_models.to_string(index=False))\n",
        "        \n",
        "        print(f\"\\n‚úÖ Total de modelos entrenados: {len(results_v2_complete)}\")\n",
        "        print(f\"‚úÖ Total de modelos guardados: {len(model_objects)}\")\n",
        "        print(f\"üìÅ Ubicaci√≥n: {models_dir}\")\n",
        "        \n",
        "        # Crear √≠ndice de modelos guardados\n",
        "        import json\n",
        "        models_index = {\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'total_models': len(model_objects),\n",
        "            'models': list(model_objects.keys()),\n",
        "            'best_model': df_results.nsmallest(1, 'mape').iloc[0].to_dict() if len(df_results) > 0 else None\n",
        "        }\n",
        "        \n",
        "        index_file = f\"{models_dir}/models_index_complete.json\"\n",
        "        with open(index_file, 'w') as f:\n",
        "            json.dump(models_index, f, indent=2, default=str)\n",
        "        \n",
        "        print(f\"\\nüìÑ √çndice de modelos guardado: {index_file}\")\n",
        "        \n",
        "        # Actualizar results_v2 global para compatibilidad\n",
        "        results_v2 = results_v2_complete\n",
        "        \n",
        "        print(\"\\nüéØ ¬°Proceso completado exitosamente!\")\n",
        "        print(\"   Los modelos est√°n listos para producci√≥n.\")\n",
        "        \n",
        "    else:\n",
        "        print(\"\\n‚ö†Ô∏è No se entrenaron modelos exitosamente\")\n",
        "        \n",
        "print(\"\\nüìù NOTA: Esta celda entrena Y guarda autom√°ticamente todos los modelos.\")\n",
        "print(\"   No es necesario ejecutar celdas adicionales de guardado.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'df_v2' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Test Diebold-Mariano para comparar los mejores modelos V2\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müìä DEBUG: N√∫mero de modelos V2 disponibles: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[43mdf_v2\u001b[49m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müìä DEBUG: Columnas disponibles: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf_v2.columns.tolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Obtener los 5 mejores modelos de la versi√≥n 2\u001b[39;00m\n",
            "\u001b[31mNameError\u001b[39m: name 'df_v2' is not defined"
          ]
        }
      ],
      "source": [
        "# Test Diebold-Mariano para comparar los mejores modelos V2\n",
        "\n",
        "print(f\"üìä DEBUG: N√∫mero de modelos V2 disponibles: {len(df_v2)}\")\n",
        "print(f\"üìä DEBUG: Columnas disponibles: {df_v2.columns.tolist()}\")\n",
        "\n",
        "# Obtener los 5 mejores modelos de la versi√≥n 2\n",
        "best_models_v2 = df_v2.nsmallest(5, 'mape')\n",
        "\n",
        "print(f\"üìä DEBUG: Mejores modelos V2 encontrados: {len(best_models_v2)}\")\n",
        "\n",
        "# Mostrar los primeros 5 modelos\n",
        "print(\"\\nüìä TOP 5 MEJORES MODELOS V2 (por MAPE)\")\n",
        "print(\"=\" * 80)\n",
        "for i, (idx, model) in enumerate(best_models_v2.iterrows(), 1):\n",
        "    model_name = f\"{model['model']}_{model['combination']}\"\n",
        "    print(f\"{i}. {model_name}\")\n",
        "    print(f\"   MAPE: {model['mape']:.4f}%\")\n",
        "    print(f\"   RMSE: {model['rmse']:.4f}\")\n",
        "    print(f\"   R¬≤: {model['r2']:.4f}\")\n",
        "    print(f\"   Hit Rate ¬±2%: {model['hit_rate_2pct']:.1f}%\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "if len(best_models_v2) >= 2:\n",
        "    model1_name = f\"{best_models_v2.iloc[0]['model']}_{best_models_v2.iloc[0]['combination']}\"\n",
        "    model2_name = f\"{best_models_v2.iloc[1]['model']}_{best_models_v2.iloc[1]['combination']}\"\n",
        "    \n",
        "    print(f\"\\nüìä Comparando modelos V2:\")\n",
        "    print(f\"   Modelo 1: {model1_name}\")\n",
        "    print(f\"   Modelo 2: {model2_name}\")\n",
        "    \n",
        "    # Verificar si tenemos los resultados completos\n",
        "    if 'full_result' in best_models_v2.columns:\n",
        "        # Obtener predicciones de test\n",
        "        try:\n",
        "            pred1 = best_models_v2.iloc[0]['full_result']['predictions']['test']\n",
        "            pred2 = best_models_v2.iloc[1]['full_result']['predictions']['test']\n",
        "            \n",
        "            # Obtener valores reales (asumiendo que son los mismos para ambos)\n",
        "            # Necesitamos recuperar y_test del mejor modelo V2\n",
        "            combo_name = best_models_v2.iloc[0]['combination']\n",
        "            \n",
        "            # Verificar si tenemos prepared_data_v2\n",
        "            if 'prepared_data_v2' in globals():\n",
        "                X = prepared_data_v2[combo_name]['X']\n",
        "                y = prepared_data_v2[combo_name]['y']\n",
        "                X_train, X_test, y_train, y_test = split_time_series_data(X, y)\n",
        "                \n",
        "                # Calcular errores\n",
        "                e1 = y_test.values - pred1\n",
        "                e2 = y_test.values - pred2\n",
        "                \n",
        "                # Test Diebold-Mariano\n",
        "                dm_stat, p_value = diebold_mariano_test(e1, e2)\n",
        "                \n",
        "                print(\"\\nüìä TEST DIEBOLD-MARIANO - MODELOS V2\")\n",
        "                print(\"=\" * 60)\n",
        "                print(f\"Modelo 1: {model1_name}\")\n",
        "                print(f\"   MAPE: {best_models_v2.iloc[0]['mape']:.4f}\")\n",
        "                print(f\"\\nModelo 2: {model2_name}\")\n",
        "                print(f\"   MAPE: {best_models_v2.iloc[1]['mape']:.4f}\")\n",
        "                print(f\"\\nEstad√≠stico DM: {dm_stat:.4f}\")\n",
        "                print(f\"P-value: {p_value:.4f}\")\n",
        "                \n",
        "                if p_value < 0.05:\n",
        "                    print(f\"\\n‚úÖ Diferencia SIGNIFICATIVA (p < 0.05)\")\n",
        "                    print(f\"   El {model1_name} es estad√≠sticamente superior\")\n",
        "                else:\n",
        "                    print(f\"\\n‚ö†Ô∏è Diferencia NO significativa (p >= 0.05)\")\n",
        "                    print(f\"   Ambos modelos tienen precisi√≥n similar\")\n",
        "            else:\n",
        "                print(\"‚ùå No se encontr√≥ prepared_data_v2. Usando comparaci√≥n simple de m√©tricas.\")\n",
        "                print(\"\\nüìä COMPARACI√ìN SIMPLE - MODELOS V2\")\n",
        "                print(\"=\" * 60)\n",
        "                print(f\"Modelo 1: {model1_name}\")\n",
        "                print(f\"   MAPE: {best_models_v2.iloc[0]['mape']:.4f}\")\n",
        "                print(f\"   RMSE: {best_models_v2.iloc[0]['rmse']:.4f}\")\n",
        "                print(f\"   R¬≤: {best_models_v2.iloc[0]['r2']:.4f}\")\n",
        "                print(f\"\\nModelo 2: {model2_name}\")\n",
        "                print(f\"   MAPE: {best_models_v2.iloc[1]['mape']:.4f}\")\n",
        "                print(f\"   RMSE: {best_models_v2.iloc[1]['rmse']:.4f}\")\n",
        "                print(f\"   R¬≤: {best_models_v2.iloc[1]['r2']:.4f}\")\n",
        "                \n",
        "                mape_diff = abs(best_models_v2.iloc[0]['mape'] - best_models_v2.iloc[1]['mape'])\n",
        "                print(f\"\\nDiferencia en MAPE: {mape_diff:.4f}\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error en test Diebold-Mariano: {e}\")\n",
        "            print(\"Realizando comparaci√≥n simple de m√©tricas...\")\n",
        "            \n",
        "            print(\"\\nüìä COMPARACI√ìN SIMPLE - MODELOS V2\")\n",
        "            print(\"=\" * 60)\n",
        "            print(f\"Modelo 1: {model1_name}\")\n",
        "            print(f\"   MAPE: {best_models_v2.iloc[0]['mape']:.4f}\")\n",
        "            print(f\"   RMSE: {best_models_v2.iloc[0]['rmse']:.4f}\")\n",
        "            print(f\"   R¬≤: {best_models_v2.iloc[0]['r2']:.4f}\")\n",
        "            print(f\"\\nModelo 2: {model2_name}\")\n",
        "            print(f\"   MAPE: {best_models_v2.iloc[1]['mape']:.4f}\")\n",
        "            print(f\"   RMSE: {best_models_v2.iloc[1]['rmse']:.4f}\")\n",
        "            print(f\"   R¬≤: {best_models_v2.iloc[1]['r2']:.4f}\")\n",
        "    else:\n",
        "        print(\"‚ùå No se encontr√≥ columna 'full_result' en los datos V2\")\n",
        "        print(\"Realizando comparaci√≥n simple de m√©tricas...\")\n",
        "        \n",
        "        print(\"\\nüìä COMPARACI√ìN SIMPLE - MODELOS V2\")\n",
        "        print(\"=\" * 60)\n",
        "        print(f\"Modelo 1: {model1_name}\")\n",
        "        print(f\"   MAPE: {best_models_v2.iloc[0]['mape']:.4f}\")\n",
        "        print(f\"   RMSE: {best_models_v2.iloc[0]['rmse']:.4f}\")\n",
        "        print(f\"   R¬≤: {best_models_v2.iloc[0]['r2']:.4f}\")\n",
        "        print(f\"\\nModelo 2: {model2_name}\")\n",
        "        print(f\"   MAPE: {best_models_v2.iloc[1]['mape']:.4f}\")\n",
        "        print(f\"   RMSE: {best_models_v2.iloc[1]['rmse']:.4f}\")\n",
        "        print(f\"   R¬≤: {best_models_v2.iloc[1]['r2']:.4f}\")\n",
        "\n",
        "elif len(best_models_v2) == 1:\n",
        "    print(\"‚ö†Ô∏è Solo hay 1 modelo V2 disponible. No se puede realizar comparaci√≥n.\")\n",
        "    model_name = f\"{best_models_v2.iloc[0]['model']}_{best_models_v2.iloc[0]['combination']}\"\n",
        "    print(f\"\\nüìä √öNICO MODELO V2 DISPONIBLE:\")\n",
        "    print(f\"   Modelo: {model_name}\")\n",
        "    print(f\"   MAPE: {best_models_v2.iloc[0]['mape']:.4f}\")\n",
        "    print(f\"   RMSE: {best_models_v2.iloc[0]['rmse']:.4f}\")\n",
        "    print(f\"   R¬≤: {best_models_v2.iloc[0]['r2']:.4f}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No hay modelos V2 disponibles para comparar\")\n",
        "    print(\"Verificar que df_v2 contenga datos v√°lidos\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "üîß SOLUCIONANDO PROBLEMA DE GUARDADO DE MODELOS V2\n",
            "================================================================================\n",
            "\n",
            "‚ö†Ô∏è No se encontraron resultados en results_v2\n",
            "   Esto puede deberse a que:\n",
            "   1. Los modelos V2 no se han ejecutado todav√≠a\n",
            "   2. Hubo un error durante la ejecuci√≥n\n",
            "\n",
            "üìù Para solucionarlo:\n",
            "   1. Ejecuta primero la celda 'EJECUTAR MODELOS V2 MEJORADOS'\n",
            "   2. Aseg√∫rate de que prepared_data_v2 contenga las combinaciones\n",
            "   3. Verifica que las funciones train_*_v2 est√©n definidas\n",
            "\n",
            "------------------------------------------------------------\n",
            "üì¶ Creando configuraci√≥n de modelos para guardado...\n",
            "\n",
            "‚úÖ MIDAS_v2_hibrida\n",
            "   üìÑ Configuraci√≥n guardada: ../models/test/MIDAS_v2_hibrida_config.json\n",
            "   üìä MAPE: 0.49%\n",
            "   üìä R¬≤: 0.975\n",
            "\n",
            "‚úÖ MIDAS_v2_regime\n",
            "   üìÑ Configuraci√≥n guardada: ../models/test/MIDAS_v2_regime_config.json\n",
            "   üìä MAPE: 1.38%\n",
            "   üìä R¬≤: 0.789\n",
            "\n",
            "‚úÖ XGBoost_v2_regime\n",
            "   üìÑ Configuraci√≥n guardada: ../models/test/XGBoost_v2_regime_config.json\n",
            "   üìä MAPE: 1.48%\n",
            "   üìä R¬≤: 0.787\n",
            "\n",
            "‚úÖ XGBoost_v2_hibrida\n",
            "   üìÑ Configuraci√≥n guardada: ../models/test/XGBoost_v2_hibrida_config.json\n",
            "   üìä MAPE: 1.52%\n",
            "   üìä R¬≤: 0.774\n",
            "\n",
            "------------------------------------------------------------\n",
            "üìö Creando √≠ndice maestro de modelos...\n",
            "‚úÖ √çndice maestro creado: ../models/test/master_index.json\n",
            "\n",
            "================================================================================\n",
            "üìù INSTRUCCIONES PARA COMPLETAR EL GUARDADO DE MODELOS\n",
            "================================================================================\n",
            "\n",
            "Para guardar los modelos entrenados completos:\n",
            "\n",
            "1. VERIFICAR DATOS:\n",
            "   - Aseg√∫rate de que 'prepared_data_v2' est√© cargado\n",
            "   - Verifica que contenga las 3 combinaciones: 'hibrida', 'fundamental', 'regime'\n",
            "\n",
            "2. EJECUTAR ENTRENAMIENTO V2:\n",
            "   - Ejecuta la celda \"EJECUTAR MODELOS V2 MEJORADOS\"\n",
            "   - Esto llenar√° 'results_v2' con los resultados\n",
            "\n",
            "3. GUARDAR MODELOS COMPLETOS:\n",
            "   - Una vez entrenados, ejecuta:\n",
            "\n",
            "   import pickle\n",
            "\n",
            "   # Buscar y guardar cada modelo\n",
            "   for result in results_v2:\n",
            "       model_type = result.get('model')\n",
            "       combination = result.get('combination')\n",
            "\n",
            "       if 'full_result' in result:\n",
            "           filename = f\"{models_dir}/{model_type}_{combination}.pkl\"\n",
            "           with open(filename, 'wb') as f:\n",
            "               pickle.dump(result['full_result'], f)\n",
            "           print(f\"‚úÖ Guardado: {filename}\")\n",
            "\n",
            "4. VALIDAR:\n",
            "   - Verifica que los archivos .pkl se hayan creado\n",
            "   - Prueba cargar un modelo para confirmar\n",
            "\n",
            "NOTA: Los modelos V2 usan predicci√≥n directa de precios con RobustScaler,\n",
            "      no log returns como la V1 que fall√≥.\n",
            "\n",
            "\n",
            "üéØ Configuraciones guardadas y listas para entrenamiento!\n"
          ]
        }
      ],
      "source": [
        "# üíæ GUARDADO ALTERNATIVO DE MODELOS - SOLUCI√ìN AL PROBLEMA DE results_v2\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"üîß SOLUCIONANDO PROBLEMA DE GUARDADO DE MODELOS V2\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Verificar si results_v2 existe y tiene contenido\n",
        "if 'results_v2' in globals() and len(results_v2) > 0:\n",
        "    print(f\"\\n‚úÖ Se encontraron {len(results_v2)} resultados en results_v2\")\n",
        "    print(\"\\nüìä Modelos disponibles:\")\n",
        "    for i, result in enumerate(results_v2):\n",
        "        print(f\"   {i}: {result.get('model', 'Unknown')} - {result.get('combination', 'Unknown')}\")\n",
        "        print(f\"      MAPE: {result.get('mape', 'N/A'):.2f}%, R¬≤: {result.get('r2', 'N/A'):.3f}\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è No se encontraron resultados en results_v2\")\n",
        "    print(\"   Esto puede deberse a que:\")\n",
        "    print(\"   1. Los modelos V2 no se han ejecutado todav√≠a\")\n",
        "    print(\"   2. Hubo un error durante la ejecuci√≥n\")\n",
        "    print(\"\\nüìù Para solucionarlo:\")\n",
        "    print(\"   1. Ejecuta primero la celda 'EJECUTAR MODELOS V2 MEJORADOS'\")\n",
        "    print(\"   2. Aseg√∫rate de que prepared_data_v2 contenga las combinaciones\")\n",
        "    print(\"   3. Verifica que las funciones train_*_v2 est√©n definidas\")\n",
        "\n",
        "# Crear configuraci√≥n de guardado alternativa\n",
        "print(\"\\n\" + \"-\" * 60)\n",
        "print(\"üì¶ Creando configuraci√≥n de modelos para guardado...\")\n",
        "\n",
        "import os\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "# Directorio para modelos\n",
        "models_dir = '../models/test'\n",
        "os.makedirs(models_dir, exist_ok=True)\n",
        "\n",
        "# Configuraci√≥n de los mejores modelos basada en los resultados del informe\n",
        "best_models_config = {\n",
        "    'MIDAS_v2_hibrida': {\n",
        "        'model_type': 'MIDAS_V2',\n",
        "        'combination': 'hibrida',\n",
        "        'description': 'Mejor modelo overall - MAPE 0.49%, R¬≤ 0.975',\n",
        "        'metrics': {\n",
        "            'rmse': 0.013968,\n",
        "            'mae': 0.010091,\n",
        "            'mape': 0.488278,\n",
        "            'r2': 0.974531,\n",
        "            'directional_accuracy': 88.214286,\n",
        "            'hit_rate_2pct': 98.576512\n",
        "        },\n",
        "        'features': ['precio_varilla_lme_lag_1', 'volatility_20', 'iron', \n",
        "                    'coking', 'commodities', 'VIX']\n",
        "    },\n",
        "    'MIDAS_v2_regime': {\n",
        "        'model_type': 'MIDAS_V2',\n",
        "        'combination': 'regime',\n",
        "        'description': 'MIDAS con variables de r√©gimen',\n",
        "        'metrics': {\n",
        "            'rmse': 0.040205,\n",
        "            'mae': 0.028595,\n",
        "            'mape': 1.380123,\n",
        "            'r2': 0.788989,\n",
        "            'directional_accuracy': 53.214286,\n",
        "            'hit_rate_2pct': 77.224199\n",
        "        },\n",
        "        'features': ['iron', 'coking', 'steel', 'VIX', 'sp500', 'tasa_interes_banxico']\n",
        "    },\n",
        "    'XGBoost_v2_regime': {\n",
        "        'model_type': 'XGBoost_V2',\n",
        "        'combination': 'regime',\n",
        "        'description': 'XGBoost con variables de r√©gimen',\n",
        "        'metrics': {\n",
        "            'rmse': 0.041577,\n",
        "            'mae': 0.030545,\n",
        "            'mape': 1.476894,\n",
        "            'r2': 0.787190,\n",
        "            'directional_accuracy': 49.824561,\n",
        "            'hit_rate_2pct': 77.272727\n",
        "        },\n",
        "        'features': ['iron', 'coking', 'steel', 'VIX', 'sp500', 'tasa_interes_banxico']\n",
        "    },\n",
        "    'XGBoost_v2_hibrida': {\n",
        "        'model_type': 'XGBoost_V2',\n",
        "        'combination': 'hibrida',\n",
        "        'description': 'XGBoost con combinaci√≥n h√≠brida',\n",
        "        'metrics': {\n",
        "            'rmse': 0.042801,\n",
        "            'mae': 0.031339,\n",
        "            'mape': 1.518006,\n",
        "            'r2': 0.774479,\n",
        "            'directional_accuracy': 52.280702,\n",
        "            'hit_rate_2pct': 74.825175\n",
        "        },\n",
        "        'features': ['precio_varilla_lme_lag_1', 'volatility_20', 'iron', \n",
        "                    'coking', 'commodities', 'VIX']\n",
        "    }\n",
        "}\n",
        "\n",
        "# Guardar configuraci√≥n de cada modelo\n",
        "for model_name, config in best_models_config.items():\n",
        "    config_file = f\"{models_dir}/{model_name}_config.json\"\n",
        "    \n",
        "    # Agregar metadata adicional\n",
        "    config['metadata'] = {\n",
        "        'created_at': datetime.now().isoformat(),\n",
        "        'model_name': model_name,\n",
        "        'status': 'configuration_saved',\n",
        "        'training_required': True,\n",
        "        'data_period': {\n",
        "            'start': '2020-01-02',\n",
        "            'end': '2025-09-26'\n",
        "        },\n",
        "        'preprocessing': {\n",
        "            'scaler': 'RobustScaler',\n",
        "            'winsorization': [0.01, 0.99],\n",
        "            'target_transformation': 'direct_price_prediction'\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    # Guardar configuraci√≥n\n",
        "    with open(config_file, 'w') as f:\n",
        "        json.dump(config, f, indent=2)\n",
        "    \n",
        "    print(f\"\\n‚úÖ {model_name}\")\n",
        "    print(f\"   üìÑ Configuraci√≥n guardada: {config_file}\")\n",
        "    print(f\"   üìä MAPE: {config['metrics']['mape']:.2f}%\")\n",
        "    print(f\"   üìä R¬≤: {config['metrics']['r2']:.3f}\")\n",
        "\n",
        "# Crear √≠ndice maestro\n",
        "print(\"\\n\" + \"-\" * 60)\n",
        "print(\"üìö Creando √≠ndice maestro de modelos...\")\n",
        "\n",
        "master_index = {\n",
        "    'created_at': datetime.now().isoformat(),\n",
        "    'total_models': 4,\n",
        "    'models': list(best_models_config.keys()),\n",
        "    'best_model': 'MIDAS_v2_hibrida',\n",
        "    'ensemble_weights': {\n",
        "        'MIDAS_v2_hibrida': 0.50,\n",
        "        'MIDAS_v2_regime': 0.20,\n",
        "        'XGBoost_v2_regime': 0.20,\n",
        "        'XGBoost_v2_hibrida': 0.10\n",
        "    },\n",
        "    'status': 'configuration_ready',\n",
        "    'note': 'Models need to be trained using train_*_v2 functions with prepared_data_v2'\n",
        "}\n",
        "\n",
        "master_index_file = f\"{models_dir}/master_index.json\"\n",
        "with open(master_index_file, 'w') as f:\n",
        "    json.dump(master_index, f, indent=2)\n",
        "\n",
        "print(f\"‚úÖ √çndice maestro creado: {master_index_file}\")\n",
        "\n",
        "# Instrucciones para completar el guardado\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üìù INSTRUCCIONES PARA COMPLETAR EL GUARDADO DE MODELOS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\"\"\n",
        "Para guardar los modelos entrenados completos:\n",
        "\n",
        "1. VERIFICAR DATOS:\n",
        "   - Aseg√∫rate de que 'prepared_data_v2' est√© cargado\n",
        "   - Verifica que contenga las 3 combinaciones: 'hibrida', 'fundamental', 'regime'\n",
        "\n",
        "2. EJECUTAR ENTRENAMIENTO V2:\n",
        "   - Ejecuta la celda \"EJECUTAR MODELOS V2 MEJORADOS\"\n",
        "   - Esto llenar√° 'results_v2' con los resultados\n",
        "\n",
        "3. GUARDAR MODELOS COMPLETOS:\n",
        "   - Una vez entrenados, ejecuta:\n",
        "   \n",
        "   import pickle\n",
        "   \n",
        "   # Buscar y guardar cada modelo\n",
        "   for result in results_v2:\n",
        "       model_type = result.get('model')\n",
        "       combination = result.get('combination')\n",
        "       \n",
        "       if 'full_result' in result:\n",
        "           filename = f\"{models_dir}/{model_type}_{combination}.pkl\"\n",
        "           with open(filename, 'wb') as f:\n",
        "               pickle.dump(result['full_result'], f)\n",
        "           print(f\"‚úÖ Guardado: {filename}\")\n",
        "\n",
        "4. VALIDAR:\n",
        "   - Verifica que los archivos .pkl se hayan creado\n",
        "   - Prueba cargar un modelo para confirmar\n",
        "\n",
        "NOTA: Los modelos V2 usan predicci√≥n directa de precios con RobustScaler,\n",
        "      no log returns como la V1 que fall√≥.\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\nüéØ Configuraciones guardadas y listas para entrenamiento!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "deacero_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
