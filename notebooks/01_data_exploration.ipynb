{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \ud83d\udcca Exploraci\u00f3n de Datos - DeAcero Steel Price Predictor\n",
    "\n",
    "Este notebook explora los datos recopilados de m\u00faltiples fuentes para la predicci\u00f3n de precios de varilla corrugada.\n",
    "\n",
    "## Objetivos:\n",
    "1. Cargar todas las series temporales desde archivos raw\n",
    "2. Analizar la calidad y completitud de los datos\n",
    "3. Identificar patrones y tendencias iniciales\n",
    "4. Evaluar correlaciones preliminares\n",
    "5. Preparar datos para an\u00e1lisis de features\n",
    "\n",
    "## Fuentes de Datos (10 fuentes funcionales):\n",
    "\n",
    "### \ud83d\udcc8 DATOS DIARIOS (cr\u00edticos para predicci\u00f3n)\n",
    "- **Yahoo Finance**: Commodities, \u00edndices y acciones (12 series)\n",
    "- **Raw Materials**: Mineral de hierro y carb\u00f3n de coque - proxies v\u00eda acciones mineras (10 series)\n",
    "- **Banxico**: Tipo de cambio, TIIE, UDIS (8 series)\n",
    "- **FRED**: Indicadores econ\u00f3micos US (8 series)\n",
    "- **LME**: Precios de metales (10 series)\n",
    "- **investing**: Precio de la varilla\n",
    "\n",
    "### \ud83d\udcca DATOS MENSUALES (importantes para contexto)\n",
    "- **World Bank Monthly** \u2b50: Commodities mensuales Pink Sheet - Mineral de hierro, carb\u00f3n, metales (9 series)\n",
    "- **INEGI**: Indicadores econ\u00f3micos M\u00e9xico (16 series)\n",
    "- **AHMSA**: Precios hist\u00f3ricos acero M\u00e9xico (7 series)\n",
    "\n",
    "### \ud83d\udcc9 DATOS ANUALES/LIMITADOS\n",
    "- **World Bank**: Indicadores macroecon\u00f3micos anuales (8 series)\n",
    "- **Trading Economics**: Solo valores actuales (5 series)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuraci\u00f3n inicial\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "import glob\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "# Configuraci\u00f3n de visualizaci\u00f3n\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuraci\u00f3n de pandas\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print(\"\ud83d\udcda Librer\u00edas importadas exitosamente\")\n",
    "print(f\"\ud83d\udcc5 Fecha de an\u00e1lisis: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci\u00f3n para cargar datos raw\n",
    "def load_raw_data(raw_dir: str = '../data/raw') -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Carga todos los archivos CSV del directorio raw\n",
    "    \n",
    "    Returns:\n",
    "        Dict con DataFrames por fuente\n",
    "    \"\"\"\n",
    "    all_data = {}\n",
    "    \n",
    "    # Obtener todos los archivos CSV\n",
    "    csv_files = glob.glob(os.path.join(raw_dir, '*.csv'))\n",
    "    \n",
    "    print(f\"\ud83d\udcc1 Encontrados {len(csv_files)} archivos CSV en {raw_dir}\")\n",
    "    \n",
    "    # Agrupar por fuente\n",
    "    sources = {}\n",
    "    for file in csv_files:\n",
    "        filename = os.path.basename(file)\n",
    "        source = filename.split('_')[0]\n",
    "        \n",
    "        if source not in sources:\n",
    "            sources[source] = []\n",
    "        sources[source].append(file)\n",
    "    \n",
    "    # Cargar datos por fuente\n",
    "    for source, files in sources.items():\n",
    "        print(f\"\\n\ud83d\udcca Cargando {len(files)} archivos de {source}...\")\n",
    "        source_data = {}\n",
    "        \n",
    "        for file in files:\n",
    "            try:\n",
    "                # Cargar CSV\n",
    "                df = pd.read_csv(file)\n",
    "                \n",
    "                # Obtener nombre de la serie\n",
    "                filename = os.path.basename(file)\n",
    "                parts = filename.replace('.csv', '').split('_')\n",
    "                \n",
    "                # El nombre de la serie es todo menos la fuente y la fecha\n",
    "                series_name = '_'.join(parts[1:-1])\n",
    "                \n",
    "                # Convertir fecha a datetime si existe\n",
    "                # Manejar diferentes nombres de columna de fecha\n",
    "                if 'fecha' in df.columns:\n",
    "                    df['fecha'] = pd.to_datetime(df['fecha'])\n",
    "                    df = df.sort_values('fecha')\n",
    "                elif 'Date' in df.columns:\n",
    "                    # Para archivos FRED que usan 'Date' en lugar de 'fecha'\n",
    "                    df['fecha'] = pd.to_datetime(df['Date'])\n",
    "                    df = df.drop(columns=['Date'])\n",
    "                    df = df.sort_values('fecha')\n",
    "                    \n",
    "                    # Identificar columna de valor para FRED\n",
    "                    # La segunda columna suele ser el valor\n",
    "                    value_cols = [col for col in df.columns if col != 'fecha']\n",
    "                    if value_cols and 'valor' not in df.columns:\n",
    "                        df['valor'] = df[value_cols[0]]\n",
    "                \n",
    "                source_data[series_name] = df\n",
    "                \n",
    "                # Cargar metadata si existe\n",
    "                metadata_file = file.replace('.csv', '_metadata.json')\n",
    "                if os.path.exists(metadata_file):\n",
    "                    with open(metadata_file, 'r') as f:\n",
    "                        metadata = json.load(f)\n",
    "                        source_data[f\"{series_name}_metadata\"] = metadata\n",
    "                        \n",
    "                print(f\"   \u2705 {series_name}: {len(df)} registros\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   \u274c Error cargando {os.path.basename(file)}: {str(e)}\")\n",
    "        \n",
    "        all_data[source] = source_data\n",
    "    \n",
    "    return all_data\n",
    "\n",
    "print(\"\ud83d\udd27 Funci\u00f3n de carga de datos definida\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcc2 Carga de Datos Raw\n",
    "\n",
    "Cargamos todas las series temporales desde los archivos CSV guardados en `data/raw/`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar todos los datos raw\n",
    "all_data = load_raw_data()\n",
    "\n",
    "# Resumen de datos cargados\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\ud83d\udcca RESUMEN DE DATOS CARGADOS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "total_series = 0\n",
    "total_points = 0\n",
    "\n",
    "for source, data in all_data.items():\n",
    "    # Filtrar solo DataFrames (no metadata)\n",
    "    dfs = {k: v for k, v in data.items() if isinstance(v, pd.DataFrame)}\n",
    "    \n",
    "    series_count = len(dfs)\n",
    "    points_count = sum(len(df) for df in dfs.values())\n",
    "    \n",
    "    total_series += series_count\n",
    "    total_points += points_count\n",
    "    \n",
    "    print(f\"\\n\ud83d\udcc1 {source}:\")\n",
    "    print(f\"   Series: {series_count}\")\n",
    "    print(f\"   Puntos totales: {points_count:,}\")\n",
    "    \n",
    "    # Mostrar rango de fechas si existe\n",
    "    for name, df in dfs.items():\n",
    "        if 'fecha' in df.columns and not df.empty:\n",
    "            fecha_min = df['fecha'].min()\n",
    "            fecha_max = df['fecha'].max()\n",
    "            print(f\"   \u2022 {name}: {fecha_min.strftime('%Y-%m-%d')} a {fecha_max.strftime('%Y-%m-%d')} ({len(df):,} puntos)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"\ud83d\udcc8 TOTAL GENERAL:\")\n",
    "print(f\"   Series temporales: {total_series}\")\n",
    "print(f\"   Puntos de datos: {total_points:,}\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcca An\u00e1lisis de Completitud de Datos\n",
    "\n",
    "Analizamos la completitud y calidad de cada serie temporal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An\u00e1lisis de completitud de datos\n",
    "def analyze_data_quality(all_data: Dict) -> pd.DataFrame:\n",
    "    \"\"\"Analiza la calidad y completitud de los datos\"\"\"\n",
    "    \n",
    "    quality_report = []\n",
    "    \n",
    "    for source, data in all_data.items():\n",
    "        # Filtrar solo DataFrames\n",
    "        dfs = {k: v for k, v in data.items() if isinstance(v, pd.DataFrame)}\n",
    "        \n",
    "        for name, df in dfs.items():\n",
    "            report = {\n",
    "                'fuente': source,\n",
    "                'serie': name,\n",
    "                'registros': len(df),\n",
    "                'columnas': len(df.columns),\n",
    "                'valores_nulos': df.isnull().sum().sum(),\n",
    "                'pct_nulos': (df.isnull().sum().sum() / (len(df) * len(df.columns)) * 100) if len(df) > 0 else 0\n",
    "            }\n",
    "            \n",
    "            # An\u00e1lisis de fechas si existe columna fecha\n",
    "            if 'fecha' in df.columns and not df.empty:\n",
    "                report['fecha_inicio'] = df['fecha'].min()\n",
    "                report['fecha_fin'] = df['fecha'].max()\n",
    "                report['dias_totales'] = (df['fecha'].max() - df['fecha'].min()).days\n",
    "                \n",
    "                # Detectar frecuencia\n",
    "                if len(df) > 1:\n",
    "                    date_diffs = df['fecha'].diff().dropna()\n",
    "                    mode_diff = date_diffs.mode()[0] if not date_diffs.empty else pd.Timedelta(days=1)\n",
    "                    \n",
    "                    if mode_diff.days == 1:\n",
    "                        report['frecuencia'] = 'Diaria'\n",
    "                    elif mode_diff.days == 7:\n",
    "                        report['frecuencia'] = 'Semanal'\n",
    "                    elif 28 <= mode_diff.days <= 31:\n",
    "                        report['frecuencia'] = 'Mensual'\n",
    "                    elif 90 <= mode_diff.days <= 92:\n",
    "                        report['frecuencia'] = 'Trimestral'\n",
    "                    elif 365 <= mode_diff.days <= 366:\n",
    "                        report['frecuencia'] = 'Anual'\n",
    "                    else:\n",
    "                        report['frecuencia'] = 'Irregular'\n",
    "                else:\n",
    "                    report['frecuencia'] = 'N/A'\n",
    "            \n",
    "            # An\u00e1lisis de valores si existe columna valor\n",
    "            if 'valor' in df.columns and not df['valor'].empty:\n",
    "                report['valor_min'] = df['valor'].min()\n",
    "                report['valor_max'] = df['valor'].max()\n",
    "                report['valor_promedio'] = df['valor'].mean()\n",
    "                report['valor_std'] = df['valor'].std()\n",
    "            \n",
    "            quality_report.append(report)\n",
    "    \n",
    "    return pd.DataFrame(quality_report)\n",
    "\n",
    "# Generar reporte de calidad\n",
    "quality_df = analyze_data_quality(all_data)\n",
    "\n",
    "# Mostrar resumen por fuente\n",
    "print(\"\\n\ud83d\udcca AN\u00c1LISIS DE CALIDAD POR FUENTE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for source in quality_df['fuente'].unique():\n",
    "    source_data = quality_df[quality_df['fuente'] == source]\n",
    "    \n",
    "    print(f\"\\n\ud83d\udd0d {source}:\")\n",
    "    print(f\"   Total series: {len(source_data)}\")\n",
    "    print(f\"   Total registros: {source_data['registros'].sum():,}\")\n",
    "    print(f\"   Promedio nulos: {source_data['pct_nulos'].mean():.2f}%\")\n",
    "    \n",
    "    # Frecuencias\n",
    "    freq_counts = source_data['frecuencia'].value_counts()\n",
    "    if not freq_counts.empty:\n",
    "        print(f\"   Frecuencias:\")\n",
    "        for freq, count in freq_counts.items():\n",
    "            print(f\"      \u2022 {freq}: {count} series\")\n",
    "\n",
    "# Mostrar tabla detallada\n",
    "print(\"\\n\ud83d\udccb DETALLE DE SERIES TEMPORALES:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Ordenar por fuente y serie\n",
    "quality_df_sorted = quality_df.sort_values(['fuente', 'serie'])\n",
    "\n",
    "# Seleccionar columnas clave para mostrar\n",
    "display_cols = ['fuente', 'serie', 'registros', 'frecuencia', 'pct_nulos']\n",
    "if 'fecha_inicio' in quality_df.columns:\n",
    "    display_cols.extend(['fecha_inicio', 'fecha_fin'])\n",
    "\n",
    "print(quality_df_sorted[display_cols].to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udfaf An\u00e1lisis Especial: World Bank Monthly Commodities\n",
    "\n",
    "An\u00e1lisis detallado de los datos MENSUALES de commodities del World Bank Pink Sheet, cr\u00edticos para la predicci\u00f3n del precio del acero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An\u00e1lisis especial de World Bank Monthly Commodities\n",
    "print(\"\ud83c\udfaf AN\u00c1LISIS DE WORLD BANK MONTHLY COMMODITIES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Verificar si tenemos datos de World Bank Monthly\n",
    "wb_monthly_found = False\n",
    "for source in all_data.keys():\n",
    "    if 'world' in source.lower():\n",
    "        # Buscar series mensuales de commodities\n",
    "        monthly_series = {}\n",
    "        for series_name, data in all_data[source].items():\n",
    "            if isinstance(data, pd.DataFrame) and 'monthly' in series_name.lower():\n",
    "                monthly_series[series_name] = data\n",
    "                wb_monthly_found = True\n",
    "        \n",
    "        if monthly_series:\n",
    "            print(f\"\\n\u2705 Encontradas {len(monthly_series)} series mensuales de commodities:\")\n",
    "            print(\"-\"*50)\n",
    "            \n",
    "            # Analizar cada serie mensual\n",
    "            commodities_summary = []\n",
    "            for name, df in monthly_series.items():\n",
    "                if 'fecha' in df.columns and 'valor' in df.columns:\n",
    "                    # Limpiar nombre\n",
    "                    commodity = name.replace('bank_monthly_', '').replace('_20250926', '')\n",
    "                    \n",
    "                    # Calcular estad\u00edsticas\n",
    "                    stats = {\n",
    "                        'Commodity': commodity.replace('_', ' ').title(),\n",
    "                        'Puntos': len(df),\n",
    "                        'Inicio': df['fecha'].min().strftime('%Y-%m'),\n",
    "                        'Fin': df['fecha'].max().strftime('%Y-%m'),\n",
    "                        '\u00daltimo Valor': f\"{df['valor'].iloc[-1]:.2f}\",\n",
    "                        'Promedio': f\"{df['valor'].mean():.2f}\",\n",
    "                        'Volatilidad': f\"{df['valor'].std():.2f}\",\n",
    "                        'Cambio %': f\"{((df['valor'].iloc[-1] / df['valor'].iloc[0]) - 1) * 100:.1f}%\"\n",
    "                    }\n",
    "                    commodities_summary.append(stats)\n",
    "                    \n",
    "                    # Mostrar info de cada commodity\n",
    "                    print(f\"\\n\ud83d\udcca {stats['Commodity']}:\")\n",
    "                    print(f\"   \u2022 Per\u00edodo: {stats['Inicio']} a {stats['Fin']} ({stats['Puntos']} meses)\")\n",
    "                    print(f\"   \u2022 \u00daltimo valor: ${stats['\u00daltimo Valor']}\")\n",
    "                    print(f\"   \u2022 Promedio hist\u00f3rico: ${stats['Promedio']}\")\n",
    "                    print(f\"   \u2022 Volatilidad (std): ${stats['Volatilidad']}\")\n",
    "                    print(f\"   \u2022 Cambio total: {stats['Cambio %']}\")\n",
    "            \n",
    "            # Crear DataFrame resumen\n",
    "            if commodities_summary:\n",
    "                wb_monthly_df = pd.DataFrame(commodities_summary)\n",
    "                print(\"\\n\ud83d\udccb RESUMEN DE COMMODITIES MENSUALES:\")\n",
    "                print(\"-\"*50)\n",
    "                print(wb_monthly_df.to_string(index=False))\n",
    "                \n",
    "                # Identificar commodities cr\u00edticos para acero\n",
    "                critical_commodities = ['iron ore', 'coal', 'copper', 'aluminum', 'zinc']\n",
    "                print(\"\\n\u2b50 COMMODITIES CR\u00cdTICOS PARA PRODUCCI\u00d3N DE ACERO:\")\n",
    "                print(\"-\"*50)\n",
    "                for commodity in critical_commodities:\n",
    "                    for name, df in monthly_series.items():\n",
    "                        if commodity.replace(' ', '_') in name.lower():\n",
    "                            if 'fecha' in df.columns and 'valor' in df.columns:\n",
    "                                latest_val = df['valor'].iloc[-1]\n",
    "                                prev_val = df['valor'].iloc[-12] if len(df) > 12 else df['valor'].iloc[0]\n",
    "                                change_12m = ((latest_val / prev_val) - 1) * 100\n",
    "                                \n",
    "                                print(f\"\u2022 {commodity.upper()}:\")\n",
    "                                print(f\"  - \u00daltimo precio: ${latest_val:.2f}\")\n",
    "                                print(f\"  - Cambio 12 meses: {change_12m:+.1f}%\")\n",
    "                                \n",
    "                                # Tendencia\n",
    "                                if len(df) > 3:\n",
    "                                    trend = \"\ud83d\udcc8 Alcista\" if df['valor'].iloc[-1] > df['valor'].iloc[-3] else \"\ud83d\udcc9 Bajista\"\n",
    "                                    print(f\"  - Tendencia 3 meses: {trend}\")\n",
    "            break\n",
    "\n",
    "if not wb_monthly_found:\n",
    "    print(\"\\n\u26a0\ufe0f No se encontraron datos de World Bank Monthly Commodities\")\n",
    "    print(\"Verificando estructura de datos...\")\n",
    "    \n",
    "    # Buscar en todas las fuentes\n",
    "    for source, data in all_data.items():\n",
    "        if 'world' in source.lower():\n",
    "            print(f\"\\n\ud83d\udcc1 Fuente: {source}\")\n",
    "            series_list = [k for k in data.keys() if isinstance(data[k], pd.DataFrame)]\n",
    "            print(f\"   Series encontradas: {', '.join(series_list[:5])}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaci\u00f3n de commodities mensuales cr\u00edticos\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Crear visualizaci\u00f3n de commodities cr\u00edticos\n",
    "fig_commodities = make_subplots(\n",
    "    rows=3, cols=2,\n",
    "    subplot_titles=(\n",
    "        'Mineral de Hierro (Iron Ore)', 'Carb\u00f3n Australiano (Coking Coal)',\n",
    "        'Cobre (Copper)', 'Aluminio (Aluminum)',\n",
    "        'Zinc', 'N\u00edquel (Nickel)'\n",
    "    ),\n",
    "    vertical_spacing=0.1,\n",
    "    horizontal_spacing=0.15\n",
    ")\n",
    "\n",
    "# Buscar y graficar series de World Bank Monthly\n",
    "plot_positions = [\n",
    "    ('iron_ore', 1, 1, 'red'),\n",
    "    ('coal_australian', 1, 2, 'black'),\n",
    "    ('copper', 2, 1, 'orange'),\n",
    "    ('aluminum', 2, 2, 'silver'),\n",
    "    ('zinc', 3, 1, 'blue'),\n",
    "    ('nickel', 3, 2, 'green')\n",
    "]\n",
    "\n",
    "series_found = False\n",
    "for source, data in all_data.items():\n",
    "    if 'world' in source.lower():\n",
    "        for series_name, df in data.items():\n",
    "            if isinstance(df, pd.DataFrame) and 'monthly' in series_name.lower():\n",
    "                # Buscar cada commodity\n",
    "                for commodity, row, col, color in plot_positions:\n",
    "                    if commodity in series_name.lower():\n",
    "                        if 'fecha' in df.columns and 'valor' in df.columns:\n",
    "                            fig_commodities.add_trace(\n",
    "                                go.Scatter(\n",
    "                                    x=df['fecha'],\n",
    "                                    y=df['valor'],\n",
    "                                    mode='lines+markers',\n",
    "                                    name=commodity.replace('_', ' ').title(),\n",
    "                                    line=dict(color=color, width=2),\n",
    "                                    marker=dict(size=4),\n",
    "                                    showlegend=False\n",
    "                                ),\n",
    "                                row=row, col=col\n",
    "                            )\n",
    "                            \n",
    "                            # Agregar l\u00ednea de tendencia\n",
    "                            if len(df) > 12:\n",
    "                                # Media m\u00f3vil de 3 meses\n",
    "                                df['ma3'] = df['valor'].rolling(window=3, center=True).mean()\n",
    "                                fig_commodities.add_trace(\n",
    "                                    go.Scatter(\n",
    "                                        x=df['fecha'],\n",
    "                                        y=df['ma3'],\n",
    "                                        mode='lines',\n",
    "                                        name='MA3',\n",
    "                                        line=dict(color=color, width=1, dash='dash'),\n",
    "                                        opacity=0.5,\n",
    "                                        showlegend=False\n",
    "                                    ),\n",
    "                                    row=row, col=col\n",
    "                                )\n",
    "                            series_found = True\n",
    "\n",
    "if series_found:\n",
    "    # Actualizar layout\n",
    "    fig_commodities.update_layout(\n",
    "        height=900,\n",
    "        title_text=\"\ud83d\udcca World Bank Pink Sheet - Commodities Mensuales Cr\u00edticos para Acero\",\n",
    "        title_font_size=16,\n",
    "        showlegend=False,\n",
    "        hovermode='x unified'\n",
    "    )\n",
    "    \n",
    "    # Actualizar ejes\n",
    "    fig_commodities.update_xaxes(title_text=\"Fecha\", tickformat=\"%Y-%m\")\n",
    "    fig_commodities.update_yaxes(title_text=\"USD\")\n",
    "    \n",
    "    # Mostrar figura\n",
    "    fig_commodities.show()\n",
    "    print(\"\u2705 Visualizaci\u00f3n de commodities mensuales generada\")\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f No se encontraron datos mensuales de commodities para visualizar\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcca An\u00e1lisis Completo de Series Mensuales por Fuente\n",
    "\n",
    "An\u00e1lisis detallado de TODAS las series con frecuencia mensual, agrupadas por fuente de datos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaci\u00f3n de Series Temporales Clave - SOLO DATOS POST-2025\n",
    "# IDENTIFICACI\u00d3N DE VARIABLE OBJETIVO\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "\n",
    "print(\"\ud83c\udfaf IDENTIFICACI\u00d3N DE VARIABLE OBJETIVO\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n\ud83d\udccc VARIABLE OBJETIVO PRINCIPAL:\")\n",
    "print(\"   \u2022 LME steel_rebar: Precio internacional de varilla corrugada (steel rebar)\")\n",
    "print(\"   \u2022 Fuente: London Metal Exchange (LME)\")\n",
    "print(\"\\n\ud83d\udccc VARIABLES OBJETIVO SECUNDARIAS/PROXY:\")\n",
    "print(\"   \u2022 AHMSA ahmsa: Precio de acero AHMSA M\u00e9xico\")\n",
    "print(\"   \u2022 LME steel_etf: ETF de acero como proxy\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Fecha de corte\n",
    "fecha_corte = pd.Timestamp('2025-01-01')\n",
    "\n",
    "# Funci\u00f3n para normalizar fechas\n",
    "def normalizar_fecha(fecha):\n",
    "    if pd.notna(fecha):\n",
    "        if hasattr(fecha, 'tz') and fecha.tz is not None:\n",
    "            return fecha.tz_localize(None)\n",
    "    return fecha\n",
    "\n",
    "# Funci\u00f3n para filtrar datos post-2025\n",
    "def filtrar_post_2025(df):\n",
    "    # Verificar si es un DataFrame\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        return None\n",
    "    if df.empty:\n",
    "        return None\n",
    "    df_copy = df.copy()\n",
    "    df_copy['fecha'] = df_copy['fecha'].apply(normalizar_fecha)\n",
    "    return df_copy[pd.to_datetime(df_copy['fecha']) > fecha_corte]\n",
    "\n",
    "# Funci\u00f3n para obtener valor de cierre\n",
    "def obtener_valor_cierre(df, fuente):\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        return None\n",
    "    \n",
    "    # Prioridad de columnas seg\u00fan la fuente\n",
    "    if 'Close' in df.columns:\n",
    "        return df['Close']\n",
    "    elif 'precio_cierre' in df.columns:\n",
    "        return df['precio_cierre']\n",
    "    elif 'valor' in df.columns:\n",
    "        return df['valor']\n",
    "    elif 'close' in df.columns:\n",
    "        return df['close']\n",
    "    \n",
    "    # Si no encuentra ninguna, buscar cualquier columna num\u00e9rica que no sea fecha\n",
    "    numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        # Excluir columnas que parecen ser \u00edndices o fechas\n",
    "        valid_cols = [col for col in numeric_cols if 'fecha' not in col.lower() and 'date' not in col.lower() and 'index' not in col.lower()]\n",
    "        if valid_cols:\n",
    "            return df[valid_cols[0]]\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Crear figura con subplots\n",
    "fig = make_subplots(\n",
    "    rows=4, cols=2,\n",
    "    subplot_titles=(\n",
    "        '\ud83c\udfaf VARIABLE OBJETIVO: Precio Varilla LME', 'Precio AHMSA M\u00e9xico (Proxy Local)',\n",
    "        'Tipo de Cambio USD/MXN', 'Precios Metales Base (LME)',\n",
    "        'Mineral de Hierro (Raw Materials)', 'Indicadores FRED (US)',\n",
    "        'Tasas de Inter\u00e9s (Banxico)', 'Indicadores Construcci\u00f3n (INEGI)'\n",
    "    ),\n",
    "    specs=[[{'secondary_y': False}, {'secondary_y': False}],\n",
    "           [{'secondary_y': False}, {'secondary_y': False}],\n",
    "           [{'secondary_y': False}, {'secondary_y': False}],\n",
    "           [{'secondary_y': False}, {'secondary_y': False}]],\n",
    "    vertical_spacing=0.08,\n",
    "    horizontal_spacing=0.12\n",
    ")\n",
    "\n",
    "series_agregadas = []\n",
    "estadisticas = {}\n",
    "\n",
    "# 1. VARIABLE OBJETIVO PRINCIPAL: Precio de varilla LME\n",
    "print(\"\\n\ud83d\udcca AN\u00c1LISIS DE VARIABLE OBJETIVO:\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "if 'LME' in all_data:\n",
    "    # Buscar steel_rebar\n",
    "    steel_rebar_keys = [k for k in all_data['LME'].keys() if 'steel_rebar' in k.lower()]\n",
    "    \n",
    "    for key in steel_rebar_keys:\n",
    "        df = all_data['LME'][key]\n",
    "        # Verificar que sea un DataFrame antes de procesar\n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "            continue\n",
    "        df_2025 = filtrar_post_2025(df)\n",
    "        if df_2025 is not None and not df_2025.empty:\n",
    "            valores = obtener_valor_cierre(df_2025, 'LME')\n",
    "            if valores is not None:\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=df_2025['fecha'], \n",
    "                        y=valores, \n",
    "                        name='\ud83c\udfaf Varilla LME (OBJETIVO)', \n",
    "                        line=dict(color='red', width=3),\n",
    "                        mode='lines+markers',\n",
    "                        marker=dict(size=3)\n",
    "                    ),\n",
    "                    row=1, col=1\n",
    "                )\n",
    "                estadisticas['LME_steel_rebar'] = {\n",
    "                    'puntos': len(df_2025),\n",
    "                    'ultimo_valor': valores.iloc[-1],\n",
    "                    'fecha_max': df_2025['fecha'].max(),\n",
    "                    'promedio': valores.mean(),\n",
    "                    'volatilidad': valores.std()\n",
    "                }\n",
    "                print(f\"\u2705 VARIABLE OBJETIVO ENCONTRADA: LME {key}\")\n",
    "                print(f\"   \u2022 Puntos de datos en 2025: {len(df_2025)}\")\n",
    "                print(f\"   \u2022 \u00daltimo valor: ${valores.iloc[-1]:.2f}\")\n",
    "                print(f\"   \u2022 Promedio 2025: ${valores.mean():.2f}\")\n",
    "                print(f\"   \u2022 Volatilidad: {valores.std():.2f}\")\n",
    "\n",
    "# 2. AHMSA - Precio local M\u00e9xico\n",
    "if 'ahmsa' in all_data:\n",
    "    ahmsa_keys = [k for k in all_data['ahmsa'].keys() if 'ahmsa' in k.lower()]\n",
    "    \n",
    "    for key in ahmsa_keys[:1]:  # Solo el primero\n",
    "        df = all_data['ahmsa'][key]\n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "            continue\n",
    "        df_2025 = filtrar_post_2025(df)\n",
    "        if df_2025 is not None and not df_2025.empty:\n",
    "            valores = obtener_valor_cierre(df_2025, 'ahmsa')\n",
    "            if valores is not None:\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=df_2025['fecha'], \n",
    "                        y=valores, \n",
    "                        name='AHMSA M\u00e9xico', \n",
    "                        line=dict(color='orange', width=2)\n",
    "                    ),\n",
    "                    row=1, col=2\n",
    "                )\n",
    "                estadisticas['AHMSA'] = {\n",
    "                    'puntos': len(df_2025),\n",
    "                    'ultimo_valor': valores.iloc[-1]\n",
    "                }\n",
    "                series_agregadas.append(f\"AHMSA: {len(df_2025)} puntos\")\n",
    "\n",
    "# 3. Tipo de cambio USD/MXN\n",
    "if 'banxico' in all_data:\n",
    "    for key in ['usd_mxn_20250926', 'SF60653']:\n",
    "        if key in all_data['banxico']:\n",
    "            df = all_data['banxico'][key]\n",
    "            if not isinstance(df, pd.DataFrame):\n",
    "                continue\n",
    "            df_2025 = filtrar_post_2025(df)\n",
    "            if df_2025 is not None and not df_2025.empty:\n",
    "                valores = obtener_valor_cierre(df_2025, 'banxico')\n",
    "                if valores is not None:\n",
    "                    fig.add_trace(\n",
    "                        go.Scatter(\n",
    "                            x=df_2025['fecha'], \n",
    "                            y=valores, \n",
    "                            name='USD/MXN', \n",
    "                            line=dict(color='green', width=2)\n",
    "                        ),\n",
    "                        row=2, col=1\n",
    "                    )\n",
    "                    series_agregadas.append(f\"USD/MXN: {len(df_2025)} puntos\")\n",
    "                    break\n",
    "\n",
    "# 4. Precios de metales base (LME)\n",
    "if 'LME' in all_data:\n",
    "    metales = ['Aluminio', 'Cobre', 'Zinc', 'iron_ore']\n",
    "    colors = ['blue', 'brown', 'purple', 'gray']\n",
    "    \n",
    "    for i, metal in enumerate(metales):\n",
    "        for key in all_data['LME'].keys():\n",
    "            if metal.lower() in key.lower():\n",
    "                df = all_data['LME'][key]\n",
    "                if not isinstance(df, pd.DataFrame):\n",
    "                    continue\n",
    "                df_2025 = filtrar_post_2025(df)\n",
    "                if df_2025 is not None and not df_2025.empty:\n",
    "                    valores = obtener_valor_cierre(df_2025, 'LME')\n",
    "                    if valores is not None:\n",
    "                        fig.add_trace(\n",
    "                            go.Scatter(\n",
    "                                x=df_2025['fecha'], \n",
    "                                y=valores, \n",
    "                                name=metal, \n",
    "                                line=dict(color=colors[i % len(colors)], width=1.5)\n",
    "                            ),\n",
    "                            row=2, col=2\n",
    "                        )\n",
    "                        series_agregadas.append(f\"{metal}: {len(df_2025)} puntos\")\n",
    "                        break\n",
    "\n",
    "# 5. Mineral de hierro - Raw Materials\n",
    "if 'RawMaterials' in all_data:\n",
    "    iron_keys = [k for k in all_data['RawMaterials'].keys() if 'MineralHierro' in k or 'iron' in k.lower()]\n",
    "    \n",
    "    for i, key in enumerate(iron_keys[:3]):  # M\u00e1ximo 3 series\n",
    "        df = all_data['RawMaterials'][key]\n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "            continue\n",
    "        df_2025 = filtrar_post_2025(df)\n",
    "        if df_2025 is not None and not df_2025.empty:\n",
    "            valores = obtener_valor_cierre(df_2025, 'RawMaterials')\n",
    "            if valores is not None:\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=df_2025['fecha'], \n",
    "                        y=valores, \n",
    "                        name=key.split('_')[1][:10], \n",
    "                        line=dict(width=1.5)\n",
    "                    ),\n",
    "                    row=3, col=1\n",
    "                )\n",
    "                series_agregadas.append(f\"{key}: {len(df_2025)} puntos\")\n",
    "\n",
    "# 6. Indicadores FRED\n",
    "if 'FRED' in all_data:\n",
    "    fred_keys = ['steel_production', 'iron_steel_scrap', 'ppi_metals', 'federal_funds_rate']\n",
    "    \n",
    "    for key_partial in fred_keys:\n",
    "        for key in all_data['FRED'].keys():\n",
    "            if key_partial in key.lower():\n",
    "                df = all_data['FRED'][key]\n",
    "                if not isinstance(df, pd.DataFrame):\n",
    "                    continue\n",
    "                df_2025 = filtrar_post_2025(df)\n",
    "                if df_2025 is not None and not df_2025.empty:\n",
    "                    valores = obtener_valor_cierre(df_2025, 'FRED')\n",
    "                    if valores is not None:\n",
    "                        fig.add_trace(\n",
    "                            go.Scatter(\n",
    "                                x=df_2025['fecha'], \n",
    "                                y=valores, \n",
    "                                name=key_partial.replace('_', ' ').title()[:15], \n",
    "                                line=dict(width=1.5)\n",
    "                            ),\n",
    "                            row=3, col=2\n",
    "                        )\n",
    "                        series_agregadas.append(f\"FRED {key_partial}: {len(df_2025)} puntos\")\n",
    "                        break\n",
    "\n",
    "# 7. Tasas de inter\u00e9s - Banxico\n",
    "if 'banxico' in all_data:\n",
    "    tasas = ['tiie_28', 'interest_rate', 'TIIE28']\n",
    "    \n",
    "    for tasa in tasas:\n",
    "        for key in all_data['banxico'].keys():\n",
    "            if tasa.lower() in key.lower():\n",
    "                df = all_data['banxico'][key]\n",
    "                if not isinstance(df, pd.DataFrame):\n",
    "                    continue\n",
    "                df_2025 = filtrar_post_2025(df)\n",
    "                if df_2025 is not None and not df_2025.empty:\n",
    "                    valores = obtener_valor_cierre(df_2025, 'banxico')\n",
    "                    if valores is not None:\n",
    "                        fig.add_trace(\n",
    "                            go.Scatter(\n",
    "                                x=df_2025['fecha'], \n",
    "                                y=valores, \n",
    "                                name=tasa.upper(), \n",
    "                                line=dict(width=1.5)\n",
    "                            ),\n",
    "                            row=4, col=1\n",
    "                        )\n",
    "                        series_agregadas.append(f\"{tasa}: {len(df_2025)} puntos\")\n",
    "                        break\n",
    "\n",
    "# 8. Indicadores construcci\u00f3n - INEGI\n",
    "if 'INEGI' in all_data:\n",
    "    construccion_keys = ['ProduccionConstruccion', 'inpp_construccion']\n",
    "    \n",
    "    for key_partial in construccion_keys:\n",
    "        for key in all_data['INEGI'].keys():\n",
    "            if key_partial in key:\n",
    "                df = all_data['INEGI'][key]\n",
    "                if not isinstance(df, pd.DataFrame):\n",
    "                    continue\n",
    "                df_2025 = filtrar_post_2025(df)\n",
    "                if df_2025 is not None and not df_2025.empty:\n",
    "                    valores = obtener_valor_cierre(df_2025, 'INEGI')\n",
    "                    if valores is not None:\n",
    "                        fig.add_trace(\n",
    "                            go.Scatter(\n",
    "                                x=df_2025['fecha'], \n",
    "                                y=valores, \n",
    "                                name=key_partial.replace('_', ' ').title()[:20], \n",
    "                                line=dict(width=1.5)\n",
    "                            ),\n",
    "                            row=4, col=2\n",
    "                        )\n",
    "                        series_agregadas.append(f\"INEGI {key_partial}: {len(df_2025)} puntos\")\n",
    "                        break\n",
    "\n",
    "# Actualizar layout\n",
    "fig.update_layout(\n",
    "    height=1400,\n",
    "    showlegend=True,\n",
    "    title_text=\"\ud83d\udcca Series Temporales Clave (Solo Datos Post-2025) - Variable Objetivo: LME Steel Rebar\",\n",
    "    title_font_size=16,\n",
    "    hovermode='x unified',\n",
    "    legend=dict(\n",
    "        orientation=\"v\",\n",
    "        yanchor=\"top\",\n",
    "        y=1,\n",
    "        xanchor=\"left\",\n",
    "        x=1.02,\n",
    "        font=dict(size=9)\n",
    "    )\n",
    ")\n",
    "\n",
    "# Actualizar todos los ejes X\n",
    "for i in range(1, 5):\n",
    "    for j in range(1, 3):\n",
    "        fig.update_xaxes(tickformat=\"%Y-%m\", tickangle=45, row=i, col=j)\n",
    "        fig.update_yaxes(title_text=\"Valor\", row=i, col=j)\n",
    "\n",
    "# Mostrar figura\n",
    "fig.show()\n",
    "\n",
    "# Resumen de series agregadas\n",
    "print(\"\\n\ud83d\udcca RESUMEN DE SERIES VISUALIZADAS:\")\n",
    "print(\"-\"*60)\n",
    "for serie in series_agregadas:\n",
    "    print(f\"   \u2022 {serie}\")\n",
    "\n",
    "print(f\"\\n\u2705 Total de series con datos post-2025: {len(series_agregadas)}\")\n",
    "\n",
    "# An\u00e1lisis de correlaci\u00f3n potencial\n",
    "if 'LME_steel_rebar' in estadisticas:\n",
    "    print(\"\\n\ud83d\udd0d AN\u00c1LISIS DE VARIABLE OBJETIVO:\")\n",
    "    print(\"-\"*60)\n",
    "    print(\"Variable: LME Steel Rebar (Precio Internacional de Varilla Corrugada)\")\n",
    "    stats = estadisticas['LME_steel_rebar']\n",
    "    print(f\"   \u2022 \u00daltimo precio: ${stats['ultimo_valor']:.2f}\")\n",
    "    print(f\"   \u2022 Promedio 2025: ${stats['promedio']:.2f}\")\n",
    "    print(f\"   \u2022 Volatilidad (std): ${stats['volatilidad']:.2f}\")\n",
    "    print(f\"   \u2022 Coeficiente de variaci\u00f3n: {(stats['volatilidad']/stats['promedio']*100):.2f}%\")\n",
    "    print(f\"   \u2022 \u00daltima actualizaci\u00f3n: {stats['fecha_max'].strftime('%Y-%m-%d')}\")\n",
    "    \n",
    "    print(\"\\n\ud83d\udca1 RECOMENDACIONES PARA MODELADO:\")\n",
    "    print(\"-\"*60)\n",
    "    print(\"1. Variable objetivo principal: LME steel_rebar (precio internacional)\")\n",
    "    print(\"2. Variables explicativas clave:\")\n",
    "    print(\"   \u2022 Tipo de cambio USD/MXN (impacto directo en precio local)\")\n",
    "    print(\"   \u2022 Precios de metales base (correlaci\u00f3n con mercado de commodities)\")\n",
    "    print(\"   \u2022 Mineral de hierro (materia prima principal)\")\n",
    "    print(\"   \u2022 Indicadores de construcci\u00f3n (demanda)\")\n",
    "    print(\"   \u2022 Tasas de inter\u00e9s (costo financiero)\")\n",
    "    print(\"3. Considerar rezagos de 1-3 d\u00edas para capturar efectos retardados\")\n",
    "    print(\"4. Normalizar todas las series antes del modelado\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An\u00e1lisis exhaustivo de todas las series mensuales\n",
    "print(\"\ud83d\udcca AN\u00c1LISIS COMPLETO DE SERIES MENSUALES POR FUENTE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Primero, identificar todas las series mensuales\n",
    "monthly_series_by_source = {}\n",
    "\n",
    "# Usar quality_df que ya tiene la informaci\u00f3n de frecuencia\n",
    "if 'quality_df' in locals() and not quality_df.empty:\n",
    "    # Filtrar solo series mensuales\n",
    "    monthly_df = quality_df[quality_df['frecuencia'] == 'Mensual'].copy()\n",
    "    \n",
    "    if not monthly_df.empty:\n",
    "        print(f\"\\n\ud83d\udcc8 Total de series mensuales encontradas: {len(monthly_df)}\")\n",
    "        print(f\"\ud83d\udcc1 Fuentes con datos mensuales: {monthly_df['fuente'].nunique()}\")\n",
    "        \n",
    "        # Agrupar por fuente\n",
    "        for source in monthly_df['fuente'].unique():\n",
    "            source_monthly = monthly_df[monthly_df['fuente'] == source]\n",
    "            monthly_series_by_source[source] = source_monthly\n",
    "            \n",
    "        # Resumen general\n",
    "        print(\"\\n\ud83d\udccb RESUMEN POR FUENTE:\")\n",
    "        print(\"-\"*60)\n",
    "        for source, series_df in monthly_series_by_source.items():\n",
    "            print(f\"\u2022 {source}: {len(series_df)} series mensuales\")\n",
    "    else:\n",
    "        print(\"\u26a0\ufe0f No se encontraron series con frecuencia mensual\")\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f No se ha ejecutado el an\u00e1lisis de calidad previo\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An\u00e1lisis detallado por cada fuente con series mensuales\n",
    "if monthly_series_by_source:\n",
    "    for source_name, source_series in monthly_series_by_source.items():\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"\ud83d\udcca FUENTE: {source_name}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Informaci\u00f3n general de la fuente\n",
    "        print(f\"\\n\ud83d\udcc8 Estad\u00edsticas Generales:\")\n",
    "        print(f\"   \u2022 Total series mensuales: {len(source_series)}\")\n",
    "        print(f\"   \u2022 Total puntos de datos: {source_series['registros'].sum():,}\")\n",
    "        \n",
    "        if 'fecha_inicio' in source_series.columns and 'fecha_fin' in source_series.columns:\n",
    "            # Encontrar el rango temporal general\n",
    "            fecha_min = source_series['fecha_inicio'].min()\n",
    "            fecha_max = source_series['fecha_fin'].max()\n",
    "            if pd.notna(fecha_min) and pd.notna(fecha_max):\n",
    "                print(f\"   \u2022 Per\u00edodo cubierto: {fecha_min.strftime('%Y-%m-%d')} a {fecha_max.strftime('%Y-%m-%d')}\")\n",
    "                print(f\"   \u2022 Duraci\u00f3n total: {(fecha_max - fecha_min).days} d\u00edas (~{(fecha_max - fecha_min).days // 30} meses)\")\n",
    "        \n",
    "        # An\u00e1lisis por cada serie\n",
    "        print(f\"\\n\ud83d\udccb Detalle de Series:\")\n",
    "        print(\"-\"*60)\n",
    "        \n",
    "        for idx, row in source_series.iterrows():\n",
    "            serie_name = row['serie']\n",
    "            \n",
    "            # Obtener el DataFrame real de la serie\n",
    "            if source_name in all_data and serie_name in all_data[source_name]:\n",
    "                df = all_data[source_name][serie_name]\n",
    "                \n",
    "                if isinstance(df, pd.DataFrame):\n",
    "                    print(f\"\\n\ud83d\udccc {serie_name}:\")\n",
    "                    print(f\"   \u2022 Registros: {len(df)}\")\n",
    "                    \n",
    "                    # An\u00e1lisis temporal\n",
    "                    if 'fecha' in df.columns and not df.empty:\n",
    "                        fecha_min = df['fecha'].min()\n",
    "                        fecha_max = df['fecha'].max()\n",
    "                        print(f\"   \u2022 Per\u00edodo: {fecha_min.strftime('%Y-%m')} a {fecha_max.strftime('%Y-%m')}\")\n",
    "                        \n",
    "                        # Calcular gaps o datos faltantes\n",
    "                        expected_months = pd.date_range(start=fecha_min, end=fecha_max, freq='MS')\n",
    "                        actual_months = pd.to_datetime(df['fecha']).dt.to_period('M').unique()\n",
    "                        missing_months = len(expected_months) - len(actual_months)\n",
    "                        \n",
    "                        if missing_months > 0:\n",
    "                            print(f\"   \u2022 \u26a0\ufe0f Meses faltantes: {missing_months}\")\n",
    "                        else:\n",
    "                            print(f\"   \u2022 \u2705 Serie completa sin gaps\")\n",
    "                    \n",
    "                    # An\u00e1lisis de valores\n",
    "                    if 'valor' in df.columns and not df['valor'].empty:\n",
    "                        valores = df['valor'].dropna()\n",
    "                        if len(valores) > 0:\n",
    "                            print(f\"   \u2022 Valor m\u00ednimo: {valores.min():.2f}\")\n",
    "                            print(f\"   \u2022 Valor m\u00e1ximo: {valores.max():.2f}\")\n",
    "                            print(f\"   \u2022 Valor promedio: {valores.mean():.2f}\")\n",
    "                            print(f\"   \u2022 Desviaci\u00f3n est\u00e1ndar: {valores.std():.2f}\")\n",
    "                            print(f\"   \u2022 Coef. variaci\u00f3n: {(valores.std() / valores.mean() * 100):.1f}%\")\n",
    "                            \n",
    "                            # Tendencia reciente (\u00faltimos 6 meses)\n",
    "                            if len(valores) >= 6:\n",
    "                                recent_values = valores.tail(6)\n",
    "                                older_values = valores.tail(12).head(6) if len(valores) >= 12 else valores.head(6)\n",
    "                                \n",
    "                                change_pct = ((recent_values.mean() - older_values.mean()) / older_values.mean() * 100)\n",
    "                                \n",
    "                                if change_pct > 5:\n",
    "                                    trend = \"\ud83d\udcc8 Tendencia alcista\"\n",
    "                                elif change_pct < -5:\n",
    "                                    trend = \"\ud83d\udcc9 Tendencia bajista\"\n",
    "                                else:\n",
    "                                    trend = \"\u27a1\ufe0f Tendencia lateral\"\n",
    "                                \n",
    "                                print(f\"   \u2022 Tendencia 6 meses: {trend} ({change_pct:+.1f}%)\")\n",
    "                            \n",
    "                            # \u00daltimo valor disponible\n",
    "                            print(f\"   \u2022 \u00daltimo valor: {valores.iloc[-1]:.2f} ({df['fecha'].iloc[-1].strftime('%Y-%m') if 'fecha' in df.columns else 'N/A'})\")\n",
    "                    \n",
    "                    # Metadata si existe\n",
    "                    metadata_key = f\"{serie_name}_metadata\"\n",
    "                    if metadata_key in all_data[source_name]:\n",
    "                        metadata = all_data[source_name][metadata_key]\n",
    "                        if isinstance(metadata, dict):\n",
    "                            if 'unit' in metadata:\n",
    "                                print(f\"   \u2022 Unidad: {metadata['unit']}\")\n",
    "                            if 'source' in metadata:\n",
    "                                print(f\"   \u2022 Origen: {metadata['source']}\")\n",
    "            else:\n",
    "                print(f\"\\n\ud83d\udccc {serie_name}: [Datos no disponibles]\")\n",
    "        \n",
    "        print()  # L\u00ednea en blanco entre fuentes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83d\udccb Resumen Ejecutivo: Series Mensuales, Semanales y Diarias Cr\u00edticas para Predicci\u00f3n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERSI\u00d3N CORREGIDA - An\u00e1lisis de series por frecuencia con datos posteriores a 2025\n",
    "# Corrige el problema de comparaci\u00f3n entre fechas con y sin timezone\n",
    "\n",
    "print(\"\ud83d\udcca AN\u00c1LISIS DE SERIES CON DATOS POSTERIORES A ENERO 2025\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Fecha de corte (sin timezone para compatibilidad)\n",
    "fecha_corte = pd.Timestamp('2025-01-01')\n",
    "\n",
    "# Funci\u00f3n helper para normalizar fechas\n",
    "def normalizar_fecha(fecha):\n",
    "    \"\"\"Elimina timezone de una fecha si lo tiene\"\"\"\n",
    "    if pd.notna(fecha):\n",
    "        if hasattr(fecha, 'tz') and fecha.tz is not None:\n",
    "            return fecha.tz_localize(None)\n",
    "    return fecha\n",
    "\n",
    "# ============================================\n",
    "# SERIES MENSUALES\n",
    "# ============================================\n",
    "print(\"\\n\ud83d\udcc5 SERIES MENSUALES\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "series_mensuales_2025 = []\n",
    "\n",
    "if 'quality_df' in locals():\n",
    "    # Filtrar solo series mensuales\n",
    "    monthly_df = quality_df[quality_df['frecuencia'] == 'Mensual'].copy()\n",
    "    \n",
    "    for idx, row in monthly_df.iterrows():\n",
    "        # Normalizar fecha_fin para comparaci\u00f3n\n",
    "        fecha_fin = normalizar_fecha(row['fecha_fin'])\n",
    "        \n",
    "        if pd.notna(fecha_fin) and fecha_fin > fecha_corte:\n",
    "            source_name = row['fuente']\n",
    "            serie_name = row['serie']\n",
    "            \n",
    "            if source_name in all_data and serie_name in all_data[source_name]:\n",
    "                df = all_data[source_name][serie_name]\n",
    "                if isinstance(df, pd.DataFrame) and 'fecha' in df.columns:\n",
    "                    # Normalizar fechas en el DataFrame\n",
    "                    df_copy = df.copy()\n",
    "                    df_copy['fecha'] = df_copy['fecha'].apply(normalizar_fecha)\n",
    "                    \n",
    "                    # Contar registros posteriores a enero 2025\n",
    "                    df_2025 = df_copy[pd.to_datetime(df_copy['fecha']) > fecha_corte]\n",
    "                    \n",
    "                    if not df_2025.empty:\n",
    "                        series_mensuales_2025.append({\n",
    "                            'fuente': source_name,\n",
    "                            'serie': serie_name,\n",
    "                            'fecha_inicio': normalizar_fecha(row['fecha_inicio']),\n",
    "                            'fecha_fin': fecha_fin,\n",
    "                            'registros_totales': row['registros'],\n",
    "                            'registros_2025': len(df_2025),\n",
    "                            'fecha_max_2025': df_2025['fecha'].max(),\n",
    "                            'ultimo_valor': df_2025['valor'].iloc[-1] if 'valor' in df_2025.columns else None\n",
    "                        })\n",
    "\n",
    "print(f\"\u2705 Series mensuales con datos post-2025: {len(series_mensuales_2025)}\")\n",
    "if series_mensuales_2025:\n",
    "    for serie in series_mensuales_2025[:5]:  # Mostrar primeras 5\n",
    "        print(f\"   \u2022 [{serie['fuente']}] {serie['serie']}: hasta {serie['fecha_max_2025'].strftime('%Y-%m-%d')}\")\n",
    "\n",
    "# ============================================\n",
    "# SERIES SEMANALES\n",
    "# ============================================\n",
    "print(\"\\n\ud83d\udcc5 SERIES SEMANALES\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "series_semanales_2025 = []\n",
    "\n",
    "if 'quality_df' in locals():\n",
    "    # Filtrar solo series semanales\n",
    "    weekly_df = quality_df[quality_df['frecuencia'] == 'Semanal'].copy()\n",
    "    \n",
    "    for idx, row in weekly_df.iterrows():\n",
    "        # Normalizar fecha_fin para comparaci\u00f3n\n",
    "        fecha_fin = normalizar_fecha(row['fecha_fin'])\n",
    "        \n",
    "        if pd.notna(fecha_fin) and fecha_fin > fecha_corte:\n",
    "            source_name = row['fuente']\n",
    "            serie_name = row['serie']\n",
    "            \n",
    "            if source_name in all_data and serie_name in all_data[source_name]:\n",
    "                df = all_data[source_name][serie_name]\n",
    "                if isinstance(df, pd.DataFrame) and 'fecha' in df.columns:\n",
    "                    # Normalizar fechas en el DataFrame\n",
    "                    df_copy = df.copy()\n",
    "                    df_copy['fecha'] = df_copy['fecha'].apply(normalizar_fecha)\n",
    "                    \n",
    "                    # Contar registros posteriores a enero 2025\n",
    "                    df_2025 = df_copy[pd.to_datetime(df_copy['fecha']) > fecha_corte]\n",
    "                    \n",
    "                    if not df_2025.empty:\n",
    "                        series_semanales_2025.append({\n",
    "                            'fuente': source_name,\n",
    "                            'serie': serie_name,\n",
    "                            'fecha_inicio': normalizar_fecha(row['fecha_inicio']),\n",
    "                            'fecha_fin': fecha_fin,\n",
    "                            'registros_totales': row['registros'],\n",
    "                            'registros_2025': len(df_2025),\n",
    "                            'fecha_max_2025': df_2025['fecha'].max(),\n",
    "                            'ultimo_valor': df_2025['valor'].iloc[-1] if 'valor' in df_2025.columns else None\n",
    "                        })\n",
    "\n",
    "print(f\"\u2705 Series semanales con datos post-2025: {len(series_semanales_2025)}\")\n",
    "if series_semanales_2025:\n",
    "    for serie in series_semanales_2025[:5]:  # Mostrar primeras 5\n",
    "        print(f\"   \u2022 [{serie['fuente']}] {serie['serie']}: hasta {serie['fecha_max_2025'].strftime('%Y-%m-%d')}\")\n",
    "else:\n",
    "    print(\"   \u26a0\ufe0f No hay series semanales con datos recientes\")\n",
    "\n",
    "# ============================================\n",
    "# SERIES DIARIAS\n",
    "# ============================================\n",
    "print(\"\\n\ud83d\udcc5 SERIES DIARIAS\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "series_diarias_2025 = []\n",
    "\n",
    "if 'quality_df' in locals():\n",
    "    # Filtrar solo series diarias\n",
    "    daily_df = quality_df[quality_df['frecuencia'] == 'Diaria'].copy()\n",
    "    \n",
    "    for idx, row in daily_df.iterrows():\n",
    "        # Normalizar fecha_fin para comparaci\u00f3n\n",
    "        fecha_fin = normalizar_fecha(row['fecha_fin'])\n",
    "        \n",
    "        if pd.notna(fecha_fin) and fecha_fin > fecha_corte:\n",
    "            source_name = row['fuente']\n",
    "            serie_name = row['serie']\n",
    "            \n",
    "            if source_name in all_data and serie_name in all_data[source_name]:\n",
    "                df = all_data[source_name][serie_name]\n",
    "                if isinstance(df, pd.DataFrame) and 'fecha' in df.columns:\n",
    "                    # Normalizar fechas en el DataFrame\n",
    "                    df_copy = df.copy()\n",
    "                    df_copy['fecha'] = df_copy['fecha'].apply(normalizar_fecha)\n",
    "                    \n",
    "                    # Contar registros posteriores a enero 2025\n",
    "                    df_2025 = df_copy[pd.to_datetime(df_copy['fecha']) > fecha_corte]\n",
    "                    \n",
    "                    if not df_2025.empty:\n",
    "                        series_diarias_2025.append({\n",
    "                            'fuente': source_name,\n",
    "                            'serie': serie_name,\n",
    "                            'fecha_inicio': normalizar_fecha(row['fecha_inicio']),\n",
    "                            'fecha_fin': fecha_fin,\n",
    "                            'registros_totales': row['registros'],\n",
    "                            'registros_2025': len(df_2025),\n",
    "                            'fecha_max_2025': df_2025['fecha'].max(),\n",
    "                            'ultimo_valor': df_2025['valor'].iloc[-1] if 'valor' in df_2025.columns else None\n",
    "                        })\n",
    "\n",
    "# Ordenar por fecha m\u00e1s reciente\n",
    "series_diarias_2025 = sorted(series_diarias_2025, key=lambda x: x['fecha_max_2025'], reverse=True)\n",
    "\n",
    "print(f\"\u2705 Series diarias con datos post-2025: {len(series_diarias_2025)}\")\n",
    "\n",
    "if series_diarias_2025:\n",
    "    # Agrupar por fuente\n",
    "    fuentes_agrupadas = {}\n",
    "    for serie in series_diarias_2025:\n",
    "        if serie['fuente'] not in fuentes_agrupadas:\n",
    "            fuentes_agrupadas[serie['fuente']] = []\n",
    "        fuentes_agrupadas[serie['fuente']].append(serie)\n",
    "    \n",
    "    # Mostrar resumen por fuente\n",
    "    print(\"\\n\ud83d\udcca Resumen por fuente de datos:\")\n",
    "    for fuente, series in fuentes_agrupadas.items():\n",
    "        total_registros = sum(s['registros_2025'] for s in series)\n",
    "        print(f\"   \u2022 {fuente}: {len(series)} series, {total_registros:,} registros en 2025\")\n",
    "    \n",
    "    # TOP 5 series con m\u00e1s datos\n",
    "    print(\"\\n\ud83c\udfc6 TOP 5 series diarias con m\u00e1s datos en 2025:\")\n",
    "    top_series = sorted(series_diarias_2025, key=lambda x: x['registros_2025'], reverse=True)[:5]\n",
    "    for i, serie in enumerate(top_series, 1):\n",
    "        print(f\"   {i}. [{serie['fuente']}] {serie['serie']}: {serie['registros_2025']} registros\")\n",
    "\n",
    "# ============================================\n",
    "# RESUMEN FINAL\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\ud83c\udfaf RESUMEN FINAL - SERIES CON DATOS POST-2025\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "total_mensuales = len(series_mensuales_2025)\n",
    "total_semanales = len(series_semanales_2025)\n",
    "total_diarias = len(series_diarias_2025)\n",
    "total_general = total_mensuales + total_semanales + total_diarias\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Distribuci\u00f3n por frecuencia:\")\n",
    "print(f\"   \u2022 Mensuales: {total_mensuales} series\")\n",
    "print(f\"   \u2022 Semanales: {total_semanales} series\")\n",
    "print(f\"   \u2022 Diarias: {total_diarias} series\")\n",
    "print(f\"\\n\u2705 TOTAL: {total_general} series con datos actualizados m\u00e1s all\u00e1 de enero 2025\")\n",
    "\n",
    "if total_diarias > 0:\n",
    "    total_registros_2025 = sum(s['registros_2025'] for s in series_diarias_2025)\n",
    "    print(f\"\\n\ud83d\udcc8 Total de registros diarios en 2025: {total_registros_2025:,}\")\n",
    "    \n",
    "print(\"\\n\" + \"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcc8 Visualizaci\u00f3n de Series Temporales Clave\n",
    "\n",
    "Visualizamos las series m\u00e1s importantes para la predicci\u00f3n del precio de varilla corrugada.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear visualizaci\u00f3n comparativa de todas las series mensuales con datos recientes\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from datetime import datetime\n",
    "\n",
    "if monthly_series_by_source:\n",
    "    # Fecha de corte: enero 2025\n",
    "    fecha_corte = pd.Timestamp('2025-01-01')\n",
    "    \n",
    "    # Filtrar series que tienen datos m\u00e1s all\u00e1 de enero 2025\n",
    "    series_recientes = []\n",
    "    \n",
    "    print(\"\ud83d\udd0d Filtrando series con datos m\u00e1s all\u00e1 de enero 2025...\")\n",
    "    \n",
    "    for source_name in sorted(monthly_series_by_source.keys()):\n",
    "        source_series = monthly_series_by_source[source_name]\n",
    "        for idx, row in source_series.iterrows():\n",
    "            serie_name = row['serie']\n",
    "            if source_name in all_data and serie_name in all_data[source_name]:\n",
    "                df = all_data[source_name][serie_name]\n",
    "                if isinstance(df, pd.DataFrame) and 'fecha' in df.columns and 'valor' in df.columns:\n",
    "                    # Verificar si tiene datos despu\u00e9s de enero 2025\n",
    "                    if not df.empty and pd.to_datetime(df['fecha'].max()) > fecha_corte:\n",
    "                        series_recientes.append({\n",
    "                            'source': source_name,\n",
    "                            'serie': serie_name,\n",
    "                            'df': df,\n",
    "                            'fecha_max': df['fecha'].max(),\n",
    "                            'registros_2025': len(df[pd.to_datetime(df['fecha']) > fecha_corte])\n",
    "                        })\n",
    "    \n",
    "    # Informar cu\u00e1ntas series cumplen el criterio\n",
    "    print(f\"\u2705 Series con datos despu\u00e9s de enero 2025: {len(series_recientes)}\")\n",
    "    print(f\"\u274c Series excluidas (sin datos recientes): {sum(len(s) for s in monthly_series_by_source.values()) - len(series_recientes)}\")\n",
    "    \n",
    "    if series_recientes:\n",
    "        # Ordenar por fecha m\u00e1xima (m\u00e1s recientes primero)\n",
    "        series_recientes = sorted(series_recientes, key=lambda x: x['fecha_max'], reverse=True)\n",
    "        \n",
    "        # Mostrar resumen de series incluidas\n",
    "        print(\"\\n\ud83d\udcca Series incluidas en la visualizaci\u00f3n:\")\n",
    "        print(\"-\" * 60)\n",
    "        for s in series_recientes[:10]:  # Mostrar las 10 m\u00e1s recientes\n",
    "            fecha_str = s['fecha_max'].strftime('%Y-%m-%d') if hasattr(s['fecha_max'], 'strftime') else str(s['fecha_max'])\n",
    "            print(f\"  \u2022 [{s['source']}] {s['serie'][:30]}: hasta {fecha_str} ({s['registros_2025']} registros en 2025)\")\n",
    "        if len(series_recientes) > 10:\n",
    "            print(f\"  ... y {len(series_recientes) - 10} series m\u00e1s\")\n",
    "        \n",
    "        # Determinar n\u00famero de filas y columnas para subplots\n",
    "        n_cols = 3\n",
    "        n_rows = (len(series_recientes) + n_cols - 1) // n_cols\n",
    "        \n",
    "        # Preparar lista de t\u00edtulos\n",
    "        subplot_titles_list = []\n",
    "        for s in series_recientes:\n",
    "            fecha_str = s['fecha_max'].strftime('%m/%Y') if hasattr(s['fecha_max'], 'strftime') else str(s['fecha_max'])[:7]\n",
    "            subplot_titles_list.append(f\"{s['source']}: {s['serie'][:20]} (hasta {fecha_str})\")\n",
    "    \n",
    "        # Rellenar con t\u00edtulos vac\u00edos si es necesario para completar la grid\n",
    "        while len(subplot_titles_list) < n_rows * n_cols:\n",
    "            subplot_titles_list.append(\"\")\n",
    "        \n",
    "        # Crear figura con subplots y t\u00edtulos\n",
    "        fig_monthly = make_subplots(\n",
    "            rows=n_rows, \n",
    "            cols=n_cols,\n",
    "            subplot_titles=subplot_titles_list[:n_rows * n_cols],  # Usar solo los t\u00edtulos necesarios\n",
    "            vertical_spacing=0.12,  # Aumentar espacio para t\u00edtulos\n",
    "            horizontal_spacing=0.1\n",
    "        )\n",
    "        \n",
    "        # Colores por fuente\n",
    "        color_map = {\n",
    "            'FRED': 'blue',\n",
    "            'INEGI': 'green',\n",
    "            'banxico': 'red',\n",
    "            'world': 'purple',\n",
    "            'ahmsa': 'orange',\n",
    "            'trading': 'brown',\n",
    "            'YahooFinance': 'pink',\n",
    "            'LME': 'gray',\n",
    "            'RawMaterials': 'cyan'\n",
    "        }\n",
    "        \n",
    "        plot_idx = 0\n",
    "        \n",
    "        # Iterar por las series filtradas con datos recientes\n",
    "        for serie_info in series_recientes:\n",
    "            source_name = serie_info['source']\n",
    "            serie_name = serie_info['serie']\n",
    "            df = serie_info['df']\n",
    "            color = color_map.get(source_name, 'black')\n",
    "            \n",
    "            plot_idx += 1\n",
    "            row_idx = (plot_idx - 1) // n_cols + 1\n",
    "            col_idx = (plot_idx - 1) % n_cols + 1\n",
    "            \n",
    "            # Filtrar solo datos desde 2020 para mejor visualizaci\u00f3n\n",
    "            df_filtered = df[pd.to_datetime(df['fecha']) >= '2020-01-01'].copy()\n",
    "            \n",
    "            if not df_filtered.empty:\n",
    "                # Normalizar valores para comparaci\u00f3n (0-100)\n",
    "                valores = df_filtered['valor'].dropna()\n",
    "                if len(valores) > 0 and valores.max() != valores.min():\n",
    "                    valores_norm = (valores - valores.min()) / (valores.max() - valores.min()) * 100\n",
    "                    \n",
    "                    # Crear DataFrame temporal con \u00edndice alineado\n",
    "                    df_temp = df_filtered[['fecha']].copy()\n",
    "                    df_temp['valor_norm'] = valores_norm.values\n",
    "                    \n",
    "                    # Resaltar datos de 2025\n",
    "                    df_2025 = df_temp[pd.to_datetime(df_temp['fecha']) > fecha_corte]\n",
    "                    df_pre_2025 = df_temp[pd.to_datetime(df_temp['fecha']) <= fecha_corte]\n",
    "                    \n",
    "                    # Agregar traza para datos anteriores a 2025\n",
    "                    if not df_pre_2025.empty:\n",
    "                        fig_monthly.add_trace(\n",
    "                            go.Scatter(\n",
    "                                x=df_pre_2025['fecha'],\n",
    "                                y=df_pre_2025['valor_norm'],\n",
    "                                mode='lines',\n",
    "                                name=f\"{source_name}: {serie_name[:20]}\",\n",
    "                                line=dict(color=color, width=1.5),\n",
    "                                showlegend=False,\n",
    "                                hovertemplate=f\"<b>{serie_name}</b><br>Fecha: %{{x|%Y-%m}}<br>Valor Norm: %{{y:.1f}}<extra></extra>\"\n",
    "                            ),\n",
    "                            row=row_idx, col=col_idx\n",
    "                        )\n",
    "                    \n",
    "                    # Agregar traza para datos de 2025 (resaltados)\n",
    "                    if not df_2025.empty:\n",
    "                        fig_monthly.add_trace(\n",
    "                            go.Scatter(\n",
    "                                x=df_2025['fecha'],\n",
    "                                y=df_2025['valor_norm'],\n",
    "                                mode='lines+markers',\n",
    "                                name=f\"{source_name}: {serie_name[:20]} (2025)\",\n",
    "                                line=dict(color=color, width=2.5, dash='solid'),\n",
    "                                marker=dict(size=6, color=color),\n",
    "                                showlegend=False,\n",
    "                                hovertemplate=f\"<b>{serie_name} (2025)</b><br>Fecha: %{{x|%Y-%m}}<br>Valor Norm: %{{y:.1f}}<extra></extra>\"\n",
    "                            ),\n",
    "                            row=row_idx, col=col_idx\n",
    "                        )\n",
    "                    \n",
    "                    # Actualizar ejes\n",
    "                    fig_monthly.update_xaxes(\n",
    "                        tickformat=\"%Y-%m\",\n",
    "                        tickangle=45,\n",
    "                        row=row_idx, col=col_idx\n",
    "                    )\n",
    "                    fig_monthly.update_yaxes(\n",
    "                        title_text=\"Norm\",\n",
    "                        row=row_idx, col=col_idx\n",
    "                    )\n",
    "    \n",
    "        # Actualizar layout general\n",
    "        fig_monthly.update_layout(\n",
    "            height=250 * n_rows,  # Altura ajustada\n",
    "            title_text=\"\ud83d\udcca Series Mensuales con Datos Posteriores a Enero 2025 (Valores Normalizados 0-100)\",\n",
    "            title_font_size=16,\n",
    "            showlegend=False,\n",
    "            hovermode='x'\n",
    "        )\n",
    "        \n",
    "        # Agregar anotaci\u00f3n explicativa\n",
    "        fig_monthly.add_annotation(\n",
    "            text=\"Nota: Los datos de 2025 se muestran con l\u00ednea m\u00e1s gruesa y marcadores\",\n",
    "            xref=\"paper\",\n",
    "            yref=\"paper\",\n",
    "            x=0.5,\n",
    "            y=-0.02,\n",
    "            showarrow=False,\n",
    "            font=dict(size=10, color=\"gray\"),\n",
    "            xanchor='center'\n",
    "        )\n",
    "        \n",
    "        # Mostrar figura\n",
    "        fig_monthly.show()\n",
    "        \n",
    "        print(f\"\\n\u2705 Visualizaci\u00f3n generada con {plot_idx} series mensuales con datos recientes\")\n",
    "        print(\"\ud83d\udccc Los datos posteriores a enero 2025 est\u00e1n resaltados con l\u00ednea m\u00e1s gruesa y marcadores\")\n",
    "    else:\n",
    "        print(\"\\n\u26a0\ufe0f No se encontraron series mensuales con datos posteriores a enero 2025\")\n",
    "        print(\"   Esto podr\u00eda indicar que los datos est\u00e1n desactualizados o que la fecha actual del sistema es anterior a 2025\")\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f No hay series mensuales para visualizar\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaci\u00f3n de series temporales DIARIAS con datos posteriores a 2025\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "print(\"\ud83d\udcca VISUALIZACI\u00d3N DE SERIES DIARIAS CON DATOS POST-2025\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Fecha de corte\n",
    "fecha_corte = pd.Timestamp('2025-01-01')\n",
    "\n",
    "# Funci\u00f3n para normalizar fechas (eliminar timezone)\n",
    "def normalizar_fecha(fecha):\n",
    "    \"\"\"Elimina timezone de una fecha si lo tiene\"\"\"\n",
    "    if pd.notna(fecha):\n",
    "        if hasattr(fecha, 'tz') and fecha.tz is not None:\n",
    "            return fecha.tz_localize(None)\n",
    "    return fecha\n",
    "\n",
    "# Funci\u00f3n para obtener el valor de cierre seg\u00fan la fuente\n",
    "def obtener_valor_cierre(df, fuente):\n",
    "    \"\"\"Obtiene la columna de valor de cierre seg\u00fan la fuente de datos\"\"\"\n",
    "    if fuente in ['ahmsa', 'LME', 'YahooFinance']:\n",
    "        if 'Close' in df.columns:\n",
    "            return df['Close']\n",
    "    elif fuente == 'RawMaterials':\n",
    "        if 'precio_cierre' in df.columns:\n",
    "            return df['precio_cierre']\n",
    "    \n",
    "    # Fallback a otras columnas comunes\n",
    "    if 'valor' in df.columns:\n",
    "        return df['valor']\n",
    "    elif 'close' in df.columns:\n",
    "        return df['close']\n",
    "    elif 'Close' in df.columns:\n",
    "        return df['Close']\n",
    "    elif 'precio_cierre' in df.columns:\n",
    "        return df['precio_cierre']\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Recopilar series diarias con datos post-2025\n",
    "series_para_graficar = []\n",
    "\n",
    "print(\"\\n\ud83d\udd0d Buscando series diarias con datos posteriores a enero 2025...\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "for fuente_nombre, fuente_datos in all_data.items():\n",
    "    if isinstance(fuente_datos, dict):\n",
    "        for serie_nombre, df in fuente_datos.items():\n",
    "            if isinstance(df, pd.DataFrame) and 'fecha' in df.columns:\n",
    "                # Normalizar fechas\n",
    "                df_copy = df.copy()\n",
    "                df_copy['fecha'] = df_copy['fecha'].apply(normalizar_fecha)\n",
    "                \n",
    "                # Verificar si hay datos despu\u00e9s de 2025\n",
    "                df_2025 = df_copy[pd.to_datetime(df_copy['fecha']) > fecha_corte]\n",
    "                \n",
    "                if not df_2025.empty:\n",
    "                    # Obtener valor de cierre\n",
    "                    valores = obtener_valor_cierre(df_2025, fuente_nombre)\n",
    "                    \n",
    "                    if valores is not None:\n",
    "                        # Verificar que sea una serie diaria (m\u00e1s de 20 puntos por mes en promedio)\n",
    "                        dias_rango = (df_2025['fecha'].max() - df_2025['fecha'].min()).days\n",
    "                        if dias_rango > 0:\n",
    "                            puntos_por_dia = len(df_2025) / dias_rango\n",
    "                            \n",
    "                            # Considerar diaria si tiene m\u00e1s de 0.5 puntos por d\u00eda (excluyendo fines de semana)\n",
    "                            if puntos_por_dia > 0.5:\n",
    "                                series_para_graficar.append({\n",
    "                                    'fuente': fuente_nombre,\n",
    "                                    'serie': serie_nombre,\n",
    "                                    'datos': df_2025,\n",
    "                                    'valores': valores,\n",
    "                                    'fecha_min': df_2025['fecha'].min(),\n",
    "                                    'fecha_max': df_2025['fecha'].max(),\n",
    "                                    'num_puntos': len(df_2025),\n",
    "                                    'ultimo_valor': valores.iloc[-1] if len(valores) > 0 else None\n",
    "                                })\n",
    "                                print(f\"\u2705 [{fuente_nombre}] {serie_nombre}: {len(df_2025)} puntos desde {df_2025['fecha'].min().strftime('%Y-%m-%d')}\")\n",
    "\n",
    "print(f\"\\n\ud83d\udcc8 Total de series diarias encontradas: {len(series_para_graficar)}\")\n",
    "\n",
    "# Crear visualizaci\u00f3n si hay series\n",
    "if series_para_graficar:\n",
    "    # Agrupar por fuente para organizar subplots\n",
    "    fuentes_unicas = list(set([s['fuente'] for s in series_para_graficar]))\n",
    "    \n",
    "    # Determinar layout de subplots\n",
    "    n_fuentes = len(fuentes_unicas)\n",
    "    n_cols = 2\n",
    "    n_rows = (n_fuentes + n_cols - 1) // n_cols\n",
    "    \n",
    "    # Crear subplots\n",
    "    fig = make_subplots(\n",
    "        rows=n_rows, \n",
    "        cols=n_cols,\n",
    "        subplot_titles=[f\"{fuente}\" for fuente in fuentes_unicas[:n_rows*n_cols]],\n",
    "        vertical_spacing=0.1,\n",
    "        horizontal_spacing=0.15\n",
    "    )\n",
    "    \n",
    "    # Colores por fuente\n",
    "    color_map = {\n",
    "        'FRED': '#1f77b4',\n",
    "        'INEGI': '#2ca02c',\n",
    "        'banxico': '#d62728',\n",
    "        'world': '#9467bd',\n",
    "        'ahmsa': '#ff7f0e',\n",
    "        'trading': '#8c564b',\n",
    "        'YahooFinance': '#e377c2',\n",
    "        'LME': '#7f7f7f',\n",
    "        'RawMaterials': '#17becf'\n",
    "    }\n",
    "    \n",
    "    # Agregar series al gr\u00e1fico\n",
    "    for idx, fuente in enumerate(fuentes_unicas[:n_rows*n_cols]):\n",
    "        row = idx // n_cols + 1\n",
    "        col = idx % n_cols + 1\n",
    "        \n",
    "        # Filtrar series de esta fuente\n",
    "        series_fuente = [s for s in series_para_graficar if s['fuente'] == fuente]\n",
    "        \n",
    "        # Agregar cada serie\n",
    "        for i, serie_info in enumerate(series_fuente[:10]):  # M\u00e1ximo 10 series por subplot\n",
    "            df_plot = serie_info['datos']\n",
    "            valores_plot = serie_info['valores']\n",
    "            \n",
    "            # Resetear \u00edndice si es necesario\n",
    "            if not isinstance(valores_plot.index, pd.RangeIndex):\n",
    "                valores_plot = valores_plot.reset_index(drop=True)\n",
    "            \n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=df_plot['fecha'],\n",
    "                    y=valores_plot,\n",
    "                    name=serie_info['serie'][:30],  # Limitar longitud del nombre\n",
    "                    mode='lines',\n",
    "                    line=dict(width=1.5, color=color_map.get(fuente, 'black')),\n",
    "                    opacity=0.8 - (i * 0.05),  # Reducir opacidad para series adicionales\n",
    "                    showlegend=(i < 3),  # Solo mostrar leyenda para las primeras 3\n",
    "                    hovertemplate=f\"<b>{serie_info['serie']}</b><br>Fecha: %{{x|%Y-%m-%d}}<br>Valor: %{{y:.2f}}<extra></extra>\"\n",
    "                ),\n",
    "                row=row, col=col\n",
    "            )\n",
    "        \n",
    "        # Actualizar ejes\n",
    "        fig.update_xaxes(\n",
    "            title_text=\"\",\n",
    "            tickformat=\"%Y-%m\",\n",
    "            tickangle=45,\n",
    "            row=row, col=col\n",
    "        )\n",
    "        fig.update_yaxes(\n",
    "            title_text=\"Valor\",\n",
    "            row=row, col=col\n",
    "        )\n",
    "    \n",
    "    # Actualizar layout general\n",
    "    fig.update_layout(\n",
    "        height=300 * n_rows,\n",
    "        title_text=\"\ud83d\udcca Series Temporales Diarias con Datos Post-2025 (Valores de Cierre)\",\n",
    "        title_font_size=16,\n",
    "        showlegend=True,\n",
    "        legend=dict(\n",
    "            orientation=\"v\",\n",
    "            yanchor=\"top\",\n",
    "            y=1,\n",
    "            xanchor=\"left\",\n",
    "            x=1.02,\n",
    "            font=dict(size=10)\n",
    "        ),\n",
    "        hovermode='x unified'\n",
    "    )\n",
    "    \n",
    "    # Mostrar figura\n",
    "    fig.show()\n",
    "    \n",
    "    # Resumen estad\u00edstico\n",
    "    print(\"\\n\ud83d\udcca RESUMEN ESTAD\u00cdSTICO\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    # Agrupar por fuente para estad\u00edsticas\n",
    "    estadisticas_por_fuente = {}\n",
    "    for serie in series_para_graficar:\n",
    "        fuente = serie['fuente']\n",
    "        if fuente not in estadisticas_por_fuente:\n",
    "            estadisticas_por_fuente[fuente] = {\n",
    "                'num_series': 0,\n",
    "                'total_puntos': 0,\n",
    "                'series': []\n",
    "            }\n",
    "        estadisticas_por_fuente[fuente]['num_series'] += 1\n",
    "        estadisticas_por_fuente[fuente]['total_puntos'] += serie['num_puntos']\n",
    "        estadisticas_por_fuente[fuente]['series'].append(serie['serie'])\n",
    "    \n",
    "    # Mostrar estad\u00edsticas\n",
    "    for fuente, stats in sorted(estadisticas_por_fuente.items()):\n",
    "        print(f\"\\n\ud83d\udccc {fuente}:\")\n",
    "        print(f\"   \u2022 Series: {stats['num_series']}\")\n",
    "        print(f\"   \u2022 Total puntos en 2025: {stats['total_puntos']:,}\")\n",
    "        print(f\"   \u2022 Series incluidas: {', '.join(stats['series'][:5])}\")\n",
    "        if len(stats['series']) > 5:\n",
    "            print(f\"     ... y {len(stats['series']) - 5} m\u00e1s\")\n",
    "    \n",
    "    # TOP 5 series con m\u00e1s datos\n",
    "    print(\"\\n\ud83c\udfc6 TOP 5 SERIES CON M\u00c1S DATOS EN 2025:\")\n",
    "    print(\"-\"*40)\n",
    "    top_series = sorted(series_para_graficar, key=lambda x: x['num_puntos'], reverse=True)[:5]\n",
    "    for i, serie in enumerate(top_series, 1):\n",
    "        print(f\"{i}. [{serie['fuente']}] {serie['serie']}: {serie['num_puntos']} puntos\")\n",
    "        if serie['ultimo_valor'] is not None:\n",
    "            print(f\"   \u00daltimo valor: {serie['ultimo_valor']:.2f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n\u26a0\ufe0f No se encontraron series diarias con datos posteriores a enero 2025\")\n",
    "    print(\"   Verificar que la ingesta de datos se haya ejecutado correctamente\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udd17 An\u00e1lisis de Correlaciones\n",
    "\n",
    "Analizamos las correlaciones entre las diferentes series temporales y el precio de la varilla.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCI\u00d3N HELPER: Manejo correcto de fechas con timezone y normalizaci\u00f3n\n",
    "def safe_to_datetime(fecha_series, normalize=True):\n",
    "    \"\"\"\n",
    "    Convierte de manera segura una serie de fechas a datetime, manejando timezones.\n",
    "    \n",
    "    Args:\n",
    "        fecha_series: Serie de pandas con fechas (puede tener timezone o no)\n",
    "        normalize: Si True, elimina la hora y deja solo fecha (a\u00f1o-mes-d\u00eda)\n",
    "    \n",
    "    Returns:\n",
    "        Serie de pandas con fechas sin timezone (timezone-naive) y normalizadas\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Intento 1: Conversi\u00f3n directa\n",
    "        result = pd.to_datetime(fecha_series)\n",
    "    except (ValueError, TypeError):\n",
    "        try:\n",
    "            # Intento 2: Con UTC=True y luego eliminar timezone\n",
    "            result = pd.to_datetime(fecha_series, utc=True)\n",
    "            result = result.dt.tz_localize(None)\n",
    "        except:\n",
    "            # Intento 3: Convertir elemento por elemento\n",
    "            result = fecha_series.apply(lambda x: \n",
    "                pd.to_datetime(x).tz_localize(None) if hasattr(pd.to_datetime(x), 'tz_localize') \n",
    "                else pd.to_datetime(x))\n",
    "    \n",
    "    # Verificaci\u00f3n final: eliminar timezone si a\u00fan existe\n",
    "    if hasattr(result.dtype, 'tz') and result.dt.tz is not None:\n",
    "        result = result.dt.tz_localize(None)\n",
    "    \n",
    "    # NORMALIZAR: Eliminar horas, minutos, segundos (dejar solo fecha)\n",
    "    if normalize:\n",
    "        result = result.dt.normalize()  # Esto pone la hora en 00:00:00\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"\u2705 Funci\u00f3n helper para manejo de fechas con timezone y normalizaci\u00f3n definida\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIAGN\u00d3STICO: Visualizar el proceso de JOIN de variables diarias\n",
    "print(\"=\"*80)\n",
    "print(\"\ud83d\udd0d DIAGN\u00d3STICO: PROCESO DE JOIN DE VARIABLES DIARIAS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Fecha de corte para verificar si las series est\u00e1n actualizadas\n",
    "fecha_corte = pd.Timestamp('2025-01-01')\n",
    "\n",
    "# 1. Primero, obtener la variable objetivo\n",
    "print(\"\\n1\ufe0f\u20e3 VARIABLE OBJETIVO:\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "variable_objetivo_df = None\n",
    "if 'LME' in all_data and 'steel_rebar' in all_data['LME']:\n",
    "    df = all_data['LME']['steel_rebar']\n",
    "    if isinstance(df, pd.DataFrame) and 'fecha' in df.columns:\n",
    "        # Buscar columna de valor\n",
    "        if 'Close' in df.columns:\n",
    "            valor_col = 'Close'\n",
    "        elif 'precio_cierre' in df.columns:\n",
    "            valor_col = 'precio_cierre'\n",
    "        elif 'valor' in df.columns:\n",
    "            valor_col = 'valor'\n",
    "        else:\n",
    "            valor_col = df.select_dtypes(include=['float64', 'int64']).columns[0] if len(df.select_dtypes(include=['float64', 'int64']).columns) > 0 else None\n",
    "        \n",
    "        if valor_col:\n",
    "            variable_objetivo_df = df[['fecha', valor_col]].copy()\n",
    "            variable_objetivo_df.columns = ['fecha', 'precio_varilla_lme']\n",
    "            \n",
    "            # Usar la funci\u00f3n helper para manejar fechas con timezone Y NORMALIZAR\n",
    "            variable_objetivo_df['fecha'] = safe_to_datetime(variable_objetivo_df['fecha'], normalize=True)\n",
    "            \n",
    "            variable_objetivo_df.set_index('fecha', inplace=True)\n",
    "            \n",
    "            # IMPORTANTE: Despu\u00e9s de normalizar, pueden haber duplicados\n",
    "            # Mantener el promedio de valores duplicados para la misma fecha\n",
    "            if variable_objetivo_df.index.duplicated().any():\n",
    "                print(f\"   \u26a0\ufe0f Fechas duplicadas detectadas despu\u00e9s de normalizar. Agregando por promedio...\")\n",
    "                variable_objetivo_df = variable_objetivo_df.groupby(level=0).mean()\n",
    "            \n",
    "            variable_objetivo_df = variable_objetivo_df.dropna()\n",
    "            \n",
    "            print(f\"\u2705 LME steel_rebar encontrada:\")\n",
    "            print(f\"   \u2022 Registros totales: {len(variable_objetivo_df)}\")\n",
    "            print(f\"   \u2022 Rango de fechas: {variable_objetivo_df.index.min()} a {variable_objetivo_df.index.max()}\")\n",
    "            print(f\"   \u2022 Frecuencia aparente: {pd.infer_freq(variable_objetivo_df.index[:100])}\")\n",
    "            print(f\"   \u2022 Primeros valores:\")\n",
    "            print(variable_objetivo_df.head(3))\n",
    "            print(f\"   \u2022 \u00daltimos valores:\")\n",
    "            print(variable_objetivo_df.tail(3))\n",
    "\n",
    "# 2. Analizar algunas series diarias antes del join\n",
    "print(\"\\n2\ufe0f\u20e3 AN\u00c1LISIS DE SERIES DIARIAS INDIVIDUALES:\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# Seleccionar algunas series diarias clave para analizar\n",
    "series_to_check = [\n",
    "    ('banxico', 'usd_mxn', 'tipo_cambio'),\n",
    "    ('YahooFinance', 'SP500', 'sp500'),\n",
    "    ('LME', 'Cobre', 'cobre_lme'),\n",
    "    ('FRED', 'dxy_index', 'dxy_index'),\n",
    "    ('RawMaterials', 'MineralHierro_VALE', 'vale')\n",
    "]\n",
    "\n",
    "series_dataframes = {}\n",
    "for source, series_key, name in series_to_check:\n",
    "    if source in all_data:\n",
    "        # Buscar la serie (puede tener fecha a\u00f1adida)\n",
    "        df = None\n",
    "        if series_key in all_data[source]:\n",
    "            df = all_data[source][series_key]\n",
    "        else:\n",
    "            # Buscar con fecha\n",
    "            matching = [k for k in all_data[source].keys() if k.startswith(series_key) and 'metadata' not in k]\n",
    "            if matching:\n",
    "                df = all_data[source][matching[0]]\n",
    "        \n",
    "        if df is not None and isinstance(df, pd.DataFrame) and 'fecha' in df.columns:\n",
    "            # Buscar columna de valor\n",
    "            valor_col = None\n",
    "            for col in ['Close', 'precio_cierre', 'valor']:\n",
    "                if col in df.columns:\n",
    "                    valor_col = col\n",
    "                    break\n",
    "            \n",
    "            if not valor_col and len(df.columns) > 1:\n",
    "                # Tomar la primera columna num\u00e9rica que no sea fecha\n",
    "                numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "                valor_col = numeric_cols[0] if len(numeric_cols) > 0 else None\n",
    "            \n",
    "            if valor_col:\n",
    "                temp_df = df[['fecha', valor_col]].copy()\n",
    "                temp_df.columns = ['fecha', name]\n",
    "                \n",
    "                # Usar la funci\u00f3n helper para manejar fechas con timezone Y NORMALIZAR\n",
    "                temp_df['fecha'] = safe_to_datetime(temp_df['fecha'], normalize=True)\n",
    "                \n",
    "                temp_df.set_index('fecha', inplace=True)\n",
    "                \n",
    "                # Manejar duplicados despu\u00e9s de normalizar\n",
    "                if temp_df.index.duplicated().any():\n",
    "                    num_duplicates = temp_df.index.duplicated().sum()\n",
    "                    print(f\"      \u26a0\ufe0f {num_duplicates} fechas duplicadas. Agregando por promedio...\")\n",
    "                    temp_df = temp_df.groupby(level=0).mean()\n",
    "                \n",
    "                temp_df = temp_df.dropna()\n",
    "                \n",
    "                series_dataframes[name] = temp_df\n",
    "                \n",
    "                print(f\"\\n\ud83d\udcca {source} - {name}:\")\n",
    "                print(f\"   \u2022 Registros: {len(temp_df)}\")\n",
    "                print(f\"   \u2022 Rango: {temp_df.index.min()} a {temp_df.index.max()}\")\n",
    "                print(f\"   \u2022 Actualizada: {'\u2705 S\u00cd' if temp_df.index.max() > fecha_corte else '\u274c NO'}\")\n",
    "                print(f\"   \u2022 Muestra de valores: {temp_df[name].head(3).values}\")\n",
    "\n",
    "# 3. Proceso de JOIN\n",
    "print(\"\\n3\ufe0f\u20e3 PROCESO DE JOIN (OUTER):\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "if variable_objetivo_df is not None and len(series_dataframes) > 0:\n",
    "    # Empezar con la variable objetivo\n",
    "    consolidated = variable_objetivo_df.copy()\n",
    "    print(f\"\\nIniciando con variable objetivo: {consolidated.shape}\")\n",
    "    \n",
    "    # Hacer join con cada serie\n",
    "    for i, (name, df) in enumerate(series_dataframes.items(), 1):\n",
    "        print(f\"\\n{i}. JOIN con {name}:\")\n",
    "        print(f\"   \u2022 DataFrame actual: {consolidated.shape}\")\n",
    "        print(f\"   \u2022 Serie a unir: {df.shape}\")\n",
    "        \n",
    "        # Hacer el join\n",
    "        consolidated = consolidated.join(df, how='outer')\n",
    "        \n",
    "        print(f\"   \u2022 Resultado despu\u00e9s del join: {consolidated.shape}\")\n",
    "        print(f\"   \u2022 NaN introducidos: {consolidated.isna().sum().sum() - consolidated.iloc[:, :-1].isna().sum().sum()}\")\n",
    "    \n",
    "    print(\"\\n4\ufe0f\u20e3 ESTADO FINAL DEL DATAFRAME CONSOLIDADO:\")\n",
    "    print(\"-\"*40)\n",
    "    print(f\"\u2022 Forma final: {consolidated.shape}\")\n",
    "    print(f\"\u2022 Rango de fechas: {consolidated.index.min()} a {consolidated.index.max()}\")\n",
    "    print(f\"\u2022 Total de NaN: {consolidated.isna().sum().sum()}\")\n",
    "    print(f\"\u2022 \u00bf\u00cdndice mon\u00f3tono?: {consolidated.index.is_monotonic_increasing}\")\n",
    "    print(f\"\u2022 \u00bfFechas duplicadas?: {consolidated.index.duplicated().any()}\")\n",
    "    \n",
    "    # An\u00e1lisis de NaN por columna\n",
    "    print(\"\\n5\ufe0f\u20e3 AN\u00c1LISIS DE NaN POR COLUMNA:\")\n",
    "    print(\"-\"*40)\n",
    "    nan_analysis = pd.DataFrame({\n",
    "        'columna': consolidated.columns,\n",
    "        'total_valores': len(consolidated),\n",
    "        'valores_no_nan': consolidated.count(),\n",
    "        'valores_nan': consolidated.isna().sum(),\n",
    "        'porcentaje_nan': (consolidated.isna().sum() / len(consolidated) * 100).round(1)\n",
    "    })\n",
    "    print(nan_analysis)\n",
    "    \n",
    "    # Mostrar algunas filas del DataFrame consolidado\n",
    "    print(\"\\n6\ufe0f\u20e3 MUESTRA DEL DATAFRAME CONSOLIDADO:\")\n",
    "    print(\"-\"*40)\n",
    "    print(\"Primeras 5 filas:\")\n",
    "    print(consolidated.head())\n",
    "    print(\"\\n\u00daltimas 5 filas:\")\n",
    "    print(consolidated.tail())\n",
    "    \n",
    "    # An\u00e1lisis de gaps en las fechas\n",
    "    print(\"\\n7\ufe0f\u20e3 AN\u00c1LISIS DE GAPS EN FECHAS:\")\n",
    "    print(\"-\"*40)\n",
    "    date_diffs = consolidated.index.to_series().diff().dropna()\n",
    "    print(f\"\u2022 Diferencia m\u00ednima entre fechas: {date_diffs.min()}\")\n",
    "    print(f\"\u2022 Diferencia m\u00e1xima entre fechas: {date_diffs.max()}\")\n",
    "    print(f\"\u2022 Diferencia promedio: {date_diffs.mean()}\")\n",
    "    print(f\"\u2022 Mediana de diferencias: {date_diffs.median()}\")\n",
    "    \n",
    "    # Distribuci\u00f3n de gaps\n",
    "    gaps_distribution = date_diffs.value_counts().head(10)\n",
    "    print(\"\\nDistribuci\u00f3n de gaps (top 10):\")\n",
    "    for gap, count in gaps_distribution.items():\n",
    "        print(f\"   \u2022 {gap.days} d\u00edas: {count} veces\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\ud83d\udca1 OBSERVACIONES CLAVE:\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "1. El JOIN 'outer' preserva TODAS las fechas de TODAS las series\n",
    "2. Esto introduce muchos NaN donde una serie no tiene datos en fechas espec\u00edficas\n",
    "3. Series con diferentes rangos de fechas causan NaN en los extremos\n",
    "4. Series diarias vs mensuales causan muchos NaN en d\u00edas intermedios\n",
    "5. El \u00edndice puede quedar desordenado despu\u00e9s de m\u00faltiples joins\n",
    "\n",
    "RECOMENDACI\u00d3N: Usar solo series diarias y actualizadas para minimizar NaN\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear matriz de correlaciones - ACTUALIZADA CON DATOS REALES\n",
    "def create_correlation_matrix(all_data: Dict) -> pd.DataFrame:\n",
    "    \"\"\"Crea una matriz de correlaci\u00f3n entre todas las series temporales\"\"\"\n",
    "    \n",
    "    # Crear DataFrame consolidado con todas las series\n",
    "    consolidated_df = pd.DataFrame()\n",
    "    \n",
    "    # Variable objetivo: precio de varilla (LME steel_rebar)\n",
    "    print(\"\ud83c\udfaf Buscando variable objetivo (precio de varilla)...\")\n",
    "    \n",
    "    # Intentar con LME steel_rebar primero\n",
    "    if 'LME' in all_data and 'steel_rebar' in all_data['LME']:\n",
    "        df = all_data['LME']['steel_rebar']\n",
    "        if isinstance(df, pd.DataFrame) and 'fecha' in df.columns:\n",
    "            # Buscar columna de valor (Close, precio_cierre, valor)\n",
    "            valor_col = None\n",
    "            if 'Close' in df.columns:\n",
    "                valor_col = 'Close'\n",
    "            elif 'precio_cierre' in df.columns:\n",
    "                valor_col = 'precio_cierre'\n",
    "            elif 'valor' in df.columns:\n",
    "                valor_col = 'valor'\n",
    "            \n",
    "            if valor_col:\n",
    "                df_pivot = df[['fecha', valor_col]].copy()\n",
    "                df_pivot.columns = ['fecha', 'precio_varilla_lme']\n",
    "                \n",
    "                # Usar funci\u00f3n helper para normalizar fechas\n",
    "                df_pivot['fecha'] = safe_to_datetime(df_pivot['fecha'], normalize=True)\n",
    "                \n",
    "                df_pivot.set_index('fecha', inplace=True)\n",
    "                \n",
    "                # Manejar duplicados despu\u00e9s de normalizar\n",
    "                if df_pivot.index.duplicated().any():\n",
    "                    df_pivot = df_pivot.groupby(level=0).mean()\n",
    "                consolidated_df = df_pivot\n",
    "                print(f\"   \u2705 Variable objetivo encontrada: LME steel_rebar ({len(df_pivot)} puntos)\")\n",
    "    \n",
    "    # Si no encontramos LME, intentar con AHMSA\n",
    "    if consolidated_df.empty and 'ahmsa' in all_data:\n",
    "        # Buscar la clave correcta para AHMSA\n",
    "        ahmsa_keys = [k for k in all_data['ahmsa'].keys() if 'ahmsa' in k.lower() and 'metadata' not in k]\n",
    "        if ahmsa_keys:\n",
    "            df = all_data['ahmsa'][ahmsa_keys[0]]\n",
    "            if isinstance(df, pd.DataFrame) and 'fecha' in df.columns:\n",
    "                valor_col = None\n",
    "                if 'Close' in df.columns:\n",
    "                    valor_col = 'Close'\n",
    "                elif 'precio_cierre' in df.columns:\n",
    "                    valor_col = 'precio_cierre'\n",
    "                elif 'valor' in df.columns:\n",
    "                    valor_col = 'valor'\n",
    "                \n",
    "                if valor_col:\n",
    "                    df_pivot = df[['fecha', valor_col]].copy()\n",
    "                    df_pivot.columns = ['fecha', 'precio_varilla_ahmsa']\n",
    "                    \n",
    "                    # Usar funci\u00f3n helper para normalizar fechas\n",
    "                    df_pivot['fecha'] = safe_to_datetime(df_pivot['fecha'], normalize=True)\n",
    "                    \n",
    "                    df_pivot.set_index('fecha', inplace=True)\n",
    "                    \n",
    "                    # Manejar duplicados despu\u00e9s de normalizar\n",
    "                    if df_pivot.index.duplicated().any():\n",
    "                        df_pivot = df_pivot.groupby(level=0).mean()\n",
    "                    consolidated_df = df_pivot\n",
    "                    print(f\"   \u2705 Variable objetivo alternativa: AHMSA ({len(df_pivot)} puntos)\")\n",
    "    \n",
    "    if consolidated_df.empty:\n",
    "        print(\"   \u26a0\ufe0f No se encontr\u00f3 variable objetivo\")\n",
    "        return pd.DataFrame(), pd.DataFrame()\n",
    "    \n",
    "    print(\"\\n\ud83d\udcca Agregando series explicativas...\")\n",
    "    \n",
    "    # Agregar otras series importantes con las claves correctas\n",
    "    series_to_add = [\n",
    "        # Banxico - usar las claves con fecha\n",
    "        ('banxico', 'usd_mxn', 'tipo_cambio', ['Close', 'valor']),\n",
    "        ('banxico', 'tiie_28', 'tiie_28', ['Close', 'valor']),\n",
    "        ('banxico', 'interest_rate', 'tasa_interes', ['Close', 'valor']),\n",
    "        ('banxico', 'udis', 'udis', ['Close', 'valor']),\n",
    "        \n",
    "        # FRED\n",
    "        ('FRED', 'federal_funds_rate', 'tasa_fed', ['valor', 'Close']),\n",
    "        ('FRED', 'ProduccionIndustrial', 'prod_industrial_us', ['valor', 'Close']),\n",
    "        ('FRED', 'steel_production', 'produccion_acero_us', ['valor', 'Close']),\n",
    "        ('FRED', 'ppi_metals', 'ppi_metales', ['valor', 'Close']),\n",
    "        ('FRED', 'dxy_index', 'dxy_index', ['valor', 'Close']),\n",
    "        ('FRED', 'natural_gas', 'gas_natural', ['valor', 'Close']),\n",
    "        \n",
    "        # LME - Metales\n",
    "        ('LME', 'Cobre', 'cobre_lme', ['Close', 'precio_cierre', 'valor']),\n",
    "        ('LME', 'Aluminio', 'aluminio_lme', ['Close', 'precio_cierre', 'valor']),\n",
    "        ('LME', 'Zinc', 'zinc_lme', ['Close', 'precio_cierre', 'valor']),\n",
    "        ('LME', 'iron_ore', 'mineral_hierro_lme', ['Close', 'precio_cierre', 'valor']),\n",
    "        ('LME', 'coking_coal', 'carbon_coque_lme', ['Close', 'precio_cierre', 'valor']),\n",
    "        \n",
    "        # INEGI\n",
    "        ('INEGI', 'inpc_general', 'inflacion_mx', ['valor', 'Close']),\n",
    "        ('INEGI', 'ProduccionConstruccion', 'construccion_mx', ['valor', 'Close']),\n",
    "        ('INEGI', 'produccion_metalurgica', 'prod_metalurgica_mx', ['valor', 'Close']),\n",
    "        ('INEGI', 'inpp_construccion', 'inpp_construccion', ['valor', 'Close']),\n",
    "        \n",
    "        # Raw Materials\n",
    "        ('RawMaterials', 'MineralHierro_VALE', 'vale_mineral_hierro', ['Close', 'precio_cierre', 'valor']),\n",
    "        ('RawMaterials', 'MineralHierro_RIO', 'rio_mineral_hierro', ['Close', 'precio_cierre', 'valor']),\n",
    "        ('RawMaterials', 'MineralHierro_BHP', 'bhp_mineral_hierro', ['Close', 'precio_cierre', 'valor']),\n",
    "        ('RawMaterials', 'ETF_Acero_SLX', 'etf_acero_slx', ['Close', 'precio_cierre', 'valor']),\n",
    "        ('RawMaterials', 'CarbonCoque_TECK', 'carbon_coque_teck', ['Close', 'precio_cierre', 'valor']),\n",
    "        \n",
    "        # Yahoo Finance\n",
    "        ('YahooFinance', 'SP500', 'sp500', ['Close', 'precio_cierre', 'valor']),\n",
    "        ('YahooFinance', 'Petroleo_WTI', 'petroleo_wti', ['Close', 'precio_cierre', 'valor']),\n",
    "        ('YahooFinance', 'Petroleo_Brent', 'petroleo_brent', ['Close', 'precio_cierre', 'valor']),\n",
    "        ('YahooFinance', 'commodities_etf', 'commodities_etf', ['Close', 'precio_cierre', 'valor']),\n",
    "        ('YahooFinance', 'materials_etf', 'materials_etf', ['Close', 'precio_cierre', 'valor']),\n",
    "        ('YahooFinance', 'VIX_Volatilidad', 'vix', ['Close', 'precio_cierre', 'valor']),\n",
    "        ('YahooFinance', 'treasury_10y', 'bonos_10y', ['Close', 'precio_cierre', 'valor']),\n",
    "    ]\n",
    "    \n",
    "    # Tambi\u00e9n buscar las series de AHMSA con fecha\n",
    "    if 'ahmsa' in all_data:\n",
    "        for key in all_data['ahmsa'].keys():\n",
    "            if 'metadata' not in key:\n",
    "                # Agregar las series de AHMSA que no sean la principal\n",
    "                if 'nucor' in key.lower():\n",
    "                    series_to_add.append(('ahmsa', key, 'nucor', ['Close', 'precio_cierre', 'valor']))\n",
    "                elif 'arcelormittal' in key.lower():\n",
    "                    series_to_add.append(('ahmsa', key, 'arcelormittal', ['Close', 'precio_cierre', 'valor']))\n",
    "                elif 'ternium' in key.lower():\n",
    "                    series_to_add.append(('ahmsa', key, 'ternium', ['Close', 'precio_cierre', 'valor']))\n",
    "                elif 'steel_etf' in key.lower():\n",
    "                    series_to_add.append(('ahmsa', key, 'steel_etf_ahmsa', ['Close', 'precio_cierre', 'valor']))\n",
    "    \n",
    "    # Procesar cada serie\n",
    "    series_added = []\n",
    "    for source, series, name, value_cols in series_to_add:\n",
    "        if source in all_data:\n",
    "            # Si la serie no tiene fecha en el nombre, buscar con fecha\n",
    "            if series in all_data[source]:\n",
    "                df = all_data[source][series]\n",
    "            else:\n",
    "                # Buscar la serie con fecha a\u00f1adida\n",
    "                matching_keys = [k for k in all_data[source].keys() \n",
    "                               if k.startswith(series) and 'metadata' not in k]\n",
    "                if matching_keys:\n",
    "                    df = all_data[source][matching_keys[0]]\n",
    "                else:\n",
    "                    continue\n",
    "            \n",
    "            if isinstance(df, pd.DataFrame) and 'fecha' in df.columns:\n",
    "                # Buscar columna de valor\n",
    "                valor_col = None\n",
    "                for col in value_cols:\n",
    "                    if col in df.columns:\n",
    "                        valor_col = col\n",
    "                        break\n",
    "                \n",
    "                if valor_col:\n",
    "                    df_temp = df[['fecha', valor_col]].copy()\n",
    "                    df_temp.columns = ['fecha', name]\n",
    "                    \n",
    "                    # Usar funci\u00f3n helper para normalizar fechas\n",
    "                    df_temp['fecha'] = safe_to_datetime(df_temp['fecha'], normalize=True)\n",
    "                    \n",
    "                    df_temp.set_index('fecha', inplace=True)\n",
    "                    \n",
    "                    # Manejar duplicados despu\u00e9s de normalizar\n",
    "                    if df_temp.index.duplicated().any():\n",
    "                        df_temp = df_temp.groupby(level=0).mean()\n",
    "                    \n",
    "                    # Eliminar valores NaN\n",
    "                    df_temp = df_temp.dropna()\n",
    "                    \n",
    "                    if len(df_temp) > 0:\n",
    "                        # Unir con el DataFrame consolidado\n",
    "                        consolidated_df = consolidated_df.join(df_temp, how='outer')\n",
    "                        series_added.append(name)\n",
    "    \n",
    "    print(f\"   \u2705 Series agregadas: {len(series_added)}\")\n",
    "    \n",
    "    # Calcular correlaciones solo con datos completos\n",
    "    if not consolidated_df.empty:\n",
    "        print(\"\\n\ud83d\udcc8 Calculando matriz de correlaci\u00f3n...\")\n",
    "        \n",
    "        # IMPORTANTE: Ordenar el \u00edndice antes de resamplear\n",
    "        print(\"   Ordenando \u00edndice de fechas...\")\n",
    "        consolidated_df = consolidated_df.sort_index()\n",
    "        \n",
    "        # Verificar si el \u00edndice es mon\u00f3tono\n",
    "        if not consolidated_df.index.is_monotonic_increasing:\n",
    "            print(\"   \u26a0\ufe0f \u00cdndice no mon\u00f3tono detectado, eliminando duplicados...\")\n",
    "            # Eliminar fechas duplicadas (mantener el primer valor)\n",
    "            consolidated_df = consolidated_df[~consolidated_df.index.duplicated(keep='first')]\n",
    "            consolidated_df = consolidated_df.sort_index()\n",
    "        \n",
    "        # Solo resamplear si tenemos series con diferentes frecuencias\n",
    "        # Para series diarias, esto no es necesario\n",
    "        print(\"   Verificando necesidad de resampling...\")\n",
    "        date_diffs = consolidated_df.index.to_series().diff().dropna()\n",
    "        if date_diffs.min() > pd.Timedelta(days=1):\n",
    "            # Hay gaps mayores a 1 d\u00eda, probablemente series mensuales mezcladas\n",
    "            print(\"   Resampling series a frecuencia diaria...\")\n",
    "            consolidated_df = consolidated_df.resample('D').ffill()\n",
    "        else:\n",
    "            print(\"   No es necesario resamplear (todas las series son diarias)\")\n",
    "        \n",
    "        # Ahora eliminar columnas con demasiados NaN\n",
    "        print(\"   Eliminando columnas con >80% NaN...\")\n",
    "        nan_threshold = 0.8\n",
    "        for col in consolidated_df.columns:\n",
    "            nan_ratio = consolidated_df[col].isna().sum() / len(consolidated_df)\n",
    "            if nan_ratio > nan_threshold:\n",
    "                print(f\"      Eliminando {col}: {nan_ratio:.1%} NaN\")\n",
    "                consolidated_df = consolidated_df.drop(columns=[col])\n",
    "        \n",
    "        # Eliminar filas con muchos NaN\n",
    "        threshold = max(2, len(consolidated_df.columns) * 0.3)  # Al menos 30% de datos presentes\n",
    "        before_rows = len(consolidated_df)\n",
    "        consolidated_df = consolidated_df.dropna(thresh=threshold)\n",
    "        after_rows = len(consolidated_df)\n",
    "        \n",
    "        print(f\"   Filas con datos suficientes: {after_rows}/{before_rows}\")\n",
    "        print(f\"   Variables en an\u00e1lisis: {len(consolidated_df.columns)}\")\n",
    "        \n",
    "        # Para las correlaciones, usar solo pares de datos completos\n",
    "        if len(consolidated_df) > 30:  # Necesitamos al menos 30 observaciones\n",
    "            # Calcular correlaci\u00f3n usando pairwise (ignora NaN en pares)\n",
    "            correlation_matrix = consolidated_df.corr(method='pearson', min_periods=30)\n",
    "            \n",
    "            # Contar correlaciones v\u00e1lidas\n",
    "            valid_corrs = (~correlation_matrix.isna()).sum().sum()\n",
    "            total_corrs = len(correlation_matrix.columns) ** 2\n",
    "            \n",
    "            print(f\"   \u2705 Matriz de correlaci\u00f3n calculada: {correlation_matrix.shape}\")\n",
    "            print(f\"   Correlaciones v\u00e1lidas: {valid_corrs}/{total_corrs} ({valid_corrs/total_corrs:.1%})\")\n",
    "            \n",
    "            # Mostrar variables con m\u00e1s correlaciones v\u00e1lidas\n",
    "            valid_counts = (~correlation_matrix.isna()).sum()\n",
    "            top_vars = valid_counts.nlargest(10)\n",
    "            print(\"\\n   \ud83d\udcca Variables con m\u00e1s correlaciones v\u00e1lidas:\")\n",
    "            for var, count in top_vars.items():\n",
    "                print(f\"      {var}: {count}/{len(correlation_matrix.columns)-1} correlaciones\")\n",
    "            \n",
    "            return correlation_matrix, consolidated_df\n",
    "        else:\n",
    "            print(f\"   \u26a0\ufe0f Datos insuficientes para correlaci\u00f3n ({len(consolidated_df)} filas)\")\n",
    "            return pd.DataFrame(), consolidated_df\n",
    "    else:\n",
    "        print(\"   \u26a0\ufe0f DataFrame consolidado vac\u00edo\")\n",
    "        return pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "# Calcular correlaciones\n",
    "print(\"=\"*80)\n",
    "print(\"\ud83d\udd0d AN\u00c1LISIS DE CORRELACIONES\")\n",
    "print(\"=\"*80)\n",
    "corr_matrix, consolidated_df = create_correlation_matrix(all_data)\n",
    "\n",
    "if not corr_matrix.empty:\n",
    "    # Buscar la columna de precio de varilla (puede ser precio_varilla_lme o precio_varilla_ahmsa)\n",
    "    target_col = None\n",
    "    if 'precio_varilla_lme' in corr_matrix.columns:\n",
    "        target_col = 'precio_varilla_lme'\n",
    "    elif 'precio_varilla_ahmsa' in corr_matrix.columns:\n",
    "        target_col = 'precio_varilla_ahmsa'\n",
    "    \n",
    "    if target_col:\n",
    "        # Mostrar correlaciones con precio de varilla\n",
    "        print(\"\\n\ud83d\udd17 CORRELACIONES CON PRECIO DE VARILLA\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Variable objetivo: {target_col}\")\n",
    "        print(\"-\"*60)\n",
    "        \n",
    "        correlations = corr_matrix[target_col].sort_values(ascending=False)\n",
    "        \n",
    "        # Separar por categor\u00edas de fuerza\n",
    "        muy_fuerte = []\n",
    "        fuerte = []\n",
    "        moderada = []\n",
    "        debil = []\n",
    "        \n",
    "        for var, corr in correlations.items():\n",
    "            if var != target_col and not pd.isna(corr):\n",
    "                abs_corr = abs(corr)\n",
    "                item = (var, corr)\n",
    "                \n",
    "                if abs_corr > 0.8:\n",
    "                    muy_fuerte.append(item)\n",
    "                elif abs_corr > 0.6:\n",
    "                    fuerte.append(item)\n",
    "                elif abs_corr > 0.4:\n",
    "                    moderada.append(item)\n",
    "                else:\n",
    "                    debil.append(item)\n",
    "        \n",
    "        # Mostrar por categor\u00edas\n",
    "        if muy_fuerte:\n",
    "            print(\"\\n\ud83d\udd34 CORRELACI\u00d3N MUY FUERTE (|r| > 0.8):\")\n",
    "            for var, corr in muy_fuerte:\n",
    "                direction = \"\u2191\" if corr > 0 else \"\u2193\"\n",
    "                print(f\"   {direction} {var:25s}: {corr:+.3f}\")\n",
    "        \n",
    "        if fuerte:\n",
    "            print(\"\\n\ud83d\udfe0 CORRELACI\u00d3N FUERTE (0.6 < |r| \u2264 0.8):\")\n",
    "            for var, corr in fuerte:\n",
    "                direction = \"\u2191\" if corr > 0 else \"\u2193\"\n",
    "                print(f\"   {direction} {var:25s}: {corr:+.3f}\")\n",
    "        \n",
    "        if moderada:\n",
    "            print(\"\\n\ud83d\udfe1 CORRELACI\u00d3N MODERADA (0.4 < |r| \u2264 0.6):\")\n",
    "            for var, corr in moderada[:10]:  # Mostrar solo las 10 primeras\n",
    "                direction = \"\u2191\" if corr > 0 else \"\u2193\"\n",
    "                print(f\"   {direction} {var:25s}: {corr:+.3f}\")\n",
    "            if len(moderada) > 10:\n",
    "                print(f\"   ... y {len(moderada)-10} m\u00e1s\")\n",
    "        \n",
    "        if debil:\n",
    "            print(f\"\\n\ud83d\udfe2 CORRELACI\u00d3N D\u00c9BIL (|r| \u2264 0.4): {len(debil)} variables\")\n",
    "            # Mostrar solo las 5 m\u00e1s fuertes de las d\u00e9biles\n",
    "            for var, corr in sorted(debil, key=lambda x: abs(x[1]), reverse=True)[:5]:\n",
    "                direction = \"\u2191\" if corr > 0 else \"\u2193\"\n",
    "                print(f\"   {direction} {var:25s}: {corr:+.3f}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "    \n",
    "    # Crear heatmap de correlaciones\n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=corr_matrix.values,\n",
    "        x=corr_matrix.columns,\n",
    "        y=corr_matrix.columns,\n",
    "        colorscale='RdBu',\n",
    "        zmid=0,\n",
    "        text=corr_matrix.values,\n",
    "        texttemplate='%{text:.2f}',\n",
    "        textfont={\"size\": 10},\n",
    "        colorbar=dict(title=\"Correlaci\u00f3n\")\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=\"Matriz de Correlaci\u00f3n entre Variables\",\n",
    "        width=800,\n",
    "        height=800,\n",
    "        xaxis_title=\"Variables\",\n",
    "        yaxis_title=\"Variables\"\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    print(\"\\n\ud83d\udcca Matriz de correlaci\u00f3n generada\")\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f No se pudo calcular la matriz de correlaci\u00f3n - datos insuficientes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udd17 Correlaci\u00f3n con Commodities Mensuales\n",
    "\n",
    "An\u00e1lisis espec\u00edfico de correlaci\u00f3n entre el precio de la varilla y los commodities mensuales del World Bank.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIAGN\u00d3STICO: Visualizar el proceso de JOIN de variables MENSUALES actualizadas\n",
    "print(\"=\"*80)\n",
    "print(\"\ud83d\udd0d DIAGN\u00d3STICO: PROCESO DE JOIN DE VARIABLES MENSUALES ACTUALIZADAS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Fecha de corte para verificar si las series est\u00e1n actualizadas\n",
    "fecha_corte = pd.Timestamp('2025-01-01')\n",
    "\n",
    "# 1. Primero, identificar todas las series mensuales actualizadas\n",
    "print(\"\\n1\ufe0f\u20e3 IDENTIFICACI\u00d3N DE SERIES MENSUALES ACTUALIZADAS:\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "series_mensuales_actualizadas = {}\n",
    "\n",
    "# FRED - Series mensuales\n",
    "fred_mensuales = [\n",
    "    ('federal_funds_rate', 'tasa_fed'),\n",
    "    ('steel_production', 'produccion_acero_us'),\n",
    "    ('ppi_metals', 'ppi_metales'),\n",
    "    ('iron_steel_scrap', 'chatarra_acero'),\n",
    "    ('GastoConstruccion', 'gasto_construccion_us'),\n",
    "    ('ProduccionIndustrial', 'produccion_industrial_us')\n",
    "]\n",
    "\n",
    "if 'FRED' in all_data:\n",
    "    for series_key, nombre in fred_mensuales:\n",
    "        # Buscar la serie\n",
    "        df = None\n",
    "        if series_key in all_data['FRED']:\n",
    "            df = all_data['FRED'][series_key]\n",
    "        else:\n",
    "            # Buscar con fecha\n",
    "            matching = [k for k in all_data['FRED'].keys() if k.startswith(series_key) and 'metadata' not in k]\n",
    "            if matching:\n",
    "                df = all_data['FRED'][matching[0]]\n",
    "        \n",
    "        if df is not None and isinstance(df, pd.DataFrame) and 'fecha' in df.columns:\n",
    "            # Usar safe_to_datetime para manejar fechas\n",
    "            df['fecha'] = safe_to_datetime(df['fecha'])\n",
    "            fecha_max = df['fecha'].max()\n",
    "            \n",
    "            if fecha_max >= fecha_corte:\n",
    "                # Buscar columna de valor\n",
    "                valor_col = None\n",
    "                for col in ['valor', 'value', 'Value']:\n",
    "                    if col in df.columns:\n",
    "                        valor_col = col\n",
    "                        break\n",
    "                \n",
    "                if not valor_col and len(df.columns) > 1:\n",
    "                    numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "                    valor_col = numeric_cols[0] if len(numeric_cols) > 0 else None\n",
    "                \n",
    "                if valor_col:\n",
    "                    temp_df = df[['fecha', valor_col]].copy()\n",
    "                    temp_df.columns = ['fecha', nombre]\n",
    "                    temp_df['fecha'] = safe_to_datetime(temp_df['fecha'])\n",
    "                    temp_df.set_index('fecha', inplace=True)\n",
    "                    \n",
    "                    # Manejar duplicados\n",
    "                    if temp_df.index.duplicated().any():\n",
    "                        temp_df = temp_df.groupby(level=0).mean()\n",
    "                    \n",
    "                    temp_df = temp_df.dropna()\n",
    "                    series_mensuales_actualizadas[nombre] = temp_df\n",
    "                    \n",
    "                    print(f\"\u2705 FRED - {nombre}:\")\n",
    "                    print(f\"   \u2022 Registros: {len(temp_df)}\")\n",
    "                    print(f\"   \u2022 Rango: {temp_df.index.min()} a {temp_df.index.max()}\")\n",
    "\n",
    "# INEGI - Series mensuales actualizadas\n",
    "inegi_mensuales = [\n",
    "    ('ProduccionConstruccion', 'produccion_construccion_mx'),\n",
    "    ('produccion_metalurgica', 'produccion_metalurgica_mx')\n",
    "]\n",
    "\n",
    "if 'INEGI' in all_data:\n",
    "    for series_key, nombre in inegi_mensuales:\n",
    "        # Buscar la serie\n",
    "        df = None\n",
    "        if series_key in all_data['INEGI']:\n",
    "            df = all_data['INEGI'][series_key]\n",
    "        else:\n",
    "            matching = [k for k in all_data['INEGI'].keys() if k.startswith(series_key) and 'metadata' not in k]\n",
    "            if matching:\n",
    "                df = all_data['INEGI'][matching[0]]\n",
    "        \n",
    "        if df is not None and isinstance(df, pd.DataFrame) and 'fecha' in df.columns:\n",
    "            df['fecha'] = safe_to_datetime(df['fecha'])\n",
    "            fecha_max = df['fecha'].max()\n",
    "            \n",
    "            if fecha_max >= pd.Timestamp('2025-07-01'):  # INEGI tiene datos hasta julio 2025\n",
    "                valor_col = None\n",
    "                for col in ['valor', 'value']:\n",
    "                    if col in df.columns:\n",
    "                        valor_col = col\n",
    "                        break\n",
    "                \n",
    "                if not valor_col and len(df.columns) > 1:\n",
    "                    numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "                    valor_col = numeric_cols[0] if len(numeric_cols) > 0 else None\n",
    "                \n",
    "                if valor_col:\n",
    "                    temp_df = df[['fecha', valor_col]].copy()\n",
    "                    temp_df.columns = ['fecha', nombre]\n",
    "                    temp_df['fecha'] = safe_to_datetime(temp_df['fecha'])\n",
    "                    temp_df.set_index('fecha', inplace=True)\n",
    "                    \n",
    "                    if temp_df.index.duplicated().any():\n",
    "                        temp_df = temp_df.groupby(level=0).mean()\n",
    "                    \n",
    "                    temp_df = temp_df.dropna()\n",
    "                    series_mensuales_actualizadas[nombre] = temp_df\n",
    "                    \n",
    "                    print(f\"\u2705 INEGI - {nombre}:\")\n",
    "                    print(f\"   \u2022 Registros: {len(temp_df)}\")\n",
    "                    print(f\"   \u2022 Rango: {temp_df.index.min()} a {temp_df.index.max()}\")\n",
    "\n",
    "# Banxico - Inflaci\u00f3n mensual\n",
    "if 'banxico' in all_data:\n",
    "    inflation_keys = [k for k in all_data['banxico'].keys() if 'inflation_monthly' in k and 'metadata' not in k]\n",
    "    if inflation_keys:\n",
    "        df = all_data['banxico'][inflation_keys[0]]\n",
    "        if isinstance(df, pd.DataFrame) and 'fecha' in df.columns:\n",
    "            df['fecha'] = safe_to_datetime(df['fecha'])\n",
    "            fecha_max = df['fecha'].max()\n",
    "            \n",
    "            if fecha_max >= fecha_corte:\n",
    "                valor_col = None\n",
    "                for col in ['valor', 'value', 'inflation_rate']:\n",
    "                    if col in df.columns:\n",
    "                        valor_col = col\n",
    "                        break\n",
    "                \n",
    "                if not valor_col and len(df.columns) > 1:\n",
    "                    numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "                    valor_col = numeric_cols[0] if len(numeric_cols) > 0 else None\n",
    "                \n",
    "                if valor_col:\n",
    "                    temp_df = df[['fecha', valor_col]].copy()\n",
    "                    temp_df.columns = ['fecha', 'inflacion_mx']\n",
    "                    temp_df['fecha'] = safe_to_datetime(temp_df['fecha'])\n",
    "                    temp_df.set_index('fecha', inplace=True)\n",
    "                    \n",
    "                    if temp_df.index.duplicated().any():\n",
    "                        temp_df = temp_df.groupby(level=0).mean()\n",
    "                    \n",
    "                    temp_df = temp_df.dropna()\n",
    "                    series_mensuales_actualizadas['inflacion_mx'] = temp_df\n",
    "                    \n",
    "                    print(f\"\u2705 Banxico - inflacion_mx:\")\n",
    "                    print(f\"   \u2022 Registros: {len(temp_df)}\")\n",
    "                    print(f\"   \u2022 Rango: {temp_df.index.min()} a {temp_df.index.max()}\")\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Total de series mensuales actualizadas: {len(series_mensuales_actualizadas)}\")\n",
    "\n",
    "# 2. Proceso de JOIN de series mensuales\n",
    "print(\"\\n2\ufe0f\u20e3 PROCESO DE JOIN DE SERIES MENSUALES:\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "if len(series_mensuales_actualizadas) > 0:\n",
    "    # Empezar con la primera serie\n",
    "    first_key = list(series_mensuales_actualizadas.keys())[0]\n",
    "    consolidated_monthly = series_mensuales_actualizadas[first_key].copy()\n",
    "    print(f\"\\nIniciando con: {first_key} - Shape: {consolidated_monthly.shape}\")\n",
    "    \n",
    "    # Hacer join con las dem\u00e1s series\n",
    "    for i, (name, df) in enumerate(list(series_mensuales_actualizadas.items())[1:], 1):\n",
    "        print(f\"\\n{i}. JOIN con {name}:\")\n",
    "        print(f\"   \u2022 DataFrame actual: {consolidated_monthly.shape}\")\n",
    "        print(f\"   \u2022 Serie a unir: {df.shape}\")\n",
    "        \n",
    "        # Hacer el join\n",
    "        consolidated_monthly = consolidated_monthly.join(df, how='outer')\n",
    "        \n",
    "        print(f\"   \u2022 Resultado despu\u00e9s del join: {consolidated_monthly.shape}\")\n",
    "        print(f\"   \u2022 NaN introducidos: {consolidated_monthly.isna().sum().sum() - consolidated_monthly.iloc[:, :-1].isna().sum().sum()}\")\n",
    "    \n",
    "    # 3. An\u00e1lisis del DataFrame consolidado mensual\n",
    "    print(\"\\n3\ufe0f\u20e3 ESTADO FINAL DEL DATAFRAME MENSUAL CONSOLIDADO:\")\n",
    "    print(\"-\"*40)\n",
    "    print(f\"\u2022 Forma final: {consolidated_monthly.shape}\")\n",
    "    print(f\"\u2022 Rango de fechas: {consolidated_monthly.index.min()} a {consolidated_monthly.index.max()}\")\n",
    "    print(f\"\u2022 Total de NaN: {consolidated_monthly.isna().sum().sum()}\")\n",
    "    print(f\"\u2022 Porcentaje de datos completos: {(1 - consolidated_monthly.isna().sum().sum() / (consolidated_monthly.shape[0] * consolidated_monthly.shape[1])) * 100:.1f}%\")\n",
    "    \n",
    "    # An\u00e1lisis de NaN por columna\n",
    "    print(\"\\n4\ufe0f\u20e3 AN\u00c1LISIS DE COMPLETITUD POR VARIABLE:\")\n",
    "    print(\"-\"*40)\n",
    "    nan_analysis_monthly = pd.DataFrame({\n",
    "        'variable': consolidated_monthly.columns,\n",
    "        'datos_disponibles': consolidated_monthly.count(),\n",
    "        'datos_faltantes': consolidated_monthly.isna().sum(),\n",
    "        'pct_completo': (consolidated_monthly.count() / len(consolidated_monthly) * 100).round(1)\n",
    "    })\n",
    "    nan_analysis_monthly = nan_analysis_monthly.sort_values('pct_completo', ascending=False)\n",
    "    print(nan_analysis_monthly)\n",
    "    \n",
    "    # Muestra del DataFrame consolidado\n",
    "    print(\"\\n5\ufe0f\u20e3 MUESTRA DEL DATAFRAME MENSUAL CONSOLIDADO:\")\n",
    "    print(\"-\"*40)\n",
    "    print(\"Primeras 5 filas:\")\n",
    "    print(consolidated_monthly.head())\n",
    "    print(\"\\n\u00daltimas 5 filas:\")\n",
    "    print(consolidated_monthly.tail())\n",
    "    \n",
    "    # 6. Resample a diario para integraci\u00f3n con series diarias\n",
    "    print(\"\\n6\ufe0f\u20e3 CONVERSI\u00d3N A FRECUENCIA DIARIA (FORWARD FILL):\")\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "    # Resample a diario con forward fill\n",
    "    consolidated_monthly_daily = consolidated_monthly.resample('D').ffill()\n",
    "    \n",
    "    print(f\"\u2022 Shape original (mensual): {consolidated_monthly.shape}\")\n",
    "    print(f\"\u2022 Shape despu\u00e9s de resample (diario): {consolidated_monthly_daily.shape}\")\n",
    "    print(f\"\u2022 Rango de fechas diario: {consolidated_monthly_daily.index.min()} a {consolidated_monthly_daily.index.max()}\")\n",
    "    \n",
    "    # An\u00e1lisis de propagaci\u00f3n\n",
    "    print(\"\\n7\ufe0f\u20e3 AN\u00c1LISIS DE PROPAGACI\u00d3N (Forward Fill):\")\n",
    "    print(\"-\"*40)\n",
    "    print(\"Ejemplo de c\u00f3mo se propagan los valores mensuales a diarios:\")\n",
    "    \n",
    "    # Tomar una muestra de transici\u00f3n de mes\n",
    "    sample_date = pd.Timestamp('2025-07-28')\n",
    "    sample_end = pd.Timestamp('2025-08-05')\n",
    "    \n",
    "    if sample_date in consolidated_monthly_daily.index and sample_end in consolidated_monthly_daily.index:\n",
    "        sample = consolidated_monthly_daily.loc[sample_date:sample_end, consolidated_monthly_daily.columns[:3]]\n",
    "        print(f\"\\nMuestra de {sample_date.date()} a {sample_end.date()}:\")\n",
    "        print(sample)\n",
    "    \n",
    "    # 8. Correlaci\u00f3n con variable objetivo (precio varilla)\n",
    "    print(\"\\n8\ufe0f\u20e3 CORRELACI\u00d3N DE VARIABLES MENSUALES CON PRECIO VARILLA:\")\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "    # Obtener precio de varilla\n",
    "    precio_varilla = None\n",
    "    if 'LME' in all_data and 'steel_rebar' in all_data['LME']:\n",
    "        df_varilla = all_data['LME']['steel_rebar']\n",
    "        if isinstance(df_varilla, pd.DataFrame) and 'fecha' in df_varilla.columns:\n",
    "            valor_col = 'Close' if 'Close' in df_varilla.columns else df_varilla.select_dtypes(include=['float64']).columns[0]\n",
    "            precio_varilla = df_varilla[['fecha', valor_col]].copy()\n",
    "            precio_varilla.columns = ['fecha', 'precio_varilla']\n",
    "            precio_varilla['fecha'] = safe_to_datetime(precio_varilla['fecha'])\n",
    "            precio_varilla.set_index('fecha', inplace=True)\n",
    "            \n",
    "            if precio_varilla.index.duplicated().any():\n",
    "                precio_varilla = precio_varilla.groupby(level=0).mean()\n",
    "    \n",
    "    if precio_varilla is not None and not consolidated_monthly_daily.empty:\n",
    "        # Unir con precio de varilla\n",
    "        combined = precio_varilla.join(consolidated_monthly_daily, how='inner')\n",
    "        \n",
    "        # Calcular correlaciones\n",
    "        correlations = combined.corr()['precio_varilla'].drop('precio_varilla').sort_values(ascending=False)\n",
    "        \n",
    "        print(f\"\\nDatos combinados: {combined.shape[0]} observaciones\")\n",
    "        print(\"\\nCorrelaciones con precio de varilla:\")\n",
    "        for var, corr in correlations.items():\n",
    "            if abs(corr) >= 0.3:\n",
    "                emoji = \"\ud83d\udd34\" if abs(corr) > 0.6 else \"\ud83d\udfe0\" if abs(corr) > 0.4 else \"\ud83d\udfe1\"\n",
    "                print(f\"  {emoji} {var:30s}: {corr:+.3f}\")\n",
    "\n",
    "else:\n",
    "    print(\"\u274c No se encontraron series mensuales actualizadas\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\ud83d\udca1 OBSERVACIONES CLAVE SOBRE VARIABLES MENSUALES:\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "1. Las variables mensuales capturan TENDENCIAS de largo plazo\n",
    "2. Forward fill propaga el valor mensual hasta el siguiente mes\n",
    "3. Esto introduce autocorrelaci\u00f3n artificial en los datos diarios\n",
    "4. Para modelos predictivos, considerar:\n",
    "   - MIDAS (Mixed Data Sampling) para preservar frecuencias originales\n",
    "   - Usar variables mensuales como features de contexto, no principales\n",
    "   - Aplicar rezagos apropiados (ej: inflaci\u00f3n del mes anterior)\n",
    "5. Variables mensuales m\u00e1s relevantes suelen ser:\n",
    "   - Inflaci\u00f3n (INPC/INPP)\n",
    "   - Producci\u00f3n industrial/construcci\u00f3n\n",
    "   - Indicadores econ\u00f3micos generales\n",
    "\"\"\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deacero_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}