#!/usr/bin/env python3
"""
Test de ingesta para Nasdaq Data Link (Quandl)
Analiza datos fundamentales de empresas de acero y series temporales
"""

import asyncio
import sys
import os
import pytest
from datetime import datetime, timedelta
import pandas as pd

# Configurar paths
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(os.path.dirname(current_dir))
sys.path.insert(0, project_root)

# Cargar variables de entorno
from dotenv import load_dotenv
load_dotenv(os.path.join(project_root, '.env'))

from src.data_ingestion.quandl_collector import QuandlCollector


class TestQuandlIngestion:
    """Test class para ingesta de Nasdaq Data Link"""
    
    @pytest.mark.asyncio
    async def test_steel_companies_fundamentals(self):
        """Test de datos fundamentales de empresas de acero"""
        
        print("ðŸ—ï¸ TEST NASDAQ DATA LINK - FUNDAMENTALES DE ACERO")
        print("=" * 50)
        
        api_key = os.getenv('QUANDL_API_KEY')
        assert api_key is not None, "Nasdaq Data Link API key debe estar configurada"
        print(f"ðŸ”‘ API Key: {api_key[:10]}...")
        
        async with QuandlCollector(api_key) as collector:
            print(f"âœ… Colector Nasdaq Data Link inicializado")
            print(f"ðŸ—ï¸ Empresas de acero configuradas: {len(collector.steel_companies)}")
            
            companies_analysis = {}
            
            # Analizar cada empresa de acero
            for company_key, company_info in collector.steel_companies.items():
                print(f"\nðŸ—ï¸ Analizando: {company_key}")
                print(f"   Ticker: {company_info['ticker']}")
                print(f"   Nombre: {company_info['name']}")
                print(f"   Importancia: {company_info['importance']}")
                
                try:
                    result = await collector.get_company_fundamentals(
                        company_key,
                        save_raw=False
                    )
                    
                    if result and 'data' in result and not result['data'].empty:
                        df = result['data']
                        metrics = result.get('metrics', {})
                        
                        companies_analysis[company_key] = {
                            'info': company_info,
                            'result': result,
                            'metrics': metrics,
                            'status': 'success'
                        }
                        
                        print(f"   âœ… Datos obtenidos: {len(df)} perÃ­odos")
                        print(f"   ðŸ“… Ãšltimo perÃ­odo: {result.get('latest_date', 'N/A')}")
                        
                        # Mostrar mÃ©tricas clave
                        if metrics:
                            if 'revenue' in metrics and metrics['revenue']:
                                print(f"   ðŸ’° Revenue: ${metrics['revenue']:,.0f}")
                            if 'ebitda' in metrics and metrics['ebitda']:
                                print(f"   ðŸ“Š EBITDA: ${metrics['ebitda']:,.0f}")
                            if 'eps_basic' in metrics and metrics['eps_basic']:
                                print(f"   ðŸ“ˆ EPS: ${metrics['eps_basic']:.2f}")
                            if 'current_ratio' in metrics and metrics['current_ratio']:
                                print(f"   ðŸ’§ Current Ratio: {metrics['current_ratio']:.2f}")
                            if 'debt_to_equity' in metrics and metrics['debt_to_equity']:
                                print(f"   ðŸ’¸ Debt/Equity: {metrics['debt_to_equity']:.2f}")
                            if 'gross_margin' in metrics and metrics['gross_margin']:
                                print(f"   ðŸ“Š Gross Margin: {metrics['gross_margin']:.1f}%")
                    else:
                        companies_analysis[company_key] = {
                            'info': company_info,
                            'status': 'no_data',
                            'error': 'Sin datos obtenidos'
                        }
                        print(f"   âŒ Sin datos disponibles")
                        
                except Exception as e:
                    companies_analysis[company_key] = {
                        'info': company_info,
                        'status': 'error',
                        'error': str(e)
                    }
                    print(f"   âŒ Error: {str(e)[:100]}")
            
            # Resumen de empresas
            successful = len([c for c in companies_analysis.values() if c['status'] == 'success'])
            critical = len([c for c in companies_analysis.values() 
                          if c['status'] == 'success' and c['info']['importance'] == 'critical'])
            
            print(f"\nðŸŽ¯ RESUMEN FUNDAMENTALES:")
            print(f"   ðŸ“Š Empresas exitosas: {successful}/{len(collector.steel_companies)}")
            print(f"   â­ Empresas crÃ­ticas con datos: {critical}")
            
            return companies_analysis
    
    @pytest.mark.asyncio
    async def test_timeseries_data_coverage(self):
        """Test de series temporales para commodities y economÃ­a"""
        
        print("\nðŸ“ˆ TEST NASDAQ DATA LINK - SERIES TEMPORALES")
        print("=" * 50)
        
        api_key = os.getenv('QUANDL_API_KEY')
        start_date = "2020-01-01"
        end_date = "2025-09-25"
        
        async with QuandlCollector(api_key) as collector:
            print(f"ðŸ“… PerÃ­odo: {start_date} a {end_date}")
            print(f"ðŸ“Š Series configuradas: {len(collector.timeseries_datasets)}")
            
            timeseries_analysis = {}
            
            for dataset_key, dataset_info in collector.timeseries_datasets.items():
                print(f"\nðŸ“ˆ Analizando: {dataset_key}")
                print(f"   CÃ³digo: {dataset_info['code']}")
                print(f"   Nombre: {dataset_info['name']}")
                print(f"   Frecuencia: {dataset_info['frequency']}")
                print(f"   CategorÃ­a: {dataset_info['category']}")
                
                try:
                    result = await collector.get_timeseries_data(
                        dataset_key,
                        start_date,
                        end_date,
                        save_raw=False
                    )
                    
                    if result and 'data' in result and not result['data'].empty:
                        df = result['data']
                        
                        # AnÃ¡lisis temporal
                        temporal_analysis = self._analyze_temporal_granularity(df, dataset_info)
                        
                        timeseries_analysis[dataset_key] = {
                            'info': dataset_info,
                            'result': result,
                            'temporal_analysis': temporal_analysis,
                            'status': 'success'
                        }
                        
                        print(f"   âœ… Datos obtenidos: {len(df)} puntos")
                        print(f"   ðŸ“… Rango: {df['fecha'].min()} a {df['fecha'].max()}")
                        print(f"   ðŸ“Š Granularidad detectada: {temporal_analysis['detected_frequency']}")
                        print(f"   â±ï¸ DÃ­as promedio entre datos: {temporal_analysis['avg_days_between']:.1f}")
                        print(f"   ðŸŽ¯ Apto para predicciÃ³n diaria: {'SÃ' if temporal_analysis['suitable_for_daily'] else 'NO'}")
                        
                        if result.get('latest_value'):
                            print(f"   ðŸ’° Ãšltimo valor: {result['latest_value']:.2f}")
                    else:
                        timeseries_analysis[dataset_key] = {
                            'info': dataset_info,
                            'status': 'no_data',
                            'error': 'Sin datos o sin acceso'
                        }
                        print(f"   âŒ Sin datos (posible restricciÃ³n de suscripciÃ³n)")
                        
                except Exception as e:
                    timeseries_analysis[dataset_key] = {
                        'info': dataset_info,
                        'status': 'error',
                        'error': str(e)
                    }
                    print(f"   âŒ Error: {str(e)[:100]}")
            
            # Resumen de series temporales
            successful = len([t for t in timeseries_analysis.values() if t['status'] == 'success'])
            daily = len([t for t in timeseries_analysis.values() 
                        if t['status'] == 'success' and t['temporal_analysis']['suitable_for_daily']])
            
            print(f"\nðŸŽ¯ RESUMEN SERIES TEMPORALES:")
            print(f"   ðŸ“Š Series exitosas: {successful}/{len(collector.timeseries_datasets)}")
            print(f"   ðŸ“… Series diarias aptas: {daily}")
            
            # Clasificar por categorÃ­a
            by_category = {}
            for key, analysis in timeseries_analysis.items():
                if analysis['status'] == 'success':
                    category = analysis['info']['category']
                    if category not in by_category:
                        by_category[category] = []
                    by_category[category].append(key)
            
            print(f"\nðŸ“Š POR CATEGORÃA:")
            for category, items in by_category.items():
                print(f"   â€¢ {category}: {len(items)} series")
                for item in items:
                    print(f"     - {item}")
            
            return timeseries_analysis
    
    def _analyze_temporal_granularity(self, df: pd.DataFrame, dataset_info: dict) -> dict:
        """Analizar granularidad temporal de series"""
        
        if df.empty or len(df) < 2:
            return {
                'detected_frequency': 'unknown',
                'suitable_for_daily': False,
                'avg_days_between': 0,
                'total_points': 0
            }
        
        # Convertir fechas y ordenar
        df = df.copy()
        df['fecha'] = pd.to_datetime(df['fecha'])
        df = df.sort_values('fecha')
        
        # Calcular diferencias entre fechas
        date_diffs = df['fecha'].diff().dropna()
        avg_days = date_diffs.dt.days.mean()
        
        # Determinar frecuencia
        if avg_days <= 1.5:
            detected_freq = 'daily'
            suitable = True
        elif avg_days <= 3:
            detected_freq = 'business_daily'
            suitable = True
        elif avg_days <= 8:
            detected_freq = 'weekly'
            suitable = True
        elif avg_days <= 35:
            detected_freq = 'monthly'
            suitable = False
        else:
            detected_freq = 'irregular'
            suitable = False
        
        # Verificar cobertura desde 2020
        coverage_from_2020 = df['fecha'].min() <= pd.Timestamp('2020-01-01')
        
        return {
            'detected_frequency': detected_freq,
            'suitable_for_daily': suitable,
            'avg_days_between': avg_days,
            'total_points': len(df),
            'date_range_days': (df['fecha'].max() - df['fecha'].min()).days,
            'coverage_from_2020': coverage_from_2020
        }
    
    @pytest.mark.asyncio
    async def test_full_ingestion(self):
        """Test completo de ingesta de Nasdaq Data Link"""
        
        print("\nðŸŽ¯ TEST COMPLETO NASDAQ DATA LINK")
        print("=" * 50)
        
        api_key = os.getenv('QUANDL_API_KEY')
        
        async with QuandlCollector(api_key) as collector:
            # Test empresas
            companies = await self.test_steel_companies_fundamentals()
            
            # Test series temporales
            timeseries = await self.test_timeseries_data_coverage()
            
            # Resumen global
            print("\n" + "=" * 50)
            print("ðŸ“‹ RESUMEN GLOBAL NASDAQ DATA LINK")
            print("=" * 50)
            
            companies_success = len([c for c in companies.values() if c['status'] == 'success'])
            timeseries_success = len([t for t in timeseries.values() if t['status'] == 'success'])
            
            print(f"\nðŸ—ï¸ EMPRESAS DE ACERO:")
            print(f"   âœ… Con datos: {companies_success}/{len(companies)}")
            critical_companies = [k for k, v in companies.items() 
                                 if v['status'] == 'success' and v['info']['importance'] == 'critical']
            if critical_companies:
                print(f"   â­ CrÃ­ticas disponibles: {', '.join(critical_companies)}")
            
            print(f"\nðŸ“ˆ SERIES TEMPORALES:")
            print(f"   âœ… Con datos: {timeseries_success}/{len(timeseries)}")
            daily_series = [k for k, v in timeseries.items() 
                          if v['status'] == 'success' and v.get('temporal_analysis', {}).get('suitable_for_daily')]
            if daily_series:
                print(f"   ðŸ“… Diarias disponibles: {', '.join(daily_series)}")
            
            print(f"\nðŸ’¡ CONCLUSIÃ“N:")
            if companies_success > 0 and timeseries_success > 0:
                print("   âœ… Nasdaq Data Link FUNCIONAL para predicciÃ³n")
                print("   â€¢ Datos fundamentales de empresas disponibles")
                print("   â€¢ Series temporales econÃ³micas disponibles")
            elif companies_success > 0:
                print("   âš ï¸ Parcialmente funcional")
                print("   â€¢ Solo datos fundamentales disponibles")
            else:
                print("   âŒ Limitado por suscripciÃ³n")
                print("   â€¢ Considerar plan premium para mÃ¡s datos")
            
            return {
                'companies': companies,
                'timeseries': timeseries,
                'summary': {
                    'companies_success': companies_success,
                    'timeseries_success': timeseries_success,
                    'total_success': companies_success + timeseries_success
                }
            }


# FunciÃ³n para ejecutar manualmente
async def run_quandl_test():
    """Ejecutar test completo de Nasdaq Data Link"""
    test_instance = TestQuandlIngestion()
    result = await test_instance.test_full_ingestion()
    return result


if __name__ == "__main__":
    result = asyncio.run(run_quandl_test())
    print("\nâœ… Test Nasdaq Data Link completado")
    
    # Exit con cÃ³digo basado en Ã©xito
    success = result['summary']['total_success'] > 0
    exit(0 if success else 1)