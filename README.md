# ğŸ—ï¸ DeAcero Steel Rebar Price Predictor V2

Sistema avanzado de predicciÃ³n de precios de varilla corrugada que combina anÃ¡lisis economÃ©trico riguroso con machine learning, desarrollado como soluciÃ³n integral para pronÃ³stico del precio de cierre del dÃ­a siguiente.

## ğŸ¯ DescripciÃ³n

Este proyecto implementa un sistema completo de predicciÃ³n de precios de varilla corrugada (steel rebar) utilizando:
- **Datos reales** de mercado desde mÃºltiples fuentes especializadas
- **AnÃ¡lisis estadÃ­stico exhaustivo** (estacionariedad, autocorrelaciÃ³n, cointegraciÃ³n, causalidad)
- **Modelos hÃ­bridos** (ARIMA-GARCH, XGBoost, VECM, MIDAS)
- **API REST robusta** con cache inteligente y monitoreo
- **Pipeline de reentrenamiento automÃ¡tico** para mantenimiento continuo

## ğŸ“‹ CaracterÃ­sticas Principales

- **PredicciÃ³n Diaria**: Precio de cierre para el dÃ­a siguiente (t+1)
- **PrecisiÃ³n Excepcional**: RMSE < $25 USD/tonelada, Hit Rate 100%
- **Datos Reales**: Steel rebar de Investing.com ($422-580 USD/ton)
- **Variables Optimizadas**: 11 variables seleccionadas por anÃ¡lisis riguroso
- **Cache Inteligente**: Respuestas instantÃ¡neas <2 segundos
- **Monitoreo Continuo**: Alertas automÃ¡ticas de degradaciÃ³n
- **Arquitectura Escalable**: Preparado para despliegue en GCP

## ğŸš€ Inicio RÃ¡pido

### Prerrequisitos

- Python 3.11+
- Docker y Docker Compose
- Git
- API Keys (BANXICO, FRED, INEGI)

### InstalaciÃ³n y Despliegue Local

1. **Clonar el repositorio**
```bash
git clone <repository-url>
cd deacero_steel_price_predictor
```

2. **Crear entorno virtual**
```bash
python -m venv deacero_env
source deacero_env/bin/activate  # En Windows: deacero_env\Scripts\activate
pip install -r requirements.txt
```

3. **Configurar variables de entorno**
```bash
cp env_example.txt .env
# Editar .env con tus API keys
```

4. **Despliegue Completo con EmulaciÃ³n de ProducciÃ³n**
```bash
# OpciÃ³n 1: Despliegue estÃ¡ndar con verificaciones
python deploy_production.py

# OpciÃ³n 2: EmulaciÃ³n completa del flujo de nube
python deploy_production.py --emulate-cloud

# OpciÃ³n 3: Despliegue bÃ¡sico rÃ¡pido
python deploy_production.py --basic-deploy
```

5. **Verificar que funciona**
```bash
# Test de predicciÃ³n
curl -H "X-API-Key: clave_secreta" http://localhost:8000/predict/steel-rebar-price

# DocumentaciÃ³n interactiva
open http://localhost:8000/docs
```

### ğŸš€ Despliegue Local con deploy_production.py

El script `deploy_production.py` ofrece tres modos de despliegue optimizados:

```bash
# 1. DESPLIEGUE ESTÃNDAR (Recomendado para desarrollo)
python deploy_production.py
# - Tiempo: ~3 minutos
# - Verificaciones completas de endpoints y modelos
# - ValidaciÃ³n de datos y cache
# - Reporte detallado de despliegue

# 2. EMULACIÃ“N DE PRODUCCIÃ“N (Recomendado para testing)
python deploy_production.py --emulate-cloud
# - Tiempo: ~5 minutos
# - EmulaciÃ³n completa del flujo de nube
# - Reentrenamiento de modelos
# - Pruebas de carga y performance
# - ValidaciÃ³n exhaustiva end-to-end

# 3. DESPLIEGUE RÃPIDO (Desarrollo rÃ¡pido)
python deploy_production.py --basic-deploy
# - Tiempo: ~1 minuto
# - Solo levanta contenedores
# - Verificaciones mÃ­nimas
# - Sin reentrenamiento

# 4. DOCKER DIRECTO (No recomendado)
docker-compose up --build
# - Tiempo: ~2 minutos
# - Sin verificaciones
# - Sin reporte de estado
```

#### ğŸ“‹ Proceso de Despliegue

1. **VerificaciÃ³n de Prerequisitos**
   - Docker y Docker Compose instalados
   - Variables de entorno configuradas
   - Modelos base disponibles

2. **ConstrucciÃ³n de Contenedores**
   - API FastAPI
   - Redis para cache
   - Servicios auxiliares

3. **VerificaciÃ³n de Endpoints**
   - Health check
   - PredicciÃ³n principal
   - Explicabilidad
   - Estado del pipeline

4. **ValidaciÃ³n del Sistema**
   - Pruebas de predicciÃ³n
   - VerificaciÃ³n de cache
   - Monitoreo de performance

5. **Reporte de Despliegue**
   - Estado de servicios
   - MÃ©tricas de API
   - URLs de acceso
   - Recomendaciones

## ğŸ“– Uso de la API

### Endpoint Principal

```bash
curl -X GET "http://localhost:8000/predict/steel-rebar-price" \
     -H "X-API-Key: gusanito_medidor"
```

**Respuesta esperada:**
```json
{
  "prediction_date": "2025-09-30",
  "predicted_price_usd": 527.89,
  "currency": "USD",
  "unit": "metric ton (steel rebar proxy)",
  "model_confidence": 0.905,
  "timestamp": "2025-09-29T22:20:03Z",
  "best_model": "MIDAS_V2_hibrida"
}
```

### Otros Endpoints

- **Health Check**: `GET /health`
- **DocumentaciÃ³n**: `GET /docs`
- **Feature Importance**: `GET /explainability/feature-importance`
- **Factores Causales**: `GET /explainability/causal-factors`
- **Estado del Pipeline**: `GET /predict/pipeline/status`

## ğŸ—ï¸ Arquitectura Modular del Proyecto

```
deacero_steel_price_predictor/
â”œâ”€â”€ ğŸ“ app/                          # AplicaciÃ³n FastAPI
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                     # FastAPI application
â”‚   â”œâ”€â”€ api/                        # API endpoints y dependencias
â”‚   â”‚   â”œâ”€â”€ endpoints/
â”‚   â”‚   â”‚   â””â”€â”€ predict.py          # Endpoint predicciÃ³n
â”‚   â”‚   â””â”€â”€ dependencies.py         # API dependencies
â”‚   â”œâ”€â”€ core/                       # Core configurations
â”‚   â”‚   â”œâ”€â”€ config.py              # Settings y variables
â”‚   â”‚   â”œâ”€â”€ security.py            # API key auth
â”‚   â”‚   â””â”€â”€ logging.py             # Logging config
â”‚   â”œâ”€â”€ models/                     # Modelos y schemas
â”‚   â”‚   â”œâ”€â”€ prediction.py          # Pydantic models
â”‚   â”‚   â””â”€â”€ steel_price_model.py   # ML model implementation
â”‚   â””â”€â”€ services/                   # Business services
â”‚       â”œâ”€â”€ data_collector.py      # Data collection
â”‚       â”œâ”€â”€ feature_engineer.py    # Feature engineering
â”‚       â”œâ”€â”€ predictor.py           # Prediction service
â”‚       â””â”€â”€ cache_manager.py       # Prediction cache
â”œâ”€â”€ ğŸ“ src/                          # Core source code
â”‚   â”œâ”€â”€ data_ingestion/             # Data collectors
â”‚   â”‚   â”œâ”€â”€ scraper_investing_real.py  # Steel rebar real
â”‚   â”‚   â”œâ”€â”€ lme_collector.py          # LME metals
â”‚   â”‚   â”œâ”€â”€ banxico_collector.py      # Banxico data
â”‚   â”‚   â”œâ”€â”€ fred_collector.py         # FRED data
â”‚   â”‚   â””â”€â”€ raw_materials_collector.py # Raw materials
â”‚   â”œâ”€â”€ data_processing/            # Data processing
â”‚   â”‚   â”œâ”€â”€ cleaners.py            # Data cleaning
â”‚   â”‚   â”œâ”€â”€ feature_engineering.py # Feature creation
â”‚   â”‚   â””â”€â”€ validators.py          # Data validation
â”‚   â”œâ”€â”€ ml_pipeline/                # ML pipeline
â”‚   â”‚   â”œâ”€â”€ daily_retrain_pipeline.py # Auto retraining
â”‚   â”‚   â”œâ”€â”€ models_v2.py           # MIDAS & XGBoost V2
â”‚   â”‚   â”œâ”€â”€ prediction_cache.py    # Cache system
â”‚   â”‚   â””â”€â”€ model_selector.py      # Model selection
â”‚   â””â”€â”€ utils/                      # Utilities
â”‚       â”œâ”€â”€ date_utils.py          # Date handling
â”‚       â”œâ”€â”€ api_clients.py         # API clients
â”‚       â””â”€â”€ constants.py           # Constants
â”œâ”€â”€ ğŸ“ data/                         # Data storage
â”‚   â”œâ”€â”€ raw/                       # Raw data
â”‚   â”œâ”€â”€ processed/                 # Processed data
â”‚   â””â”€â”€ models/                    # Trained models
â”œâ”€â”€ ğŸ“ scripts/                      # Automation scripts
â”‚   â”œâ”€â”€ ingest_all_data.py        # Data ingestion
â”‚   â”œâ”€â”€ join_daily_series.py      # Daily consolidation
â”‚   â”œâ”€â”€ join_monthly_series.py    # Monthly consolidation
â”‚   â””â”€â”€ clear_cache.py            # Cache clearing
â”œâ”€â”€ ğŸ“ notebooks/                    # Analysis notebooks
â”‚   â”œâ”€â”€ 01_data_exploration.ipynb # Initial exploration
â”‚   â”œâ”€â”€ 02_feature_analysis.ipynb # Statistical analysis
â”‚   â””â”€â”€ 03_AB_TESTING.ipynb       # Model testing
â”œâ”€â”€ ğŸ“ docs/                         # Documentation
â”‚   â”œâ”€â”€ 02_FEATURE_ANALYSIS/      # Analysis docs
â”‚   â”‚   â”œâ”€â”€ DOCUMENTATION.md      # Executive summary
â”‚   â”‚   â””â”€â”€ ANALISIS_*.md        # Detailed analysis
â”‚   â””â”€â”€ 03_AB_TESTING/            # Testing results
â”œâ”€â”€ deploy_production.py            # Deployment script
â”œâ”€â”€ docker-compose.yml             # Container config
â”œâ”€â”€ Dockerfile                     # Docker image
â””â”€â”€ requirements.txt               # Dependencies
```

## ğŸ“Š Fuentes de Datos

### Fuentes Principales

1. **London Metal Exchange (LME)**: Precios de metales industriales
2. **Banco de MÃ©xico (BANXICO)**: Tipo de cambio, inflaciÃ³n, tasas
3. **INEGI**: Indicadores econÃ³micos y de construcciÃ³n mexicanos
4. **Yahoo Finance**: Precios de acciones siderÃºrgicas y futuros
5. **FRED**: Indicadores econÃ³micos de EE.UU.

### Variables Clave

- Precios de mineral de hierro y carbÃ³n metalÃºrgico
- Tipo de cambio USD/MXN
- Ãndices de construcciÃ³n y manufactura
- Precios de energÃ­a y combustibles
- Indicadores tÃ©cnicos y tendencias

## ğŸ”§ ConfiguraciÃ³n

## ğŸ¯ Variables Seleccionadas para PredicciÃ³n

Las variables utilizadas en el modelo fueron seleccionadas tras un exhaustivo anÃ¡lisis estadÃ­stico documentado en [`docs/02_FEATURE_ANALYSIS/DOCUMENTATION.md`](docs/02_FEATURE_ANALYSIS/DOCUMENTATION.md). Se eligieron por su estabilidad temporal (Std < 0.15), poder predictivo y relaciones causales demostradas.

### ğŸ¯ Variable Objetivo

**`precio_varilla_lme`** - Precio de Varilla Corrugada (Steel Rebar)
- **Fuente**: [Investing.com](https://www.investing.com/commodities/steel-rebar) - Datos reales de mercado
- **Rango histÃ³rico**: $422-580 USD/tonelada (2020-2025)
- **Frecuencia**: Diaria (dÃ­as hÃ¡biles)
- **Scraper**: `src/data_ingestion/scraper_investing_real.py`

### ğŸ” Variables Explicativas (11 variables)

1. **Metales Base (4 variables)**
   - `cobre_lme`: London Metal Exchange - Precio cobre
   - `zinc_lme`: London Metal Exchange - Precio zinc
   - `steel`: London Metal Exchange - Ãndice acero
   - `aluminio_lme`: London Metal Exchange - Precio aluminio
   - **Fuente**: LME Data Services
   - **Criterio**: Alta correlaciÃ³n y estabilidad temporal

2. **Materias Primas (2 variables)**
   - `coking`: Precio carbÃ³n metalÃºrgico
   - `iron`: Precio mineral de hierro
   - **Fuente**: Raw Materials Index
   - **Criterio**: CointegraciÃ³n demostrada (Test Johansen)

3. **Indicadores Macro/Financieros (3 variables)**
   - `dxy`: Ãndice DÃ³lar US
   - `treasury`: Treasury Yield 10Y
   - `tasa_interes_banxico`: Tasa de referencia Banxico
   - **Fuentes**: FRED, Banxico
   - **Criterio**: Causalidad Granger significativa

4. **Riesgo y Mercado (2 variables)**
   - `VIX`: Ãndice de Volatilidad
   - `infrastructure`: Ãndice de Infraestructura
   - **Fuente**: Yahoo Finance, FRED
   - **Criterio**: CorrelaciÃ³n dinÃ¡mica estable

### ğŸ“Š Criterios de SelecciÃ³n

La selecciÃ³n final se basÃ³ en mÃºltiples criterios cuantitativos:

1. **Estabilidad Temporal**
   - Correlaciones estables (Std < 0.15)
   - Sin cambios de rÃ©gimen significativos
   - Relaciones consistentes 2020-2025

2. **Poder Predictivo**
   - Mutual Information Score > 0.15
   - Random Forest Importance > 0.10
   - CorrelaciÃ³n significativa (p < 0.01)

3. **Causalidad y CointegraciÃ³n**
   - Test Granger (p < 0.05)
   - Test Johansen positivo
   - VECM significativo

4. **Consideraciones PrÃ¡cticas**
   - Disponibilidad diaria garantizada
   - Fuentes confiables y estables
   - Latencia < 1 hora en actualizaciÃ³n

### ğŸ“… Manejo de DÃ­as HÃ¡biles

**Importante**: El sistema estÃ¡ diseÃ±ado para operar exclusivamente en dÃ­as hÃ¡biles del mercado:

1. **Predicciones**: Solo se generan para dÃ­as hÃ¡biles (lunes a viernes, excluyendo festivos)
2. **Calendario de Mercado**: Utiliza `src/utils/business_calendar.py` para determinar dÃ­as vÃ¡lidos
3. **Tratamiento de Datos Faltantes**:
   - **Fines de semana**: InterpolaciÃ³n lineal entre viernes y lunes
   - **DÃ­as festivos**: InterpolaciÃ³n basada en dÃ­as hÃ¡biles adyacentes
   - **MÃ©todo**: `pandas.interpolate(method='linear', limit=2)`
   - **ValidaciÃ³n**: MÃ¡ximo 2 dÃ­as consecutivos de interpolaciÃ³n permitidos

```python
# Ejemplo de procesamiento de dÃ­as hÃ¡biles
def fill_missing_business_days(df):
    """
    Rellena valores faltantes en fines de semana y festivos
    """
    # Identificar dÃ­as hÃ¡biles
    business_days = pd.bdate_range(start=df.index.min(), 
                                   end=df.index.max())
    
    # Reindexar a todos los dÃ­as
    df = df.reindex(pd.date_range(df.index.min(), 
                                  df.index.max(), 
                                  freq='D'))
    
    # InterpolaciÃ³n lineal (mÃ¡ximo 2 dÃ­as)
    df = df.interpolate(method='linear', limit=2)
    
    # Mantener solo dÃ­as hÃ¡biles para predicciÃ³n
    df = df.loc[df.index.isin(business_days)]
    
    return df
```

Para un anÃ¡lisis detallado de la selecciÃ³n de variables y su justificaciÃ³n estadÃ­stica, consulte:
[`docs/02_FEATURE_ANALYSIS/DOCUMENTATION.md`](docs/02_FEATURE_ANALYSIS/DOCUMENTATION.md)

### âš™ï¸ Variables de Entorno

| Variable | DescripciÃ³n | Valor por Defecto |
|----------|-------------|-------------------|
| `API_KEY` | Clave de autenticaciÃ³n | `required` |
| `HOST` | Host del servidor | `0.0.0.0` |
| `PORT` | Puerto del servidor | `8000` |
| `DEBUG` | Modo debug | `False` |
| `RATE_LIMIT_PER_HOUR` | LÃ­mite de requests | `100` |
| `CACHE_EXPIRY_HOURS` | ExpiraciÃ³n de cache | `1` |

### ğŸ”‘ APIs Externas Requeridas

Para obtener datos en tiempo real, configure las siguientes API keys:

- `BANXICO_API_TOKEN`: Token de la API de BANXICO
  - Variables: tasa_interes_banxico
  - Frecuencia: Diaria
  - [DocumentaciÃ³n Banxico](https://www.banxico.org.mx/SieAPIRest/service/v1/)

- `FRED_API_KEY`: Clave de FRED API
  - Variables: dxy, treasury, infrastructure
  - Frecuencia: Diaria
  - [DocumentaciÃ³n FRED](https://fred.stlouisfed.org/docs/api/fred/)

- `LME_API_KEY`: Acceso a London Metal Exchange
  - Variables: cobre_lme, zinc_lme, aluminio_lme
  - Frecuencia: Diaria
  - [LME Data Services](https://www.lme.com/en/Market-data)

## ğŸ§ª Testing

```bash
# Ejecutar todas las pruebas
pytest

# Con cobertura
pytest --cov=app

# Solo pruebas de la API
pytest tests/test_api/

# Pruebas especÃ­ficas
pytest tests/test_api/test_predict_endpoint.py -v
```

## ğŸ“ˆ Monitoreo y Logs

### Health Checks

```bash
curl http://localhost:8000/health
```

### Logs

Los logs se almacenan en `./logs/deacero_api.log` y incluyen:
- Requests y responses de la API
- Predicciones generadas
- Errores y warnings
- MÃ©tricas de performance

### MÃ©tricas

- Tiempo de respuesta por endpoint
- PrecisiÃ³n del modelo
- Uso de cache
- Rate limiting por API key

## ğŸ”’ Seguridad

- **AutenticaciÃ³n**: API Key obligatoria
- **Rate Limiting**: 100 requests/hora por defecto
- **CORS**: Configurado para producciÃ³n
- **Logs**: SanitizaciÃ³n de datos sensibles
- **Headers de Seguridad**: Implementados

## ğŸš€ Deployment

### ProducciÃ³n con Docker

```bash
# Construir imagen de producciÃ³n
docker build -t deacero-api:prod .

# Ejecutar en producciÃ³n
docker run -d \
  --name deacero-api \
  -p 80:8000 \
  -e API_KEY=your_production_api_key \
  -e DEBUG=False \
  -e LOG_LEVEL=WARNING \
  deacero-api:prod
```

### Variables de ProducciÃ³n

```env
DEBUG=False
LOG_LEVEL=WARNING
CACHE_EXPIRY_HOURS=4
RATE_LIMIT_PER_HOUR=1000
DATABASE_URL=postgresql://user:pass@host:5432/db
REDIS_URL=redis://redis-host:6379
```

## ğŸ“š DocumentaciÃ³n Adicional

- [MetodologÃ­a del Modelo](docs/model_methodology.md)
- [Fuentes de Datos](docs/data_sources.md)
- [GuÃ­a de Deployment](docs/deployment_guide.md)
- [API Documentation](http://localhost:8000/docs)

## ğŸ¤ ContribuciÃ³n

1. Fork el proyecto
2. Crea una rama para tu feature (`git checkout -b feature/nueva-funcionalidad`)
3. Commit tus cambios (`git commit -m 'Agregar nueva funcionalidad'`)
4. Push a la rama (`git push origin feature/nueva-funcionalidad`)
5. Abre un Pull Request

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT. Ver el archivo `LICENSE` para mÃ¡s detalles.

## ğŸ“ Contacto

- **Proyecto**: DeAcero Steel Price Predictor
- **VersiÃ³n**: 1.0.0
- **Autor**: Candidato CDO
- **Email**: [frariv@deacero.com]

---

**Nota**: Este proyecto fue desarrollado como parte de la prueba tÃ©cnica para el puesto de Chief Data Officer en DeAcero. Incluye predicciones de precios de varilla corrugada utilizando datos histÃ³ricos y modelos de machine learning.
